{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3304,
     "status": "ok",
     "timestamp": 1600469965527,
     "user": {
      "displayName": "Derian Fernando",
      "photoUrl": "",
      "userId": "13994662697554874148"
     },
     "user_tz": 180
    },
    "id": "F4Hwms6UN_mS",
    "outputId": "93648687-b507-411c-f35a-9c729449eb89",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: nltk in /home/jamelly/.virtualenvs/deeplearning/lib/python3.8/site-packages (3.5)\nRequirement already satisfied: pycld2 in /home/jamelly/.virtualenvs/deeplearning/lib/python3.8/site-packages (0.41)\nRequirement already satisfied: wordsegment in /home/jamelly/.virtualenvs/deeplearning/lib/python3.8/site-packages (1.3.1)\nRequirement already satisfied: autocorrect in /home/jamelly/.virtualenvs/deeplearning/lib/python3.8/site-packages (2.1.0)\nRequirement already satisfied: click in /home/jamelly/.virtualenvs/deeplearning/lib/python3.8/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: tqdm in /home/jamelly/.virtualenvs/deeplearning/lib/python3.8/site-packages (from nltk) (4.49.0)\nRequirement already satisfied: regex in /home/jamelly/.virtualenvs/deeplearning/lib/python3.8/site-packages (from nltk) (2020.7.14)\nRequirement already satisfied: joblib in /home/jamelly/.virtualenvs/deeplearning/lib/python3.8/site-packages (from nltk) (0.15.1)\n\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\nYou should consider upgrading via the '/home/jamelly/.virtualenvs/deeplearning/bin/python -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"
    }
   ],
   "source": [
    "pip install nltk pycld2 wordsegment autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3948,
     "status": "ok",
     "timestamp": 1600469966180,
     "user": {
      "displayName": "Derian Fernando",
      "photoUrl": "",
      "userId": "13994662697554874148"
     },
     "user_tz": 180
    },
    "id": "s5ReWfACN6Zl",
    "outputId": "76059f8e-ec14-469c-8124-5d7c6dc77a83",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /home/jamelly/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "from nltk.corpus import twitter_samples\n",
    "twitter_samples.fileids()\n",
    "positive_tweets = [t for t in twitter_samples.strings(\"positive_tweets.json\")]\n",
    "negative_tweets = [t for t in twitter_samples.strings(\"negative_tweets.json\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3938,
     "status": "ok",
     "timestamp": 1600469966181,
     "user": {
      "displayName": "Derian Fernando",
      "photoUrl": "",
      "userId": "13994662697554874148"
     },
     "user_tz": 180
    },
    "id": "ODi-rkNCU1y0",
    "outputId": "6613acda-d7c0-4a5d-a627-c3982aca1c82",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5000\n5000\n"
    }
   ],
   "source": [
    "print(len(positive_tweets))\n",
    "print(len(negative_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3931,
     "status": "ok",
     "timestamp": 1600469966182,
     "user": {
      "displayName": "Derian Fernando",
      "photoUrl": "",
      "userId": "13994662697554874148"
     },
     "user_tz": 180
    },
    "id": "Xdu9utDSWVLa"
   },
   "outputs": [],
   "source": [
    "train_pos_tweets = positive_tweets[:3500]\n",
    "test_pos_tweets = positive_tweets[3500:]\n",
    "train_neg_tweets = negative_tweets[:3500]\n",
    "test_neg_tweets = negative_tweets[3500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3923,
     "status": "ok",
     "timestamp": 1600469966182,
     "user": {
      "displayName": "Derian Fernando",
      "photoUrl": "",
      "userId": "13994662697554874148"
     },
     "user_tz": 180
    },
    "id": "DnpGNGhHWz9Y",
    "outputId": "f1c9a614-9859-41bf-d595-8fbf866f084e",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3500\n1500\n3500\n1500\n"
    }
   ],
   "source": [
    "print(len(train_pos_tweets))\n",
    "print(len(test_pos_tweets))\n",
    "print(len(train_neg_tweets))\n",
    "print(len(test_neg_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3915,
     "status": "ok",
     "timestamp": 1600469966183,
     "user": {
      "displayName": "Derian Fernando",
      "photoUrl": "",
      "userId": "13994662697554874148"
     },
     "user_tz": 180
    },
    "id": "xRvzAc5wXJf2",
    "outputId": "dca0e8a7-a703-4f99-a2fc-3aa83f6b02c3",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7000\n3000\n"
    }
   ],
   "source": [
    "print(len(train_pos_tweets + train_neg_tweets))\n",
    "print(len(test_pos_tweets + test_neg_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3907,
     "status": "ok",
     "timestamp": 1600469966183,
     "user": {
      "displayName": "Derian Fernando",
      "photoUrl": "",
      "userId": "13994662697554874148"
     },
     "user_tz": 180
    },
    "id": "-Z_F-R2KsLO6",
    "outputId": "d395729f-eba5-4927-986b-7a5cdfa62177",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3500\n3147\n3500\n3058\n1500\n1365\n1500\n1326\n"
    }
   ],
   "source": [
    "import pycld2 as cld2\n",
    "\n",
    "def extract_engTweets(tweets):\n",
    "    en_tweets = []\n",
    "    for e in tweets:\n",
    "      details = cld2.detect(e)\n",
    "      # Extracts only the string with lang name i.e: 'en'\n",
    "      if details[2][0][1] == 'en': \n",
    "        en_tweets.append(e)\n",
    "    return en_tweets\n",
    "\n",
    "en_train_pos_tweets = extract_engTweets(train_pos_tweets)\n",
    "en_train_neg_tweets = extract_engTweets(train_neg_tweets)\n",
    "en_test_pos_tweets = extract_engTweets(test_pos_tweets)\n",
    "en_test_neg_tweets = extract_engTweets(test_neg_tweets)\n",
    "print(len(train_pos_tweets))\n",
    "print(len(en_train_pos_tweets))\n",
    "print(len(train_neg_tweets))\n",
    "print(len(en_train_neg_tweets))\n",
    "print(len(test_pos_tweets))\n",
    "print(len(en_test_pos_tweets))\n",
    "print(len(test_neg_tweets))\n",
    "print(len(en_test_neg_tweets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3899,
     "status": "ok",
     "timestamp": 1600469966184,
     "user": {
      "displayName": "Derian Fernando",
      "photoUrl": "",
      "userId": "13994662697554874148"
     },
     "user_tz": 180
    },
    "id": "CWCB3_0n2AVm",
    "outputId": "c7923bdb-1862-4098-b6f2-8e52ab0a6c10",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3147\n3058\n1365\n1326\n"
    }
   ],
   "source": [
    "# Remove duplicate\n",
    "en_train_pos_tweets = list (set(en_train_pos_tweets))\n",
    "en_train_neg_tweets = list (set(en_train_neg_tweets))\n",
    "en_test_pos_tweets = list (set(en_test_pos_tweets))\n",
    "en_test_neg_tweets = list (set(en_test_neg_tweets))\n",
    "print(len(en_train_pos_tweets))\n",
    "print(len(en_train_neg_tweets))\n",
    "print(len(en_test_pos_tweets))\n",
    "print(len(en_test_neg_tweets))\n",
    "# To lowercase\n",
    "en_train_pos_tweets=[x.lower() for x in en_train_pos_tweets]\n",
    "en_train_neg_tweets=[x.lower() for x in en_train_neg_tweets]\n",
    "en_test_pos_tweets=[x.lower() for x in en_test_pos_tweets]\n",
    "en_test_neg_tweets=[x.lower() for x in en_test_neg_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2282,
     "status": "ok",
     "timestamp": 1600480669156,
     "user": {
      "displayName": "Derian Fernando",
      "photoUrl": "",
      "userId": "13994662697554874148"
     },
     "user_tz": 180
    },
    "id": "D6OJduna_adr",
    "outputId": "f389e88d-4281-46d6-fa30-2fa285c28cdf",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/jamelly/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "#pre processing\n",
    "import re\n",
    "import wordsegment\n",
    "import autocorrect\n",
    "from wordsegment import load, segment\n",
    "load()\n",
    "from autocorrect import Speller\n",
    "spell=Speller()\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "punctuations = '''!()-![]{};:+'\"\\,<>./?@#$%^&*_~'''\n",
    "testTweet1='@PERKSOFNIALLJH RTed! Goood Luck :)'\n",
    "testTweet2='My legs hurt so bad :))))))))))))))))))'\n",
    "testTweet3='#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris trouble for being top engaged members in my community this week :)'\n",
    "testTweet4='@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!'\n",
    "def pre_process (tweets):\n",
    "  for tweet in tweets:  \n",
    "    tweet = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \"\", tweet, flags=re.MULTILINE) # to remove links that start with HTTP/HTTPS in the tweet\n",
    "    tweet = re.sub(r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \"\", tweet, flags=re.MULTILINE) # to remove other url links\n",
    "    #tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\"\",tweet).split()) #remove '#' and '@'\n",
    "    tweet = re.sub(r\"RT\", ' ', tweet, flags=re.MULTILINE) #remove 'RT'\n",
    "    tweet = re.sub(r\"#(\\w+)\", ' ', tweet, flags=re.MULTILINE) #remove '#'\n",
    "    tweet = re.sub(r\"@(\\w+)\", ' ', tweet, flags=re.MULTILINE) #remove '@'\n",
    "    tweet = ''.join([i for i in tweet if not i in punctuations]) #remove ponctuations\n",
    "    tweet = ' '.join(segment(tweet)) #Remove repeated letters\n",
    "    tweet = ' '.join([spell (w) for w in tweet.split()]) #Avoiding misspellings and slang words\n",
    "    tweet = re.sub(r\"\\d\", \"\", tweet) #remove digits\n",
    "    tweet = tweet.lower()\n",
    "    #print(tweet)\n",
    "    tokens = tknzr.tokenize(tweet) #tokenize\n",
    "    #tweet = (\" \").join([word for word in tokens if not word in stopwords.words()]) # stopwords filter\n",
    "    tokens = [word for word in tokens if not word in stopwords.words()]\n",
    "    tweet = (\" \").join(stemmer.stem(token) for token in tokens)\n",
    "    #print(tweet)\n",
    "pre_process([testTweet1,testTweet2,testTweet3,testTweet4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(en_train_pos_tweets)\n",
    "pre_process(en_train_neg_tweets)\n",
    "pre_process(en_test_pos_tweets)\n",
    "pre_process(en_test_neg_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.75, ngram_range=(1,3), max_features=10000)\n",
    "X_1 = vectorizer.fit_transform(en_train_pos_tweets)\n",
    "X_2 = vectorizer.fit_transform(en_train_neg_tweets)\n",
    "X_3 = vectorizer.fit_transform(en_test_pos_tweets)\n",
    "X_4 = vectorizer.fit_transform(en_test_neg_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "decision_tree.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
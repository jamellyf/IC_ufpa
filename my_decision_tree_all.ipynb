{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_decision_tree",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELTksXUpFZe-"
      },
      "source": [
        "# Interesting links: \n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
        "# https://medium.com/swlh/sentiment-classification-for-restaurant-reviews-using-tf-idf-42f707bfe44d\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "# https://medium.com/analytics-vidhya/complete-guide-to-machine-learning-evaluation-metrics-615c2864d916"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Hwms6UN_mS",
        "outputId": "70396a97-2092-4e3e-d4f1-d163bdfeaab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "pip install nltk pycld2 wordsegment autocorrect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Collecting pycld2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4MB 100kB/s \n",
            "\u001b[?25hCollecting wordsegment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/6c/e6f4734d6f7d28305f52ec81377d7ce7d1856b97b814278e9960183235ad/wordsegment-1.3.1-py2.py3-none-any.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 45.3MB/s \n",
            "\u001b[?25hCollecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e9/138a0f19b9c02f3e1795f24940c9e771ff262a4e6f9585f05e9da09db658/autocorrect-2.1.0.tar.gz (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Building wheels for collected packages: pycld2, autocorrect\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp36-cp36m-linux_x86_64.whl size=9833517 sha256=ecda3280d7f3567abc2e5a3580f927b50982b838cba39518a9272fedafcb57c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.1.0-cp36-none-any.whl size=1811979 sha256=55ecd500df7c2a7ca02683e707095d5a5850b1fb92968d08ec3937e5fd4774df\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/33/69/a42db65d595bb685429c788ab45e11aa2a4b6549298ee38e02\n",
            "Successfully built pycld2 autocorrect\n",
            "Installing collected packages: pycld2, wordsegment, autocorrect\n",
            "Successfully installed autocorrect-2.1.0 pycld2-0.41 wordsegment-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ReWfACN6Zl",
        "outputId": "6a241026-05ce-4963-ade3-48064ce6d12e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples\n",
        "twitter_samples.fileids()\n",
        "positive_tweets = [t for t in twitter_samples.strings(\"positive_tweets.json\")]\n",
        "negative_tweets = [t for t in twitter_samples.strings(\"negative_tweets.json\")] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrT7XWNwDwpQ",
        "outputId": "d07f69a4-3fd6-4f50-b769-5d71d4d52506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus).toarray()\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.shape)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "(4, 9)\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODi-rkNCU1y0",
        "outputId": "70a2984c-d962-459f-aa25-8eda1d7f90e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(positive_tweets))\n",
        "print(len(negative_tweets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdu9utDSWVLa"
      },
      "source": [
        "train_pos_tweets = positive_tweets[:3500]\n",
        "test_pos_tweets = positive_tweets[3500:]\n",
        "train_neg_tweets = negative_tweets[:3500]\n",
        "test_neg_tweets = negative_tweets[3500:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1XNnj8gxeb3",
        "outputId": "3a231e94-b099-4552-d3cf-cdd6f0184b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_pos_tweets[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQRBjYf1xnUL",
        "outputId": "0e016650-e085-4a8a-a654-9bdaf325cbfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_neg_tweets[7])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnpGNGhHWz9Y",
        "outputId": "5419fd27-2eeb-49be-c7a7-0daf013c6b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(len(train_pos_tweets))\n",
        "print(len(test_pos_tweets))\n",
        "print(len(train_neg_tweets))\n",
        "print(len(test_neg_tweets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3500\n",
            "1500\n",
            "3500\n",
            "1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRvzAc5wXJf2",
        "outputId": "f3adb6ac-7660-43ee-838d-a9f2bb8d7021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(train_pos_tweets + train_neg_tweets))\n",
        "print(len(test_pos_tweets + test_neg_tweets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7000\n",
            "3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z_F-R2KsLO6"
      },
      "source": [
        "# import pycld2 as cld2\n",
        "\n",
        "# def extract_engTweets(tweets):\n",
        "#     en_tweets = []\n",
        "#     for e in tweets:\n",
        "#       details = cld2.detect(e)\n",
        "#       # Extracts only the string with lang name i.e: 'en'\n",
        "#       if details[2][0][1] == 'en': \n",
        "#         en_tweets.append(e)\n",
        "#     return en_tweets\n",
        "\n",
        "# en_train_pos_tweets = extract_engTweets(train_pos_tweets)\n",
        "# en_train_neg_tweets = extract_engTweets(train_neg_tweets)\n",
        "# en_test_pos_tweets = extract_engTweets(test_pos_tweets)\n",
        "# en_test_neg_tweets = extract_engTweets(test_neg_tweets)\n",
        "# print(len(train_pos_tweets))\n",
        "# print(len(en_train_pos_tweets))\n",
        "# print(len(train_neg_tweets))\n",
        "# print(len(en_train_neg_tweets))\n",
        "# print(len(test_pos_tweets))\n",
        "# print(len(en_test_pos_tweets))\n",
        "# print(len(test_neg_tweets))\n",
        "# print(len(en_test_neg_tweets))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBwmdzQ3f5tr",
        "outputId": "e00f85c4-4831-4e2f-fb6e-8af1e92521ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#pre processing\n",
        "import re\n",
        "import wordsegment\n",
        "import autocorrect\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "from autocorrect import Speller\n",
        "spell=Speller()\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "punctuations = '''!()-![]{};:+'\"\\,<>./?@#$%^&*_~'''\n",
        "testTweet1='@PERKSOFNIALLJH RTed! Goood Luck :)'\n",
        "testTweet2='My legs hurt so bad :))))))))))))))))))'\n",
        "testTweet3='#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris trouble for being top engaged members in my community this week :)'\n",
        "testTweet4='@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!'\n",
        "\n",
        "def pre_process (tweets):\n",
        "  result_tweets = []\n",
        "  for tweet in tweets:  \n",
        "    # print(f'raw tweet: {tweet}')\n",
        "    tweet = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \"\", tweet, flags=re.MULTILINE) # to remove links that start with HTTP/HTTPS in the tweet\n",
        "    tweet = re.sub(r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \"\", tweet, flags=re.MULTILINE) # to remove other url links\n",
        "    tweet = re.sub(r\"RT\", ' ', tweet, flags=re.MULTILINE) #remove 'RT'\n",
        "    tweet = re.sub(r\"#(\\w+)\", ' ', tweet, flags=re.MULTILINE) #remove '#'\n",
        "    tweet = re.sub(r\"@(\\w+)\", ' ', tweet, flags=re.MULTILINE) #remove '@'\n",
        "    tweet = ''.join([i for i in tweet if not i in punctuations]) #remove ponctuations\n",
        "    tweet = ' '.join(segment(tweet)) #Remove repeated letters\n",
        "    tweet = ' '.join([spell(w) for w in tweet.split()]) #Avoiding misspellings and slang words\n",
        "    tweet = re.sub(r\"\\d\", \"\", tweet) #remove digits\n",
        "    tweet = tweet.lower()\n",
        "    tokens = tknzr.tokenize(tweet) #tokenize\n",
        "    tokens = [word for word in tokens if not word in stopwords.words()]\n",
        "    tweet = (\" \").join(stemmer.stem(token) for token in tokens)\n",
        "    # print(f'preprocessed tweet: {tweet}')\n",
        "    result_tweets.append(tweet)\n",
        "  \n",
        "  return result_tweets\n",
        "\n",
        "# proc_en_train_pos_tweets = pre_process([testTweet1,testTweet2,testTweet3,testTweet4])\n",
        "# proc_en_train_pos_tweets = pre_process([en_train_pos_tweets[0]])\n",
        "# proc_en_train_neg_tweets = pre_process([en_train_neg_tweets[0]])\n",
        "proc_train_pos_tweets = pre_process(train_pos_tweets)\n",
        "proc_train_neg_tweets = pre_process(train_neg_tweets)\n",
        "proc_test_pos_tweets = pre_process(test_pos_tweets)\n",
        "proc_test_neg_tweets = pre_process(test_neg_tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fgGxJDsDyW7",
        "outputId": "e1e9836e-5e68-45f7-ab4a-b772b0abeeda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(len(train_pos_tweets))\n",
        "print(len(test_pos_tweets))\n",
        "print(len(train_neg_tweets))\n",
        "print(len(test_neg_tweets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3500\n",
            "1500\n",
            "3500\n",
            "1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nEnwpH_1Pxo"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=1, max_df=0.75, ngram_range=(1,3), max_features=10000)\n",
        "features_train_pos = vectorizer.fit_transform(proc_train_pos_tweets).toarray()\n",
        "features_train_neg = vectorizer.fit_transform(proc_train_neg_tweets).toarray()\n",
        "features_test_pos = vectorizer.fit_transform(proc_test_pos_tweets).toarray()\n",
        "features_test_neg = vectorizer.fit_transform(proc_test_neg_tweets).toarray()\n",
        "# print(f'features_train_pos: {features_train_pos}')\n",
        "# print(f'type(features_train_pos): {type(features_train_pos)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvc4UsGRgqwh",
        "outputId": "32b6dd8c-8324-4729-c920-077458366c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(f'features_train_pos.shape: {features_train_pos.shape}')\n",
        "print(f'features_train_neg.shape: {features_train_neg.shape}')\n",
        "print(f'features_test_pos.shape: {features_test_pos.shape}')\n",
        "print(f'features_test_neg.shape: {features_test_neg.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features_train_pos.shape: (3500, 10000)\n",
            "features_train_neg.shape: (3500, 10000)\n",
            "features_test_pos.shape: (1500, 10000)\n",
            "features_test_neg.shape: (1500, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QElcHuMj5Dxh"
      },
      "source": [
        "import numpy as np\n",
        "all_tweets = np.concatenate((features_train_pos,features_train_neg,features_test_pos,features_test_neg))\n",
        "train_features = np.concatenate((features_train_pos,features_train_neg))\n",
        "test_features = np.concatenate((features_test_pos,features_test_neg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM4zRH_t1Tti"
      },
      "source": [
        "## Scoring and labeling the tweets\n",
        "# First calculate the overall polarity score\n",
        "# Assume only positive and negative\n",
        "# Classify via the number of neg/pos words and emoticons\n",
        "\n",
        "# 'tweet is expected to be an integer iterable' \n",
        "def polarity_score(tweet):\n",
        "  score = 0\n",
        "  for feature in tweet:\n",
        "    score += feature\n",
        "  return score\n",
        "\n",
        "# 'pol_score' is expected to be a scalar\n",
        "def determine_sentiment(pol_score):\n",
        "  # Highly positive: +2 \n",
        "  if pol_score >= 2 and pol_score <= 4:\n",
        "    return 2 \n",
        "  # Moderate positive: +1 \n",
        "  if pol_score > 0 and pol_score < 2:\n",
        "    return 1\n",
        "  # Moderate negative: -1 \n",
        "  if pol_score > -2 and pol_score < 0:\n",
        "    return -1 \n",
        "  # Highly negative: -2 \n",
        "  if pol_score >= -4 and pol_score <= -2:\n",
        "    return -2\n",
        "  if pol_score == 0: \n",
        "    return 0\n",
        "  else:#Tirar depois\n",
        "    return 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGZXFeqF53_s"
      },
      "source": [
        "## Testing the balancing and scoring\n",
        "# Classify the tweets\n",
        "def score_tweets(tweets):\n",
        "  tweet, length = tweets.shape\n",
        "  tweets_class = []\n",
        "  for i in range(tweet):\n",
        "    score = polarity_score(tweets[i,:])\n",
        "    sentiment = determine_sentiment(score)\n",
        "    tweets_class.append(sentiment)\n",
        "  return tweets_class\n",
        "\n",
        "all_tweets_classes = np.array(score_tweets(all_tweets))\n",
        "train_classes = np.array(score_tweets(train_features))\n",
        "test_classes = np.array(score_tweets(test_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES0uNivrkT3p",
        "outputId": "ed74d60b-8bbb-4b3e-ef8a-5f3bbc10c484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "#Table 1 - Polarity of tweets\n",
        "high_positive = 0\n",
        "moderate_positive = 0\n",
        "neutral = 0\n",
        "moderate_negative = 0\n",
        "high_negative = 0\n",
        "\n",
        "for t in all_tweets_classes:\n",
        "  if t == 2:\n",
        "    high_positive += 1\n",
        "  if t == 1:\n",
        "    moderate_positive += 1\n",
        "  if t == 0:\n",
        "    neutral += 1\n",
        "  if t == -1:\n",
        "    moderate_negative += 1\n",
        "  if t == -2:\n",
        "    high_negative += 1\n",
        "\n",
        "hp_ratio = (high_positive/all_tweets_classes.size)*100\n",
        "mp_ratio = (moderate_positive/all_tweets_classes.size)*100\n",
        "neutral_ratio = (neutral/all_tweets_classes.size)*100\n",
        "mn_ratio = (moderate_negative/all_tweets_classes.size)*100\n",
        "hn_ratio = (high_negative/all_tweets_classes.size)*100\n",
        "\n",
        "print(f'high_positive: {high_positive} ({hp_ratio}%)')\n",
        "print(f'moderate_positive: {moderate_positive} ({mp_ratio}%)')\n",
        "print(f'neutral: {neutral} ({neutral_ratio}%)')\n",
        "print(f'moderate_negative: {moderate_negative} ({mn_ratio}%)')\n",
        "print(f'high_negative: {high_negative} ({hn_ratio}%)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "high_positive: 6154 (61.53999999999999%)\n",
            "moderate_positive: 3579 (35.79%)\n",
            "neutral: 267 (2.67%)\n",
            "moderate_negative: 0 (0.0%)\n",
            "high_negative: 0 (0.0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqM3EwQ1XlY5"
      },
      "source": [
        "# Decision tree implementation\n",
        "from sklearn import tree\n",
        "\n",
        "decision_tree = tree.DecisionTreeClassifier(n_jobs=-1)\n",
        "decision_tree.fit(train_features, train_classes)\n",
        "y_predict = decision_tree.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwwMA3Wrt1mQ"
      },
      "source": [
        "# Random forest implementation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier(n_jobs=-1)\n",
        "random_forest.fit(train_features, train_classes)\n",
        "y_predict = random_forest.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyBxpMC7aObw"
      },
      "source": [
        "# SVR implementation\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "svr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
        "svr.fit(train_features, train_classes)\n",
        "y_predict = svr.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbB4acCk2HDJ"
      },
      "source": [
        "# Multinomial Logistic Regression (Softmax)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "softmax = LogisticRegression(multi_class='multinomial')\n",
        "softmax.fit(train_features, train_classes)\n",
        "y_predict = softmax.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmmpp84Ja6zy",
        "outputId": "da360901-0719-45c2-a1cc-6e00fa14b89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(test_classes.shape)\n",
        "print(y_predict.shape)\n",
        "print(test_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3000,)\n",
            "(3000,)\n",
            "(3000, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX5EZWVfM2Y3",
        "outputId": "ac0432e8-8462-4029-b7bf-cc2213338f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = test_classes\n",
        "y_pred = y_predict \n",
        "target_names = ['Neutral', 'Moderate Positive', 'High Positive']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "          Neutral       0.03      1.00      0.05        46\n",
            "Moderate Positive       0.29      0.12      0.17       919\n",
            "    High Positive       0.91      0.40      0.56      2035\n",
            "\n",
            "         accuracy                           0.32      3000\n",
            "        macro avg       0.41      0.51      0.26      3000\n",
            "     weighted avg       0.71      0.32      0.43      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InwudDjdklra",
        "outputId": "dd1fd2a8-5029-47c8-bc42-ac431662c0bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# 10-fold cross-validation\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# prepare the cross-validation procedure\n",
        "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "# clf = decision_tree ou random_forest ou svr ou softmax\n",
        "scores = cross_val_score(clf, train_features, train_classes, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# Report performance\n",
        "print(f'scores.shape{scores.shape}')\n",
        "print(f'scores{scores}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scores.shape(10,)\n",
            "scores[0.8        0.79285714 0.78714286 0.78857143 0.80142857 0.79285714\n",
            " 0.80142857 0.80142857 0.79       0.77857143]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_decision_tree",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Hwms6UN_mS",
        "outputId": "e82b2de1-dd5f-47fa-b43f-88a18bf23f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "pip install nltk pycld2 wordsegment autocorrect"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Collecting pycld2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4MB 113kB/s \n",
            "\u001b[?25hCollecting wordsegment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/6c/e6f4734d6f7d28305f52ec81377d7ce7d1856b97b814278e9960183235ad/wordsegment-1.3.1-py2.py3-none-any.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 35.8MB/s \n",
            "\u001b[?25hCollecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e9/138a0f19b9c02f3e1795f24940c9e771ff262a4e6f9585f05e9da09db658/autocorrect-2.1.0.tar.gz (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 34.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Building wheels for collected packages: pycld2, autocorrect\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp36-cp36m-linux_x86_64.whl size=9833500 sha256=d2b9eb1a5e23dd98b140ea75fb066a9486206816c61b961cb55032aff080a89a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.1.0-cp36-none-any.whl size=1811979 sha256=3e7bcdc982ebedde84cd5f24be6622697d27337bb2ee50388c564318a253bc02\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/33/69/a42db65d595bb685429c788ab45e11aa2a4b6549298ee38e02\n",
            "Successfully built pycld2 autocorrect\n",
            "Installing collected packages: pycld2, wordsegment, autocorrect\n",
            "Successfully installed autocorrect-2.1.0 pycld2-0.41 wordsegment-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ReWfACN6Zl",
        "outputId": "214a2439-6ba7-4b38-a385-f18a789d8c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples\n",
        "twitter_samples.fileids()\n",
        "positive_tweets = [t for t in twitter_samples.strings(\"positive_tweets.json\")]\n",
        "negative_tweets = [t for t in twitter_samples.strings(\"negative_tweets.json\")] "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODi-rkNCU1y0",
        "outputId": "8e8642a3-2b91-4d82-a6bc-87787436640f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(positive_tweets))\n",
        "print(len(negative_tweets))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdu9utDSWVLa"
      },
      "source": [
        "train_pos_tweets = positive_tweets[:3500]\n",
        "test_pos_tweets = positive_tweets[3500:]\n",
        "train_neg_tweets = negative_tweets[:3500]\n",
        "test_neg_tweets = negative_tweets[3500:]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1XNnj8gxeb3",
        "outputId": "9f97d059-3844-4b12-d98c-48138c9b696c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_pos_tweets[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQRBjYf1xnUL",
        "outputId": "19e4ed0a-3790-48e1-d6a4-81617c881c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_neg_tweets[7])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnpGNGhHWz9Y",
        "outputId": "c6bd3897-c58f-47cf-e7c2-dfe035bb28e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(len(train_pos_tweets))\n",
        "print(len(test_pos_tweets))\n",
        "print(len(train_neg_tweets))\n",
        "print(len(test_neg_tweets))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3500\n",
            "1500\n",
            "3500\n",
            "1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRvzAc5wXJf2",
        "outputId": "f0ea28bb-00fc-43d8-ff53-cc9c6dfed253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(train_pos_tweets + train_neg_tweets))\n",
        "print(len(test_pos_tweets + test_neg_tweets))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7000\n",
            "3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z_F-R2KsLO6"
      },
      "source": [
        "# import pycld2 as cld2\n",
        "\n",
        "# def extract_engTweets(tweets):\n",
        "#     en_tweets = []\n",
        "#     for e in tweets:\n",
        "#       details = cld2.detect(e)\n",
        "#       # Extracts only the string with lang name i.e: 'en'\n",
        "#       if details[2][0][1] == 'en': \n",
        "#         en_tweets.append(e)\n",
        "#     return en_tweets\n",
        "\n",
        "# en_train_pos_tweets = extract_engTweets(train_pos_tweets)\n",
        "# en_train_neg_tweets = extract_engTweets(train_neg_tweets)\n",
        "# en_test_pos_tweets = extract_engTweets(test_pos_tweets)\n",
        "# en_test_neg_tweets = extract_engTweets(test_neg_tweets)\n",
        "# print(len(train_pos_tweets))\n",
        "# print(len(en_train_pos_tweets))\n",
        "# print(len(train_neg_tweets))\n",
        "# print(len(en_train_neg_tweets))\n",
        "# print(len(test_pos_tweets))\n",
        "# print(len(en_test_pos_tweets))\n",
        "# print(len(test_neg_tweets))\n",
        "# print(len(en_test_neg_tweets))\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBwmdzQ3f5tr",
        "outputId": "937ea0fb-94c3-4949-9687-d0a6c9cf2c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#pre processing\n",
        "import re\n",
        "import wordsegment\n",
        "import autocorrect\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "from autocorrect import Speller\n",
        "spell=Speller()\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "punctuations = '''!()-![]{};:+'\"\\,<>./?@#$%^&*_~'''\n",
        "testTweet1='@PERKSOFNIALLJH RTed! Goood Luck :)'\n",
        "testTweet2='My legs hurt so bad :))))))))))))))))))'\n",
        "testTweet3='#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris trouble for being top engaged members in my community this week :)'\n",
        "testTweet4='@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!'\n",
        "\n",
        "def pre_process (tweets):\n",
        "  result_tweets = []\n",
        "  for tweet in tweets:  \n",
        "    # print(f'raw tweet: {tweet}')\n",
        "    tweet = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \"\", tweet, flags=re.MULTILINE) # to remove links that start with HTTP/HTTPS in the tweet\n",
        "    tweet = re.sub(r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \"\", tweet, flags=re.MULTILINE) # to remove other url links\n",
        "    tweet = re.sub(r\"RT\", ' ', tweet, flags=re.MULTILINE) #remove 'RT'\n",
        "    tweet = re.sub(r\"#(\\w+)\", ' ', tweet, flags=re.MULTILINE) #remove '#'\n",
        "    tweet = re.sub(r\"@(\\w+)\", ' ', tweet, flags=re.MULTILINE) #remove '@'\n",
        "    tweet = ''.join([i for i in tweet if not i in punctuations]) #remove ponctuations\n",
        "    tweet = ' '.join(segment(tweet)) #Remove repeated letters\n",
        "    tweet = ' '.join([spell(w) for w in tweet.split()]) #Avoiding misspellings and slang words\n",
        "    tweet = re.sub(r\"\\d\", \"\", tweet) #remove digits\n",
        "    tweet = tweet.lower()\n",
        "    tokens = tknzr.tokenize(tweet) #tokenize\n",
        "    tokens = [word for word in tokens if not word in stopwords.words()]\n",
        "    tweet = (\" \").join(stemmer.stem(token) for token in tokens)\n",
        "    # print(f'preprocessed tweet: {tweet}')\n",
        "    result_tweets.append(tweet)\n",
        "  \n",
        "  return result_tweets\n",
        "\n",
        "# proc_en_train_pos_tweets = pre_process([testTweet1,testTweet2,testTweet3,testTweet4])\n",
        "# proc_en_train_pos_tweets = pre_process([en_train_pos_tweets[0]])\n",
        "# proc_en_train_neg_tweets = pre_process([en_train_neg_tweets[0]])\n",
        "proc_train_pos_tweets = pre_process(train_pos_tweets)\n",
        "proc_train_neg_tweets = pre_process(train_neg_tweets)\n",
        "proc_test_pos_tweets = pre_process(test_pos_tweets)\n",
        "proc_test_neg_tweets = pre_process(test_neg_tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fgGxJDsDyW7"
      },
      "source": [
        "print(len(train_pos_tweets))\n",
        "print(len(test_pos_tweets))\n",
        "print(len(train_neg_tweets))\n",
        "print(len(test_neg_tweets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nEnwpH_1Pxo"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=1, max_df=0.75, ngram_range=(1,3), max_features=10000)\n",
        "features_train_pos = vectorizer.fit_transform(proc_train_pos_tweets).toarray()\n",
        "features_train_neg = vectorizer.fit_transform(proc_train_neg_tweets).toarray()\n",
        "features_test_pos = vectorizer.fit_transform(proc_test_pos_tweets).toarray()\n",
        "features_test_neg = vectorizer.fit_transform(proc_test_neg_tweets).toarray()\n",
        "# print(f'features_train_pos: {features_train_pos}')\n",
        "# print(f'type(features_train_pos): {type(features_train_pos)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvc4UsGRgqwh"
      },
      "source": [
        "print(f'features_train_pos.shape: {features_train_pos.shape}')\n",
        "print(f'features_train_neg.shape: {features_train_neg.shape}')\n",
        "print(f'features_test_pos.shape: {features_test_pos.shape}')\n",
        "print(f'features_test_neg.shape: {features_test_neg.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QElcHuMj5Dxh"
      },
      "source": [
        "import numpy as np\n",
        "all_tweets = np.concatenate((features_train_pos,features_train_neg,features_test_pos,features_test_neg))\n",
        "train_features = np.concatenate((features_train_pos,features_train_neg))\n",
        "test_features = np.concatenate((features_test_pos,features_test_neg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM4zRH_t1Tti"
      },
      "source": [
        "## Scoring and labeling the tweets\n",
        "# First calculate the overall polarity score\n",
        "# Assume only positive and negative\n",
        "# Classify via the number of neg/pos words and emoticons\n",
        "\n",
        "# 'tweet is expected to be an integer iterable' \n",
        "def polarity_score(tweet):\n",
        "  score = 0\n",
        "  for feature in tweet:\n",
        "    score += feature\n",
        "  return score\n",
        "\n",
        "# 'pol_score' is expected to be a scalar\n",
        "def determine_sentiment(pol_score):\n",
        "  # Highly positive: +2 \n",
        "  if pol_score >= 2 and pol_score <= 4:\n",
        "    return 2 \n",
        "  # Moderate positive: +1 \n",
        "  if pol_score > 0 and pol_score < 2:\n",
        "    return 1\n",
        "  # Moderate negative: -1 \n",
        "  if pol_score > -2 and pol_score < 0:\n",
        "    return -1 \n",
        "  # Highly negative: -2 \n",
        "  if pol_score >= -4 and pol_score <= -2:\n",
        "    return -2\n",
        "  if pol_score == 0: \n",
        "    return 0\n",
        "  else:#Tirar depois\n",
        "    return 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGZXFeqF53_s"
      },
      "source": [
        "## Testing the balancing and scoring\n",
        "# Classify the tweets\n",
        "def score_tweets(tweets):\n",
        "  tweet, length = tweets.shape\n",
        "  tweets_class = []\n",
        "  for i in range(tweet):\n",
        "    score = polarity_score(tweets[i,:])\n",
        "    sentiment = determine_sentiment(score)\n",
        "    tweets_class.append(sentiment)\n",
        "  return tweets_class\n",
        "\n",
        "all_tweets_classes = np.array(score_tweets(all_tweets))\n",
        "train_classes = np.array(score_tweets(train_features))\n",
        "test_classes = np.array(score_tweets(test_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES0uNivrkT3p"
      },
      "source": [
        "#Table 1 - Polarity of tweets\n",
        "high_positive = 0\n",
        "moderate_positive = 0\n",
        "neutral = 0\n",
        "moderate_negative = 0\n",
        "high_negative = 0\n",
        "\n",
        "for t in all_tweets_classes:\n",
        "  if t == 2:\n",
        "    high_positive += 1\n",
        "  if t == 1:\n",
        "    moderate_positive += 1\n",
        "  if t == 0:\n",
        "    neutral += 1\n",
        "  if t == -1:\n",
        "    moderate_negative += 1\n",
        "  if t == -2:\n",
        "    high_negative += 1\n",
        "\n",
        "hp_ratio = (high_positive/all_tweets_classes.size)*100\n",
        "mp_ratio = (moderate_positive/all_tweets_classes.size)*100\n",
        "neutral_ratio = (neutral/all_tweets_classes.size)*100\n",
        "mn_ratio = (moderate_negative/all_tweets_classes.size)*100\n",
        "hn_ratio = (high_negative/all_tweets_classes.size)*100\n",
        "\n",
        "print(f'high_positive: {high_positive} ({hp_ratio}%)')\n",
        "print(f'moderate_positive: {moderate_positive} ({mp_ratio}%)')\n",
        "print(f'neutral: {neutral} ({neutral_ratio}%)')\n",
        "print(f'moderate_negative: {moderate_negative} ({mn_ratio}%)')\n",
        "print(f'high_negative: {high_negative} ({hn_ratio}%)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-5wBkew2JES"
      },
      "source": [
        "all_tweets_classes[0] == 2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAvD5LtgUy-1"
      },
      "source": [
        "# # Decision tree implementation\n",
        "# from sklearn import tree\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.datasets import load_iris # remove after real values come from Jamelly\n",
        "# from sklearn.model_selection import train_test_split # remove after real values come from Jamelly\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# # prepare the cross-validation procedure\n",
        "# cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "# ## create model\n",
        "# #model = LogisticRegression()\n",
        "# ## evaluate model\n",
        "# #scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# ## report performance\n",
        "# #print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "\n",
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# y = iris.target\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# x = X_train\n",
        "# y = y_train\n",
        "\n",
        "# clf = tree.DecisionTreeClassifier()\n",
        "\n",
        "# clf.fit(x,y)\n",
        "# clf.predict(X_test)\n",
        "# print(x.shape)\n",
        "# print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3eUY7NhZmfn"
      },
      "source": [
        "train_classes[0:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRRim-41aHXn"
      },
      "source": [
        "np.isnan(train_features).any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqM3EwQ1XlY5"
      },
      "source": [
        "# Decision tree implementation\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(all_tweets, all_tweets_classes, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "\n",
        "clf.fit(train_features, train_classes)\n",
        "y_predict = clf.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InwudDjdklra"
      },
      "source": [
        "# 10-fold cross-validation\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# prepare the cross-validation procedure\n",
        "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "scores = cross_val_score(clf, train_features, train_classes, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "## report performance\n",
        "# print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "print(f'scores.shape{scores.shape}')\n",
        "print(f'scores{scores}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmmpp84Ja6zy"
      },
      "source": [
        "print(test_classes.shape)\n",
        "print(y_predict.shape)\n",
        "print(test_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX5EZWVfM2Y3"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# y_true = [0, 1, 2, 2, 2]\n",
        "# y_pred = [0, 0, 2, 2, 1]\n",
        "# target_names = ['class 0', 'class 1', 'class 2']\n",
        "y_true = test_classes\n",
        "y_pred = y_predict \n",
        "target_names = ['Neutral', 'Moderate Positive', 'High Positive']\n",
        "# target_names = [0, 1, 2]\n",
        "# print(classification_report(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaQ-RhxPrTpb"
      },
      "source": [
        "# Brief explanation about machine learning validation metrics\n",
        "# https://medium.com/analytics-vidhya/complete-guide-to-machine-learning-evaluation-metrics-615c2864d916\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5en54I2Hkx7l"
      },
      "source": [
        "## create model\n",
        "#model = LogisticRegression()\n",
        "## evaluate model\n",
        "#scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "## report performance\n",
        "#print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BFXSA-VaQ16"
      },
      "source": [
        "# print(f'proc_en_train_pos_tweets: {len(proc_en_train_pos_tweets)}')\n",
        "# print(f'en_train_pos_classes: {en_train_pos_classes.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELTksXUpFZe-"
      },
      "source": [
        "# Calculate model precision, recall, f1-score and support\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dd2BOXaPh8O"
      },
      "source": [
        "# https://medium.com/swlh/sentiment-classification-for-restaurant-reviews-using-tf-idf-42f707bfe44d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPgMcqRuIVZu"
      },
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
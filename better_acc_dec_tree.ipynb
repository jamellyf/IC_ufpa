{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_dec_tree",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELTksXUpFZe-"
      },
      "source": [
        "# Interesting links: \n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
        "# https://medium.com/swlh/sentiment-classification-for-restaurant-reviews-using-tf-idf-42f707bfe44d\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "# https://medium.com/analytics-vidhya/complete-guide-to-machine-learning-evaluation-metrics-615c2864d916"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Hwms6UN_mS",
        "outputId": "71519449-ddef-4437-dd39-5c1aaa65020e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "pip install nltk pycld2 wordsegment autocorrect"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: pycld2 in /usr/local/lib/python3.6/dist-packages (0.41)\n",
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ReWfACN6Zl",
        "outputId": "e4c1f291-982f-4f83-d89a-6b27732e3725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples\n",
        "twitter_samples.fileids()\n",
        "positive_tweets = [t for t in twitter_samples.strings(\"positive_tweets.json\")]\n",
        "negative_tweets = [t for t in twitter_samples.strings(\"negative_tweets.json\")] "
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrT7XWNwDwpQ",
        "outputId": "09bf17a8-5091-443f-fad6-cf138677cc46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus).toarray()\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.shape)\n",
        "print(X)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "(4, 9)\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODi-rkNCU1y0",
        "outputId": "d2cd6cc7-32ba-4fb1-dbc6-3097b83ca649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(positive_tweets))\n",
        "print(len(negative_tweets))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdu9utDSWVLa"
      },
      "source": [
        "train_pos_tweets = positive_tweets[:3500]\n",
        "test_pos_tweets = positive_tweets[3500:]\n",
        "train_neg_tweets = negative_tweets[:3500]\n",
        "test_neg_tweets = negative_tweets[3500:]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1XNnj8gxeb3",
        "outputId": "4b441f4d-5acf-41f3-ccd4-e82e2cbdd9e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_pos_tweets[0])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQRBjYf1xnUL",
        "outputId": "22c032ed-6888-473b-e91a-3a7c0de61478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_neg_tweets[7])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnpGNGhHWz9Y",
        "outputId": "56a49272-eb4a-4d63-9cda-aad8a1c9731b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(len(train_pos_tweets))\n",
        "print(len(test_pos_tweets))\n",
        "print(len(train_neg_tweets))\n",
        "print(len(test_neg_tweets))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3500\n",
            "1500\n",
            "3500\n",
            "1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRvzAc5wXJf2",
        "outputId": "fa651ee9-f036-49cc-fc3d-47c521e7097c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(train_pos_tweets + train_neg_tweets))\n",
        "print(len(test_pos_tweets + test_neg_tweets))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7000\n",
            "3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z_F-R2KsLO6"
      },
      "source": [
        "# import pycld2 as cld2\n",
        "\n",
        "# def extract_engTweets(tweets):\n",
        "#     en_tweets = []\n",
        "#     for e in tweets:\n",
        "#       details = cld2.detect(e)\n",
        "#       # Extracts only the string with lang name i.e: 'en'\n",
        "#       if details[2][0][1] == 'en': \n",
        "#         en_tweets.append(e)\n",
        "#     return en_tweets\n",
        "\n",
        "# en_train_pos_tweets = extract_engTweets(train_pos_tweets)\n",
        "# en_train_neg_tweets = extract_engTweets(train_neg_tweets)\n",
        "# en_test_pos_tweets = extract_engTweets(test_pos_tweets)\n",
        "# en_test_neg_tweets = extract_engTweets(test_neg_tweets)\n",
        "# print(len(train_pos_tweets))\n",
        "# print(len(en_train_pos_tweets))\n",
        "# print(len(train_neg_tweets))\n",
        "# print(len(en_train_neg_tweets))\n",
        "# print(len(test_pos_tweets))\n",
        "# print(len(en_test_pos_tweets))\n",
        "# print(len(test_neg_tweets))\n",
        "# print(len(en_test_neg_tweets))\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBwmdzQ3f5tr",
        "outputId": "95fdfa37-a148-4261-d18a-9410a94b5771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#pre processing\n",
        "import re\n",
        "import wordsegment\n",
        "import autocorrect\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "from autocorrect import Speller\n",
        "spell=Speller()\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "punctuations = '''!()-![]{};:+'\"\\,<>./?@#$%^&*_~'''\n",
        "testTweet1='@PERKSOFNIALLJH RTed! Goood Luck :)'\n",
        "testTweet2='My legs hurt so bad :))))))))))))))))))'\n",
        "testTweet3='#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris trouble for being top engaged members in my community this week :)'\n",
        "testTweet4='@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!'\n",
        "\n",
        "def pre_process (tweets):\n",
        "  result_tweets = []\n",
        "  for tweet in tweets:  \n",
        "    # print(f'raw tweet: {tweet}')\n",
        "    tweet = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \"\", tweet, flags=re.MULTILINE) # to remove links that start with HTTP/HTTPS in the tweet\n",
        "    tweet = re.sub(r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \"\", tweet, flags=re.MULTILINE) # to remove other url links\n",
        "    tweet = re.sub(r\"RT\", ' ', tweet, flags=re.MULTILINE) #remove 'RT'\n",
        "    tweet = re.sub(r\"#(\\w+)\", ' ', tweet, flags=re.MULTILINE) #remove '#'\n",
        "    tweet = re.sub(r\"@(\\w+)\", ' ', tweet, flags=re.MULTILINE) #remove '@'\n",
        "    tweet = ''.join([i for i in tweet if not i in punctuations]) #remove ponctuations\n",
        "    tweet = ' '.join(segment(tweet)) #Remove repeated letters\n",
        "    tweet = ' '.join([spell(w) for w in tweet.split()]) #Avoiding misspellings and slang words\n",
        "    tweet = re.sub(r\"\\d\", \"\", tweet) #remove digits\n",
        "    tweet = tweet.lower()\n",
        "    tokens = tknzr.tokenize(tweet) #tokenize\n",
        "    tokens = [word for word in tokens if not word in stopwords.words()]\n",
        "    tweet = (\" \").join(stemmer.stem(token) for token in tokens)\n",
        "    # print(f'preprocessed tweet: {tweet}')\n",
        "    result_tweets.append(tweet)\n",
        "  \n",
        "  return result_tweets\n",
        "\n",
        "# proc_en_train_pos_tweets = pre_process([testTweet1,testTweet2,testTweet3,testTweet4])\n",
        "# proc_en_train_pos_tweets = pre_process([en_train_pos_tweets[0]])\n",
        "# proc_en_train_neg_tweets = pre_process([en_train_neg_tweets[0]])\n",
        "proc_train_pos_tweets = pre_process(train_pos_tweets)\n",
        "proc_train_neg_tweets = pre_process(train_neg_tweets)\n",
        "proc_test_pos_tweets = pre_process(test_pos_tweets)\n",
        "proc_test_neg_tweets = pre_process(test_neg_tweets)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fgGxJDsDyW7",
        "outputId": "6702c033-469c-41a7-d937-a31a9f3690ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(len(train_pos_tweets))\n",
        "print(len(test_pos_tweets))\n",
        "print(len(train_neg_tweets))\n",
        "print(len(test_neg_tweets))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3500\n",
            "1500\n",
            "3500\n",
            "1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFfBS8c_Itkv"
      },
      "source": [
        "import numpy as np\n",
        "proc_train_tweets = np.concatenate((proc_train_pos_tweets, proc_train_neg_tweets))\n",
        "proc_test_tweets = np.concatenate((proc_test_pos_tweets, proc_test_neg_tweets))\n",
        "proc_pos_tweets = np.concatenate((proc_train_pos_tweets, proc_test_pos_tweets))\n",
        "proc_neg_tweets = np.concatenate((proc_train_neg_tweets, proc_test_neg_tweets))\n",
        "# proc_all_tweets = np.concatenate((proc_train_pos_tweets, proc_train_neg_tweets, \n",
        "#                                   proc_test_pos_tweets, proc_test_neg_tweets))\n",
        "proc_all_tweets = np.concatenate((proc_train_pos_tweets, proc_test_pos_tweets, \n",
        "                                  proc_train_neg_tweets, proc_test_neg_tweets))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nEnwpH_1Pxo"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=5, max_df=0.75, ngram_range=(1,3), max_features=10000)\n",
        "# vectorizer = TfidfVectorizer(min_df=1, max_df=0.75, ngram_range=(1,3), max_features=10000)\n",
        "features_train_pos = vectorizer.fit_transform(proc_train_pos_tweets).toarray()\n",
        "features_train_neg = vectorizer.fit_transform(proc_train_neg_tweets).toarray()\n",
        "features_test_pos = vectorizer.fit_transform(proc_test_pos_tweets).toarray()\n",
        "features_test_neg = vectorizer.fit_transform(proc_test_neg_tweets).toarray()\n",
        "################################################################################\n",
        "features_train = vectorizer.fit_transform(proc_train_tweets).toarray()\n",
        "features_test = vectorizer.fit_transform(proc_test_tweets).toarray()\n",
        "features_pos = vectorizer.fit_transform(proc_pos_tweets).toarray()\n",
        "features_neg = vectorizer.fit_transform(proc_neg_tweets).toarray()\n",
        "\n",
        "features_all = vectorizer.fit_transform(proc_all_tweets).toarray()\n",
        "# print(f'features_train_pos: {features_train_pos}')\n",
        "# print(f'type(features_train_pos): {type(features_train_pos)}')"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvc4UsGRgqwh",
        "outputId": "3e245a61-9040-47b6-e2d7-4b033732cecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "print(f'features_train.shape: {features_train.shape}')\n",
        "print(f'features_test.shape: {features_test.shape}')\n",
        "print(f'features_train.shape: {features_pos.shape}')\n",
        "print(f'features_test.shape: {features_neg.shape}')\n",
        "print(f'features_train_pos.shape: {features_train_pos.shape}')\n",
        "print(f'features_train_neg.shape: {features_train_neg.shape}')\n",
        "print(f'features_test_pos.shape: {features_test_pos.shape}')\n",
        "print(f'features_test_neg.shape: {features_test_neg.shape}')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features_train.shape: (7000, 1430)\n",
            "features_test.shape: (3000, 690)\n",
            "features_train.shape: (5000, 1104)\n",
            "features_test.shape: (5000, 1027)\n",
            "features_train_pos.shape: (3500, 806)\n",
            "features_train_neg.shape: (3500, 752)\n",
            "features_test_pos.shape: (1500, 393)\n",
            "features_test_neg.shape: (1500, 342)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QElcHuMj5Dxh"
      },
      "source": [
        "# import numpy as np\n",
        "# all_tweets = np.concatenate((features_train_pos,features_train_neg,features_test_pos,features_test_neg))\n",
        "# train_features = np.concatenate((features_train_pos,features_train_neg))\n",
        "# test_features = np.concatenate((features_test_pos,features_test_neg))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM4zRH_t1Tti"
      },
      "source": [
        "## Scoring and labeling the tweets\n",
        "# First calculate the overall polarity score\n",
        "# Assume only positive and negative\n",
        "# Classify via the number of neg/pos words and emoticons\n",
        "\n",
        "# 'tweet is expected to be an integer iterable' \n",
        "def polarity_score(tweet):\n",
        "  score = 0\n",
        "  for feature in tweet:\n",
        "    score += feature\n",
        "  return score\n",
        "\n",
        "# 'pol_score' is expected to be a scalar\n",
        "def determine_sentiment(pol_score):\n",
        "  # Highly positive: +2 \n",
        "  if pol_score >= 2 and pol_score <= 4:\n",
        "    return 2 \n",
        "  # Moderate positive: +1 \n",
        "  if pol_score > 0 and pol_score < 2:\n",
        "    return 1\n",
        "  # Moderate negative: -1 \n",
        "  if pol_score > -2 and pol_score < 0:\n",
        "    return -1 \n",
        "  # Highly negative: -2 \n",
        "  if pol_score >= -4 and pol_score <= -2:\n",
        "    return -2\n",
        "  if pol_score == 0: \n",
        "    return 0\n",
        "  else:#Tirar depois\n",
        "    # return 2\n",
        "    return 0"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGZXFeqF53_s",
        "outputId": "7af9217e-d69b-4d51-ee2e-4d798bc95827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## Testing the balancing and scoring\n",
        "# Classify the tweets\n",
        "def score_tweets(tweets):\n",
        "  tweet, length = tweets.shape\n",
        "  tweets_class = []\n",
        "  for i in range(tweet):\n",
        "    score = polarity_score(tweets[i,:])\n",
        "    sentiment = determine_sentiment(score)\n",
        "    tweets_class.append(sentiment)\n",
        "  return tweets_class\n",
        "\n",
        "# all_tweets_classes = np.array(score_tweets(all_tweets))\n",
        "# train_pos_classes = np.array(score_tweets(features_train_pos))\n",
        "# train_neg_classes = np.negative(np.array(score_tweets(features_train_neg)))\n",
        "# test_pos_classes = np.array(score_tweets(features_test_pos))\n",
        "# test_neg_classes = np.negative(np.array(score_tweets(features_test_neg)))\n",
        "\n",
        "pos_classes = np.array(score_tweets(features_pos))\n",
        "# neg_classes = np.array(score_tweets(features_neg))\n",
        "neg_classes = np.negative(np.array(score_tweets(features_neg)))\n",
        "\n",
        "# train_classes = np.concatenate((train_pos_classes, train_neg_classes))\n",
        "# test_classes = np.concatenate((test_pos_classes, test_neg_classes))\n",
        "# train_classes = np.array(score_tweets(features_train))\n",
        "# test_classes = np.array(score_tweets(features_test))\n",
        "# all_tweets_features = np.concatenate((features_pos, features_neg))\n",
        "all_tweets_classes = np.concatenate((pos_classes, neg_classes)) \n",
        "print(all_tweets_classes.size)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVddVw8gsQ1w"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "features_train, features_test, train_classes, test_classes = train_test_split(\n",
        "    features_all, all_tweets_classes, test_size=0.3)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES0uNivrkT3p"
      },
      "source": [
        "# #Table 4 - Polarity of tweets\n",
        "# high_positive = 0\n",
        "# moderate_positive = 0\n",
        "# neutral = 0\n",
        "# moderate_negative = 0\n",
        "# high_negative = 0\n",
        "\n",
        "# for t in all_tweets_classes:\n",
        "#   if t == 2:\n",
        "#     high_positive += 1\n",
        "#   if t == 1:\n",
        "#     moderate_positive += 1\n",
        "#   if t == 0:\n",
        "#     neutral += 1\n",
        "#   if t == -1:\n",
        "#     moderate_negative += 1\n",
        "#   if t == -2:\n",
        "#     high_negative += 1\n",
        "\n",
        "# hp_ratio = (high_positive/all_tweets_classes.size)*100\n",
        "# mp_ratio = (moderate_positive/all_tweets_classes.size)*100\n",
        "# neutral_ratio = (neutral/all_tweets_classes.size)*100\n",
        "# mn_ratio = (moderate_negative/all_tweets_classes.size)*100\n",
        "# hn_ratio = (high_negative/all_tweets_classes.size)*100\n",
        "\n",
        "# print(f'high_positive: {high_positive} ({hp_ratio}%)')\n",
        "# print(f'moderate_positive: {moderate_positive} ({mp_ratio}%)')\n",
        "# print(f'neutral: {neutral} ({neutral_ratio}%)')\n",
        "# print(f'moderate_negative: {moderate_negative} ({mn_ratio}%)')\n",
        "# print(f'high_negative: {high_negative} ({hn_ratio}%)')"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvn2eBPcZwKU",
        "outputId": "6e880c89-0f37-4e90-ead1-d8ae319a49d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Table 4 - Polarity of positive tweets\n",
        "high_positive = 0\n",
        "moderate_positive = 0\n",
        "part_pos_neutral = 0\n",
        "\n",
        "pos_classes = np.array(score_tweets(features_pos))\n",
        "\n",
        "for t in pos_classes:\n",
        "  if t == 2:\n",
        "    high_positive += 1\n",
        "  if t == 1:\n",
        "    moderate_positive += 1\n",
        "  if t == 0:\n",
        "    part_pos_neutral += 1\n",
        "\n",
        "print(f'high_positive: {high_positive}')\n",
        "print(f'moderate_positive: {moderate_positive}')\n",
        "print(f'part_pos_neutral: {part_pos_neutral}')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "high_positive: 1695\n",
            "moderate_positive: 2876\n",
            "part_pos_neutral: 429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm75qHGE9Ro2",
        "outputId": "7ed56b77-864f-48c7-e7d6-85aef99b3e23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Table 4 - Polarity of negative tweets\n",
        "moderate_negative = 0\n",
        "high_negative = 0\n",
        "part_neg_neutral = 0\n",
        "\n",
        "neg_classes = np.array(score_tweets(features_neg))\n",
        "\n",
        "for t in neg_classes:\n",
        "  if t == 2:\n",
        "    high_negative += 1\n",
        "  if t == 1:\n",
        "    moderate_negative += 1\n",
        "  if t == 0:\n",
        "    part_neg_neutral += 1\n",
        "\n",
        "print(f'high_negative: {high_negative}')\n",
        "print(f'moderate_negative: {moderate_negative}')\n",
        "print(f'part_neg_neutral: {part_neg_neutral}')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "high_negative: 1636\n",
            "moderate_negative: 2948\n",
            "part_neg_neutral: 416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6pxu0GTbP_c",
        "outputId": "b4e73c4f-bbac-4c6e-a42b-1614dfc12287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "high_positive + high_negative + moderate_positive + moderate_negative + part_pos_neutral + part_neg_neutral"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmnD6YrgX7Kg",
        "outputId": "a22bc33a-37cd-478a-9fde-8655abac0713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Gambiarra table 4\n",
        "neutral = part_pos_neutral + part_neg_neutral\n",
        "\n",
        "hp_ratio = (high_positive/all_tweets_classes.size)*100\n",
        "mp_ratio = (moderate_positive/all_tweets_classes.size)*100\n",
        "neutral_ratio = (neutral/all_tweets_classes.size)*100\n",
        "mn_ratio = (moderate_negative/all_tweets_classes.size)*100\n",
        "hn_ratio = (high_negative/all_tweets_classes.size)*100\n",
        "\n",
        "print(f'high_positive: {high_positive} ({hp_ratio}%)')\n",
        "print(f'moderate_positive: {moderate_positive} ({mp_ratio}%)')\n",
        "print(f'neutral: {neutral} ({neutral_ratio}%)')\n",
        "print(f'moderate_negative: {moderate_negative} ({mn_ratio}%)')\n",
        "print(f'high_negative: {high_negative} ({hn_ratio}%)')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "high_positive: 1695 (16.950000000000003%)\n",
            "moderate_positive: 2876 (28.76%)\n",
            "neutral: 845 (8.450000000000001%)\n",
            "moderate_negative: 2948 (29.48%)\n",
            "high_negative: 1636 (16.36%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqM3EwQ1XlY5"
      },
      "source": [
        "# Decision tree implementation\n",
        "from sklearn import tree\n",
        "\n",
        "decision_tree = tree.DecisionTreeClassifier()\n",
        "# X = features_train, y = train_classes\n",
        "\n",
        "# features_train = vectorizer.fit_transform(proc_train_tweets).toarray()\n",
        "# train_classes = np.concatenate((train_pos_classes, train_neg_classes))\n",
        "# train_pos_classes = np.array(score_tweets(features_train_pos))\n",
        "# train_neg_classes = np.negative(np.array(score_tweets(features_train_neg)))\n",
        "\n",
        "decision_tree.fit(features_train, train_classes)\n",
        "y_predict = decision_tree.predict(features_test)\n",
        "\n",
        "# decision_tree.fit(train_features, train_classes)\n",
        "# y_predict = decision_tree.predict(test_features)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnAWjAQQ44-d",
        "outputId": "894361ad-d90d-422b-f023-87157e197a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(features_train.shape)\n",
        "print(train_classes.shape)\n",
        "print(features_test.shape)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7000, 1958)\n",
            "(7000,)\n",
            "(3000, 1958)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwwMA3Wrt1mQ"
      },
      "source": [
        "# # Random forest implementation\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# random_forest = RandomForestClassifier(n_jobs=-1)\n",
        "# random_forest.fit(features_train, train_classes)\n",
        "# y_predict = random_forest.predict(features_test)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyBxpMC7aObw"
      },
      "source": [
        "# # SVR implementation\n",
        "# from sklearn.svm import SVR\n",
        "\n",
        "# svr = SVR(C=1.0, epsilon=0.2)\n",
        "# svr.fit(features_train, train_classes)\n",
        "# y_predict = svr.predict(features_test)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbB4acCk2HDJ"
      },
      "source": [
        "# # Multinomial Logistic Regression (Softmax)\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# softmax = LogisticRegression(multi_class='multinomial')\n",
        "# softmax.fit(features_train, train_classes)\n",
        "# y_predict = softmax.predict(features_test)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmmpp84Ja6zy",
        "outputId": "f2d50e3e-1356-4c0f-ea84-b72ca533af7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(test_classes.shape)\n",
        "print(y_predict.shape)\n",
        "print(features_test.shape)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3000,)\n",
            "(3000,)\n",
            "(3000, 1958)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nspxMkmnLwXW",
        "outputId": "81240c3e-18ba-4139-a911-be0fe6b8eaf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(y_predict[:10])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1  2 -1  1 -1 -1 -1  1 -1  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX5EZWVfM2Y3",
        "outputId": "9b1a5c61-c1fa-468b-d503-c83118d79e2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = test_classes\n",
        "y_pred = y_predict \n",
        "target_names = ['High Negative', 'Moderate Negative', 'Neutral', 'Moderate Positive', 'High Positive']\n",
        "# target_names = ['Neutral', 'Moderate Positive', 'High Positive']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "    High Negative       0.51      0.54      0.53       475\n",
            "Moderate Negative       0.62      0.60      0.61       887\n",
            "          Neutral       0.67      0.97      0.79       264\n",
            "Moderate Positive       0.62      0.60      0.61       848\n",
            "    High Positive       0.61      0.51      0.55       526\n",
            "\n",
            "         accuracy                           0.61      3000\n",
            "        macro avg       0.61      0.64      0.62      3000\n",
            "     weighted avg       0.61      0.61      0.60      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXtuX0TDYTiU"
      },
      "source": [
        "# from sklearn.metrics import classification_report\n",
        "# y_predict = random_forest.predict(features_train)\n",
        "# y_true = train_classes\n",
        "# y_pred = y_predict \n",
        "# # target_names = ['High Negative', 'Moderate Negative', 'Neutral', 'Moderate Positive', 'High Positive']\n",
        "# target_names = ['Neutral', 'Moderate Positive', 'High Positive']\n",
        "# print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InwudDjdklra",
        "outputId": "d1313910-c974-465e-a22e-df566f8c72b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# 10-fold cross-validation\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# prepare the cross-validation procedure\n",
        "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "# clf = decision_tree ou random_forest ou svr ou softmax\n",
        "scores = cross_val_score(decision_tree, features_train, train_classes, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# Report performance\n",
        "print(f'scores.shape{scores.shape}')\n",
        "print(f'scores{scores}')\n",
        "print(f'mean score: {np.mean(scores)}')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scores.shape(10,)\n",
            "scores[0.59714286 0.6        0.58857143 0.61571429 0.58571429 0.61285714\n",
            " 0.59142857 0.59142857 0.58428571 0.61571429]\n",
            "mean score: 0.5982857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_decision_tree",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELTksXUpFZe-"
      },
      "source": [
        "# Interesting links: \n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
        "# https://medium.com/swlh/sentiment-classification-for-restaurant-reviews-using-tf-idf-42f707bfe44d\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "# https://medium.com/analytics-vidhya/complete-guide-to-machine-learning-evaluation-metrics-615c2864d916"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Hwms6UN_mS",
        "outputId": "d51c5479-126c-49f0-e293-282daf618de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "pip install nltk pycld2 wordsegment autocorrect"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Collecting pycld2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4MB 87kB/s \n",
            "\u001b[?25hCollecting wordsegment\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/6c/e6f4734d6f7d28305f52ec81377d7ce7d1856b97b814278e9960183235ad/wordsegment-1.3.1-py2.py3-none-any.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 34.0MB/s \n",
            "\u001b[?25hCollecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e9/138a0f19b9c02f3e1795f24940c9e771ff262a4e6f9585f05e9da09db658/autocorrect-2.1.0.tar.gz (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Building wheels for collected packages: pycld2, autocorrect\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp36-cp36m-linux_x86_64.whl size=9833501 sha256=0b184c38e436333bbfb2eeeb4621404e68f701f004449ea8146c980455b7f95c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.1.0-cp36-none-any.whl size=1811979 sha256=097a8435f7dcaa057c0c12411f5880a79039a2ec348f11a4f6a72725a008809e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/33/69/a42db65d595bb685429c788ab45e11aa2a4b6549298ee38e02\n",
            "Successfully built pycld2 autocorrect\n",
            "Installing collected packages: pycld2, wordsegment, autocorrect\n",
            "Successfully installed autocorrect-2.1.0 pycld2-0.41 wordsegment-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5ReWfACN6Zl",
        "outputId": "9d8d1d7c-48c6-46e7-9d4d-b9b1552b992f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples\n",
        "twitter_samples.fileids()\n",
        "positive_tweets = [t for t in twitter_samples.strings(\"positive_tweets.json\")]\n",
        "negative_tweets = [t for t in twitter_samples.strings(\"negative_tweets.json\")] "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrT7XWNwDwpQ",
        "outputId": "51d72816-e522-4b2a-c8cc-4ba296d138f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus).toarray()\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.shape)\n",
        "print(X)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "(4, 9)\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODi-rkNCU1y0",
        "outputId": "40da9861-7d03-4636-de4e-47b7953ee42c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(positive_tweets))\n",
        "print(len(negative_tweets))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdu9utDSWVLa"
      },
      "source": [
        "train_pos_tweets = positive_tweets[:3500]\n",
        "test_pos_tweets = positive_tweets[3500:]\n",
        "train_neg_tweets = negative_tweets[:3500]\n",
        "test_neg_tweets = negative_tweets[3500:]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1XNnj8gxeb3",
        "outputId": "64473f87-a81d-48b9-cae9-489dbc4276ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_pos_tweets[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQRBjYf1xnUL",
        "outputId": "f96fc2d3-1cc6-47c1-e1ef-0f4cef14fba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train_neg_tweets[7])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnpGNGhHWz9Y",
        "outputId": "7c5e5859-8ba4-45bb-8067-7e1a5f0f6776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(len(train_pos_tweets))\n",
        "print(len(test_pos_tweets))\n",
        "print(len(train_neg_tweets))\n",
        "print(len(test_neg_tweets))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3500\n",
            "1500\n",
            "3500\n",
            "1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRvzAc5wXJf2",
        "outputId": "33195d6d-9bf6-4d22-a8ee-9070506ebf4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(train_pos_tweets + train_neg_tweets))\n",
        "print(len(test_pos_tweets + test_neg_tweets))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7000\n",
            "3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z_F-R2KsLO6"
      },
      "source": [
        "# import pycld2 as cld2\n",
        "\n",
        "# def extract_engTweets(tweets):\n",
        "#     en_tweets = []\n",
        "#     for e in tweets:\n",
        "#       details = cld2.detect(e)\n",
        "#       # Extracts only the string with lang name i.e: 'en'\n",
        "#       if details[2][0][1] == 'en': \n",
        "#         en_tweets.append(e)\n",
        "#     return en_tweets\n",
        "\n",
        "# en_train_pos_tweets = extract_engTweets(train_pos_tweets)\n",
        "# en_train_neg_tweets = extract_engTweets(train_neg_tweets)\n",
        "# en_test_pos_tweets = extract_engTweets(test_pos_tweets)\n",
        "# en_test_neg_tweets = extract_engTweets(test_neg_tweets)\n",
        "# print(len(train_pos_tweets))\n",
        "# print(len(en_train_pos_tweets))\n",
        "# print(len(train_neg_tweets))\n",
        "# print(len(en_train_neg_tweets))\n",
        "# print(len(test_pos_tweets))\n",
        "# print(len(en_test_pos_tweets))\n",
        "# print(len(test_neg_tweets))\n",
        "# print(len(en_test_neg_tweets))\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBwmdzQ3f5tr",
        "outputId": "3c4668c6-1614-4159-d578-1e2ded78f3e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#pre processing\n",
        "import re\n",
        "import wordsegment\n",
        "import autocorrect\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "from autocorrect import Speller\n",
        "spell=Speller()\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "punctuations = '''!()-![]{};:+'\"\\,<>./?@#$%^&*_~'''\n",
        "testTweet1='@PERKSOFNIALLJH RTed! Goood Luck :)'\n",
        "testTweet2='My legs hurt so bad :))))))))))))))))))'\n",
        "testTweet3='#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris trouble for being top engaged members in my community this week :)'\n",
        "testTweet4='@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!'\n",
        "\n",
        "def pre_process (tweets):\n",
        "  result_tweets = []\n",
        "  for tweet in tweets:  \n",
        "    # print(f'raw tweet: {tweet}')\n",
        "    tweet = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \"\", tweet, flags=re.MULTILINE) # to remove links that start with HTTP/HTTPS in the tweet\n",
        "    tweet = re.sub(r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \"\", tweet, flags=re.MULTILINE) # to remove other url links\n",
        "    tweet = re.sub(r\"RT\", ' ', tweet, flags=re.MULTILINE) #remove 'RT'\n",
        "    tweet = re.sub(r\"#(\\w+)\", ' ', tweet, flags=re.MULTILINE) #remove '#'\n",
        "    tweet = re.sub(r\"@(\\w+)\", ' ', tweet, flags=re.MULTILINE) #remove '@'\n",
        "    tweet = ''.join([i for i in tweet if not i in punctuations]) #remove ponctuations\n",
        "    tweet = ' '.join(segment(tweet)) #Remove repeated letters\n",
        "    tweet = ' '.join([spell(w) for w in tweet.split()]) #Avoiding misspellings and slang words\n",
        "    tweet = re.sub(r\"\\d\", \"\", tweet) #remove digits\n",
        "    tweet = tweet.lower()\n",
        "    tokens = tknzr.tokenize(tweet) #tokenize\n",
        "    tokens = [word for word in tokens if not word in stopwords.words()]\n",
        "    tweet = (\" \").join(stemmer.stem(token) for token in tokens)\n",
        "    # print(f'preprocessed tweet: {tweet}')\n",
        "    result_tweets.append(tweet)\n",
        "  \n",
        "  return result_tweets\n",
        "\n",
        "# proc_en_train_pos_tweets = pre_process([testTweet1,testTweet2,testTweet3,testTweet4])\n",
        "# proc_en_train_pos_tweets = pre_process([en_train_pos_tweets[0]])\n",
        "# proc_en_train_neg_tweets = pre_process([en_train_neg_tweets[0]])\n",
        "proc_train_pos_tweets = pre_process(train_pos_tweets)\n",
        "proc_train_neg_tweets = pre_process(train_neg_tweets)\n",
        "proc_test_pos_tweets = pre_process(test_pos_tweets)\n",
        "proc_test_neg_tweets = pre_process(test_neg_tweets)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fgGxJDsDyW7",
        "outputId": "3686c59b-3da0-42a1-d25f-fbc26115468b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(len(train_pos_tweets))\n",
        "print(len(test_pos_tweets))\n",
        "print(len(train_neg_tweets))\n",
        "print(len(test_neg_tweets))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3500\n",
            "1500\n",
            "3500\n",
            "1500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFfBS8c_Itkv"
      },
      "source": [
        "import numpy as np\n",
        "proc_train_tweets = np.concatenate((proc_train_pos_tweets, proc_train_neg_tweets))\n",
        "proc_test_tweets = np.concatenate((proc_test_pos_tweets, proc_test_neg_tweets))\n",
        "proc_pos_tweets = np.concatenate((proc_train_pos_tweets, proc_test_pos_tweets))\n",
        "proc_neg_tweets = np.concatenate((proc_train_neg_tweets, proc_test_neg_tweets))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nEnwpH_1Pxo"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# vectorizer = TfidfVectorizer(min_df=5, max_df=0.75, ngram_range=(1,3), max_features=10000)\n",
        "vectorizer = TfidfVectorizer(min_df=1, max_df=0.75, ngram_range=(1,3), max_features=10000)\n",
        "features_train_pos = vectorizer.fit_transform(proc_train_pos_tweets).toarray()\n",
        "features_train_neg = vectorizer.fit_transform(proc_train_neg_tweets).toarray()\n",
        "features_test_pos = vectorizer.fit_transform(proc_test_pos_tweets).toarray()\n",
        "features_test_neg = vectorizer.fit_transform(proc_test_neg_tweets).toarray()\n",
        "################################################################################\n",
        "features_train = vectorizer.fit_transform(proc_train_tweets).toarray()\n",
        "features_test = vectorizer.fit_transform(proc_test_tweets).toarray()\n",
        "features_pos = vectorizer.fit_transform(proc_pos_tweets).toarray()\n",
        "features_neg = vectorizer.fit_transform(proc_neg_tweets).toarray()\n",
        "# print(f'features_train_pos: {features_train_pos}')\n",
        "# print(f'type(features_train_pos): {type(features_train_pos)}')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvc4UsGRgqwh",
        "outputId": "6866457f-e69c-4b63-8077-0bab41b8a7f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "print(f'features_train.shape: {features_train.shape}')\n",
        "print(f'features_test.shape: {features_test.shape}')\n",
        "print(f'features_train.shape: {features_pos.shape}')\n",
        "print(f'features_test.shape: {features_neg.shape}')\n",
        "print(f'features_train_pos.shape: {features_train_pos.shape}')\n",
        "print(f'features_train_neg.shape: {features_train_neg.shape}')\n",
        "print(f'features_test_pos.shape: {features_test_pos.shape}')\n",
        "print(f'features_test_neg.shape: {features_test_neg.shape}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features_train.shape: (7000, 10000)\n",
            "features_test.shape: (3000, 10000)\n",
            "features_train.shape: (5000, 10000)\n",
            "features_test.shape: (5000, 10000)\n",
            "features_train_pos.shape: (3500, 10000)\n",
            "features_train_neg.shape: (3500, 10000)\n",
            "features_test_pos.shape: (1500, 10000)\n",
            "features_test_neg.shape: (1500, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QElcHuMj5Dxh"
      },
      "source": [
        "# import numpy as np\n",
        "# all_tweets = np.concatenate((features_train_pos,features_train_neg,features_test_pos,features_test_neg))\n",
        "# train_features = np.concatenate((features_train_pos,features_train_neg))\n",
        "# test_features = np.concatenate((features_test_pos,features_test_neg))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM4zRH_t1Tti"
      },
      "source": [
        "## Scoring and labeling the tweets\n",
        "# First calculate the overall polarity score\n",
        "# Assume only positive and negative\n",
        "# Classify via the number of neg/pos words and emoticons\n",
        "\n",
        "# 'tweet is expected to be an integer iterable' \n",
        "def polarity_score(tweet):\n",
        "  score = 0\n",
        "  for feature in tweet:\n",
        "    score += feature\n",
        "  return score\n",
        "\n",
        "# 'pol_score' is expected to be a scalar\n",
        "def determine_sentiment(pol_score):\n",
        "  # Highly positive: +2 \n",
        "  if pol_score >= 2 and pol_score <= 4:\n",
        "    return 2 \n",
        "  # Moderate positive: +1 \n",
        "  if pol_score > 0 and pol_score < 2:\n",
        "    return 1\n",
        "  # Moderate negative: -1 \n",
        "  if pol_score > -2 and pol_score < 0:\n",
        "    return -1 \n",
        "  # Highly negative: -2 \n",
        "  if pol_score >= -4 and pol_score <= -2:\n",
        "    return -2\n",
        "  if pol_score == 0: \n",
        "    return 0\n",
        "  else:#Tirar depois\n",
        "    # return 2\n",
        "    return 0"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGZXFeqF53_s",
        "outputId": "ab889d60-2329-48c1-ccce-3e5729ffc53c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## Testing the balancing and scoring\n",
        "# Classify the tweets\n",
        "def score_tweets(tweets):\n",
        "  tweet, length = tweets.shape\n",
        "  tweets_class = []\n",
        "  for i in range(tweet):\n",
        "    score = polarity_score(tweets[i,:])\n",
        "    sentiment = determine_sentiment(score)\n",
        "    tweets_class.append(sentiment)\n",
        "  return tweets_class\n",
        "\n",
        "# all_tweets_classes = np.array(score_tweets(all_tweets))\n",
        "train_pos_classes = np.array(score_tweets(features_train_pos))\n",
        "train_neg_classes = np.negative(np.array(score_tweets(features_train_neg)))\n",
        "test_pos_classes = np.array(score_tweets(features_test_pos))\n",
        "test_neg_classes = np.negative(np.array(score_tweets(features_test_neg)))\n",
        "pos_classes = np.array(score_tweets(features_pos))\n",
        "neg_classes = np.negative(np.array(score_tweets(features_neg)))\n",
        "train_classes = np.concatenate((train_pos_classes, train_neg_classes))\n",
        "test_classes = np.concatenate((test_pos_classes, test_neg_classes))\n",
        "# train_classes = np.array(score_tweets(features_train))\n",
        "# test_classes = np.array(score_tweets(features_test))\n",
        "all_tweets_classes = np.concatenate((train_classes, test_classes)) \n",
        "print(all_tweets_classes.size)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES0uNivrkT3p"
      },
      "source": [
        "# #Table 4 - Polarity of tweets\n",
        "# high_positive = 0\n",
        "# moderate_positive = 0\n",
        "# neutral = 0\n",
        "# moderate_negative = 0\n",
        "# high_negative = 0\n",
        "\n",
        "# for t in all_tweets_classes:\n",
        "#   if t == 2:\n",
        "#     high_positive += 1\n",
        "#   if t == 1:\n",
        "#     moderate_positive += 1\n",
        "#   if t == 0:\n",
        "#     neutral += 1\n",
        "#   if t == -1:\n",
        "#     moderate_negative += 1\n",
        "#   if t == -2:\n",
        "#     high_negative += 1\n",
        "\n",
        "# hp_ratio = (high_positive/all_tweets_classes.size)*100\n",
        "# mp_ratio = (moderate_positive/all_tweets_classes.size)*100\n",
        "# neutral_ratio = (neutral/all_tweets_classes.size)*100\n",
        "# mn_ratio = (moderate_negative/all_tweets_classes.size)*100\n",
        "# hn_ratio = (high_negative/all_tweets_classes.size)*100\n",
        "\n",
        "# print(f'high_positive: {high_positive} ({hp_ratio}%)')\n",
        "# print(f'moderate_positive: {moderate_positive} ({mp_ratio}%)')\n",
        "# print(f'neutral: {neutral} ({neutral_ratio}%)')\n",
        "# print(f'moderate_negative: {moderate_negative} ({mn_ratio}%)')\n",
        "# print(f'high_negative: {high_negative} ({hn_ratio}%)')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvn2eBPcZwKU",
        "outputId": "6773d7c1-ff09-4a91-9179-48aece96528d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Table 4 - Polarity of positive tweets\n",
        "high_positive = 0\n",
        "moderate_positive = 0\n",
        "part_pos_neutral = 0\n",
        "\n",
        "pos_train_classes = np.array(score_tweets(features_pos))\n",
        "\n",
        "for t in pos_train_classes:\n",
        "  if t == 2:\n",
        "    high_positive += 1\n",
        "  if t == 1:\n",
        "    moderate_positive += 1\n",
        "  if t == 0:\n",
        "    part_pos_neutral += 1\n",
        "\n",
        "print(f'high_positive: {high_positive}')\n",
        "print(f'moderate_positive: {moderate_positive}')\n",
        "print(f'part_pos_neutral: {part_pos_neutral}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "high_positive: 2480\n",
            "moderate_positive: 2000\n",
            "part_pos_neutral: 520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm75qHGE9Ro2",
        "outputId": "7b0e3bff-22ad-45db-b16f-be5217ad0ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Table 4 - Polarity of negative tweets\n",
        "moderate_negative = 0\n",
        "high_negative = 0\n",
        "part_neg_neutral = 0\n",
        "\n",
        "neg_train_classes = np.array(score_tweets(features_neg))\n",
        "\n",
        "for t in neg_train_classes:\n",
        "  if t == 2:\n",
        "    high_negative += 1\n",
        "  if t == 1:\n",
        "    moderate_negative += 1\n",
        "  if t == 0:\n",
        "    part_neg_neutral += 1\n",
        "\n",
        "print(f'high_negative: {high_negative}')\n",
        "print(f'moderate_negative: {moderate_negative}')\n",
        "print(f'part_neg_neutral: {part_neg_neutral}')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "high_negative: 2435\n",
            "moderate_negative: 2119\n",
            "part_neg_neutral: 446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6pxu0GTbP_c",
        "outputId": "08271ca6-adaf-43db-8bda-5d1d0540e45a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "high_positive + high_negative + moderate_positive + moderate_negative + part_pos_neutral + part_neg_neutral"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmnD6YrgX7Kg",
        "outputId": "67211d0d-2c8b-4205-c6c4-1cfd15b7961c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Gambiarra table 1\n",
        "neutral = part_pos_neutral + part_neg_neutral\n",
        "\n",
        "hp_ratio = (high_positive/all_tweets_classes.size)*100\n",
        "mp_ratio = (moderate_positive/all_tweets_classes.size)*100\n",
        "neutral_ratio = (neutral/all_tweets_classes.size)*100\n",
        "mn_ratio = (moderate_negative/all_tweets_classes.size)*100\n",
        "hn_ratio = (high_negative/all_tweets_classes.size)*100\n",
        "\n",
        "print(f'high_positive: {high_positive} ({hp_ratio}%)')\n",
        "print(f'moderate_positive: {moderate_positive} ({mp_ratio}%)')\n",
        "print(f'neutral: {neutral} ({neutral_ratio}%)')\n",
        "print(f'moderate_negative: {moderate_negative} ({mn_ratio}%)')\n",
        "print(f'high_negative: {high_negative} ({hn_ratio}%)')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "high_positive: 2480 (24.8%)\n",
            "moderate_positive: 2000 (20.0%)\n",
            "neutral: 966 (9.66%)\n",
            "moderate_negative: 2119 (21.19%)\n",
            "high_negative: 2435 (24.349999999999998%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqM3EwQ1XlY5"
      },
      "source": [
        "# Decision tree implementation\n",
        "from sklearn import tree\n",
        "\n",
        "decision_tree = tree.DecisionTreeClassifier()\n",
        "# X = features_train, y = train_classes\n",
        "\n",
        "# features_train = vectorizer.fit_transform(proc_train_tweets).toarray()\n",
        "# train_classes = np.concatenate((train_pos_classes, train_neg_classes))\n",
        "# train_pos_classes = np.array(score_tweets(features_train_pos))\n",
        "# train_neg_classes = np.negative(np.array(score_tweets(features_train_neg)))\n",
        "\n",
        "decision_tree.fit(features_train, train_classes)\n",
        "y_predict = decision_tree.predict(features_test)\n",
        "\n",
        "# decision_tree.fit(train_features, train_classes)\n",
        "# y_predict = decision_tree.predict(test_features)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwwMA3Wrt1mQ"
      },
      "source": [
        "# # Random forest implementation\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# random_forest = RandomForestClassifier(n_jobs=-1)\n",
        "# random_forest.fit(features_train, train_classes)\n",
        "# y_predict = random_forest.predict(features_test)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyBxpMC7aObw"
      },
      "source": [
        "# # SVR implementation\n",
        "# from sklearn.svm import SVR\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# svr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
        "# svr.fit(features_train, train_classes)\n",
        "# y_predict = svr.predict(features_test)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbB4acCk2HDJ"
      },
      "source": [
        "# # Multinomial Logistic Regression (Softmax)\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# softmax = LogisticRegression(multi_class='multinomial')\n",
        "# softmax.fit(features_train, train_classes)\n",
        "# y_predict = softmax.predict(features_test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmmpp84Ja6zy",
        "outputId": "14ff6105-040c-4667-8d1b-4c500e79ca9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(test_classes.shape)\n",
        "print(y_predict.shape)\n",
        "print(features_test.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3000,)\n",
            "(3000,)\n",
            "(3000, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX5EZWVfM2Y3",
        "outputId": "a7b12a47-0b88-4bc9-ae22-36a09754238d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = test_classes\n",
        "y_pred = y_predict \n",
        "target_names = ['High Negative', 'Moderate Negative', 'Neutral', 'Moderate Positive', 'High Positive']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "    High Negative       0.22      0.05      0.08       711\n",
            "Moderate Negative       0.13      0.08      0.10       461\n",
            "          Neutral       0.22      0.69      0.34       726\n",
            "Moderate Positive       0.19      0.06      0.09       458\n",
            "    High Positive       0.23      0.07      0.10       644\n",
            "\n",
            "         accuracy                           0.21      3000\n",
            "        macro avg       0.20      0.19      0.14      3000\n",
            "     weighted avg       0.21      0.21      0.15      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InwudDjdklra"
      },
      "source": [
        "# # 10-fold cross-validation\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# # prepare the cross-validation procedure\n",
        "# cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "# # clf = decision_tree ou random_forest ou svr ou softmax\n",
        "# scores = cross_val_score(clf, features_train, train_classes, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# # Report performance\n",
        "# print(f'scores.shape{scores.shape}')\n",
        "# print(f'scores{scores}')"
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}
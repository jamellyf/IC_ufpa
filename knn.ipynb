{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "16ce07b1-20d0-47a5-9f64-e3fa59a98225",
      "display_name": "'Python Interactive'"
    },
    "colab": {
      "name": "knn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g0HSvjwwc7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "58c7e3d4-031b-4a44-de3f-1b048eada44a"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\"\n",
        "\n",
        "# Read dataset to pandas dataframe\n",
        "datasetPandas = pd.read_csv(url)\n",
        "# XPandas = datasetPandas.iloc[0:, :-1].transpose()\n",
        "# yPandas = datasetPandas.iloc[0:, 4].transpose()\n",
        "XPandas = datasetPandas.iloc[0:, :-1]\n",
        "yPandas = datasetPandas.iloc[0:, 4]\n",
        "X = XPandas.values\n",
        "y = yPandas.values\n",
        "\n",
        "XPandas.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Recency (months)</th>\n",
              "      <th>Frequency (times)</th>\n",
              "      <th>Monetary (c.c. blood)</th>\n",
              "      <th>Time (months)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>50</td>\n",
              "      <td>12500</td>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>3250</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>4000</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>5000</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>6000</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)\n",
              "0                 2                 50                  12500             98\n",
              "1                 0                 13                   3250             28\n",
              "2                 1                 16                   4000             35\n",
              "3                 2                 20                   5000             45\n",
              "4                 1                 24                   6000             77"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z_-fCO6DbzE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "1bafe8c0-a659-40a7-f71d-d7f4455f8596"
      },
      "source": [
        "print(XPandas.var().round(3).to_string())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recency (months)              65.535\n",
            "Frequency (times)             34.098\n",
            "Monetary (c.c. blood)    2131094.230\n",
            "Time (months)                594.224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UZ2m0LcEkJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "ad8b495f-2690-43db-c1d1-8e19313694b4"
      },
      "source": [
        "# Copy X_train and X_test into X_train_normed and X_test_normed\n",
        "XPandas_normed= XPandas.copy()\n",
        "\n",
        "# Specify which column to normalize\n",
        "col_to_normalize = XPandas_normed.var().idxmax(axis=1)\n",
        "\n",
        "# Log normalization\n",
        "for df_ in [XPandas_normed]:\n",
        "    # Add log normalized column\n",
        "    df_['monetary_log'] = np.log(df_[col_to_normalize])\n",
        "    # Drop the original column\n",
        "    df_.drop(columns=col_to_normalize, inplace=True)\n",
        "\n",
        "print(XPandas_normed.var().round(3).to_string())\n",
        "\n",
        "XPandas_normed.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recency (months)      65.535\n",
            "Frequency (times)     34.098\n",
            "Time (months)        594.224\n",
            "monetary_log           0.836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Recency (months)</th>\n",
              "      <th>Frequency (times)</th>\n",
              "      <th>Time (months)</th>\n",
              "      <th>monetary_log</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>50</td>\n",
              "      <td>98</td>\n",
              "      <td>9.433484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>28</td>\n",
              "      <td>8.086410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>35</td>\n",
              "      <td>8.294050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>45</td>\n",
              "      <td>8.517193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>77</td>\n",
              "      <td>8.699515</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Recency (months)  Frequency (times)  Time (months)  monetary_log\n",
              "0                 2                 50             98      9.433484\n",
              "1                 0                 13             28      8.086410\n",
              "2                 1                 16             35      8.294050\n",
              "3                 2                 20             45      8.517193\n",
              "4                 1                 24             77      8.699515"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWLb61slwc7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Create training and test splits\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtpbeB7mwc72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature scaling\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(X)\n",
        "# X = scaler.transform(X)\n",
        "\n",
        "# scaler.fit(X_train)\n",
        "\n",
        "# X_train = scaler.transform(X_train)\n",
        "# X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBfiu7BeAREo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e346d2f7-0ba9-44c9-fa06-5d80a05617bd"
      },
      "source": [
        "# print(XPandas.shape)\n",
        "# print(yPandas.shape)\n",
        "# print(X.shape)\n",
        "# print(y.shape)\n",
        "# print(X_train.shape)\n",
        "# print(X_test.shape)\n",
        "# print(y_train.shape)\n",
        "# print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(598, 4)\n",
            "(150, 4)\n",
            "(598,)\n",
            "(150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw5fLLnKsS45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "6d5eaa4a-12ba-4c62-eaee-3dc2d9a27f5c"
      },
      "source": [
        "# df = pd.DataFrame(X_train,index=X_train[:,0])\n",
        "# df = pd.DataFrame(X)\n",
        "XPandas_normed = XPandas_normed.transpose()\n",
        "corrMatrixPandas = XPandas_normed.corr()\n",
        "# print(corrMatrixPandas)\n",
        "# type(corrMatrixPandas)\n",
        "corrMatrix = corrMatrixPandas.values # W\n",
        "print(corrMatrix.shape)\n",
        "print(corrMatrix)\n",
        "# type(corrMatrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(748, 748)\n",
            "[[1.         0.97099123 0.98472666 ... 0.71694339 0.23087357 0.24764505]\n",
            " [0.97099123 1.         0.99686935 ... 0.69958617 0.15362695 0.16138163]\n",
            " [0.98472666 0.99686935 1.         ... 0.73385821 0.21092666 0.2207664 ]\n",
            " ...\n",
            " [0.71694339 0.69958617 0.73385821 ... 1.         0.80750082 0.80750208]\n",
            " [0.23087357 0.15362695 0.21092666 ... 0.80750082 1.         0.99908668]\n",
            " [0.24764505 0.16138163 0.2207664  ... 0.80750208 0.99908668 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsUHWCCH6oYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "9b63639a-6ba0-4f69-9a8d-f2948bf3aa18"
      },
      "source": [
        "from scipy.sparse import csgraph\n",
        "laplacianMatrix = csgraph.laplacian(corrMatrix, normed=False)\n",
        "# laplacianMatrix = csgraph.laplacian(corrMatrix, normed=True)\n",
        "\n",
        "df_lp = pd.DataFrame(data=laplacianMatrix)\n",
        "df_lp.head()\n",
        "# print(laplacianMatrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>708</th>\n",
              "      <th>709</th>\n",
              "      <th>710</th>\n",
              "      <th>711</th>\n",
              "      <th>712</th>\n",
              "      <th>713</th>\n",
              "      <th>714</th>\n",
              "      <th>715</th>\n",
              "      <th>716</th>\n",
              "      <th>717</th>\n",
              "      <th>718</th>\n",
              "      <th>719</th>\n",
              "      <th>720</th>\n",
              "      <th>721</th>\n",
              "      <th>722</th>\n",
              "      <th>723</th>\n",
              "      <th>724</th>\n",
              "      <th>725</th>\n",
              "      <th>726</th>\n",
              "      <th>727</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>419.387861</td>\n",
              "      <td>-0.970991</td>\n",
              "      <td>-0.984727</td>\n",
              "      <td>-0.990951</td>\n",
              "      <td>-0.974451</td>\n",
              "      <td>0.459631</td>\n",
              "      <td>-0.893150</td>\n",
              "      <td>-0.962339</td>\n",
              "      <td>-0.947687</td>\n",
              "      <td>-0.998131</td>\n",
              "      <td>-0.986365</td>\n",
              "      <td>-0.170031</td>\n",
              "      <td>-0.953319</td>\n",
              "      <td>-0.954503</td>\n",
              "      <td>-0.881467</td>\n",
              "      <td>-0.793635</td>\n",
              "      <td>-0.958187</td>\n",
              "      <td>-0.963176</td>\n",
              "      <td>-0.881467</td>\n",
              "      <td>0.053790</td>\n",
              "      <td>0.053790</td>\n",
              "      <td>-0.962199</td>\n",
              "      <td>-0.887410</td>\n",
              "      <td>-0.887410</td>\n",
              "      <td>-0.895367</td>\n",
              "      <td>-0.966069</td>\n",
              "      <td>-0.872963</td>\n",
              "      <td>-0.960039</td>\n",
              "      <td>-0.784313</td>\n",
              "      <td>-0.931752</td>\n",
              "      <td>-0.948915</td>\n",
              "      <td>-0.949855</td>\n",
              "      <td>-0.949855</td>\n",
              "      <td>-0.939741</td>\n",
              "      <td>-0.952136</td>\n",
              "      <td>-0.928995</td>\n",
              "      <td>-0.945437</td>\n",
              "      <td>-0.880799</td>\n",
              "      <td>-0.947882</td>\n",
              "      <td>-0.909342</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.756362</td>\n",
              "      <td>-0.710507</td>\n",
              "      <td>-0.203651</td>\n",
              "      <td>-0.658456</td>\n",
              "      <td>-0.688180</td>\n",
              "      <td>-0.829669</td>\n",
              "      <td>-0.783292</td>\n",
              "      <td>-0.813092</td>\n",
              "      <td>-0.359530</td>\n",
              "      <td>-0.551806</td>\n",
              "      <td>-0.551806</td>\n",
              "      <td>-0.779915</td>\n",
              "      <td>-0.589818</td>\n",
              "      <td>-0.504394</td>\n",
              "      <td>-0.611208</td>\n",
              "      <td>-0.351741</td>\n",
              "      <td>-0.194595</td>\n",
              "      <td>-0.194595</td>\n",
              "      <td>-0.661827</td>\n",
              "      <td>-0.194595</td>\n",
              "      <td>-0.194595</td>\n",
              "      <td>-0.559882</td>\n",
              "      <td>-0.507908</td>\n",
              "      <td>-0.819923</td>\n",
              "      <td>-0.201951</td>\n",
              "      <td>-0.201951</td>\n",
              "      <td>-0.201951</td>\n",
              "      <td>-0.201951</td>\n",
              "      <td>-0.201951</td>\n",
              "      <td>-0.201951</td>\n",
              "      <td>-0.201951</td>\n",
              "      <td>-0.680930</td>\n",
              "      <td>-0.201951</td>\n",
              "      <td>-0.802752</td>\n",
              "      <td>-0.815585</td>\n",
              "      <td>-0.535148</td>\n",
              "      <td>-0.686925</td>\n",
              "      <td>-0.716943</td>\n",
              "      <td>-0.230874</td>\n",
              "      <td>-0.247645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.970991</td>\n",
              "      <td>442.203316</td>\n",
              "      <td>-0.996869</td>\n",
              "      <td>-0.989762</td>\n",
              "      <td>-0.975281</td>\n",
              "      <td>0.236934</td>\n",
              "      <td>-0.974669</td>\n",
              "      <td>-0.987446</td>\n",
              "      <td>-0.990966</td>\n",
              "      <td>-0.969112</td>\n",
              "      <td>-0.977393</td>\n",
              "      <td>-0.386928</td>\n",
              "      <td>-0.985197</td>\n",
              "      <td>-0.973840</td>\n",
              "      <td>-0.963489</td>\n",
              "      <td>-0.914584</td>\n",
              "      <td>-0.972376</td>\n",
              "      <td>-0.975072</td>\n",
              "      <td>-0.963489</td>\n",
              "      <td>-0.183170</td>\n",
              "      <td>-0.183170</td>\n",
              "      <td>-0.980107</td>\n",
              "      <td>-0.964079</td>\n",
              "      <td>-0.964079</td>\n",
              "      <td>-0.868677</td>\n",
              "      <td>-0.973536</td>\n",
              "      <td>-0.946177</td>\n",
              "      <td>-0.973417</td>\n",
              "      <td>-0.892345</td>\n",
              "      <td>-0.969269</td>\n",
              "      <td>-0.965010</td>\n",
              "      <td>-0.971887</td>\n",
              "      <td>-0.971887</td>\n",
              "      <td>-0.967037</td>\n",
              "      <td>-0.962463</td>\n",
              "      <td>-0.968001</td>\n",
              "      <td>-0.964227</td>\n",
              "      <td>-0.943033</td>\n",
              "      <td>-0.961751</td>\n",
              "      <td>-0.949837</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.764625</td>\n",
              "      <td>-0.683777</td>\n",
              "      <td>-0.140867</td>\n",
              "      <td>-0.622018</td>\n",
              "      <td>-0.683525</td>\n",
              "      <td>-0.829251</td>\n",
              "      <td>-0.779817</td>\n",
              "      <td>-0.811819</td>\n",
              "      <td>-0.311539</td>\n",
              "      <td>-0.517027</td>\n",
              "      <td>-0.517027</td>\n",
              "      <td>-0.759299</td>\n",
              "      <td>-0.558996</td>\n",
              "      <td>-0.461924</td>\n",
              "      <td>-0.582718</td>\n",
              "      <td>-0.299264</td>\n",
              "      <td>-0.136577</td>\n",
              "      <td>-0.136577</td>\n",
              "      <td>-0.625068</td>\n",
              "      <td>-0.136577</td>\n",
              "      <td>-0.136577</td>\n",
              "      <td>-0.523144</td>\n",
              "      <td>-0.471983</td>\n",
              "      <td>-0.825124</td>\n",
              "      <td>-0.140063</td>\n",
              "      <td>-0.140063</td>\n",
              "      <td>-0.140063</td>\n",
              "      <td>-0.140063</td>\n",
              "      <td>-0.140063</td>\n",
              "      <td>-0.140063</td>\n",
              "      <td>-0.140063</td>\n",
              "      <td>-0.655789</td>\n",
              "      <td>-0.140063</td>\n",
              "      <td>-0.791283</td>\n",
              "      <td>-0.818294</td>\n",
              "      <td>-0.498644</td>\n",
              "      <td>-0.669464</td>\n",
              "      <td>-0.699586</td>\n",
              "      <td>-0.153627</td>\n",
              "      <td>-0.161382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.984727</td>\n",
              "      <td>-0.996869</td>\n",
              "      <td>452.615620</td>\n",
              "      <td>-0.997945</td>\n",
              "      <td>-0.987169</td>\n",
              "      <td>0.308969</td>\n",
              "      <td>-0.956756</td>\n",
              "      <td>-0.991127</td>\n",
              "      <td>-0.987959</td>\n",
              "      <td>-0.985069</td>\n",
              "      <td>-0.990484</td>\n",
              "      <td>-0.313255</td>\n",
              "      <td>-0.987376</td>\n",
              "      <td>-0.981580</td>\n",
              "      <td>-0.949836</td>\n",
              "      <td>-0.887202</td>\n",
              "      <td>-0.981684</td>\n",
              "      <td>-0.984523</td>\n",
              "      <td>-0.949836</td>\n",
              "      <td>-0.108536</td>\n",
              "      <td>-0.108536</td>\n",
              "      <td>-0.987183</td>\n",
              "      <td>-0.952989</td>\n",
              "      <td>-0.952989</td>\n",
              "      <td>-0.897420</td>\n",
              "      <td>-0.984291</td>\n",
              "      <td>-0.939304</td>\n",
              "      <td>-0.982768</td>\n",
              "      <td>-0.876125</td>\n",
              "      <td>-0.972332</td>\n",
              "      <td>-0.974868</td>\n",
              "      <td>-0.979186</td>\n",
              "      <td>-0.979186</td>\n",
              "      <td>-0.973516</td>\n",
              "      <td>-0.974138</td>\n",
              "      <td>-0.970763</td>\n",
              "      <td>-0.973466</td>\n",
              "      <td>-0.940751</td>\n",
              "      <td>-0.972613</td>\n",
              "      <td>-0.954298</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.789720</td>\n",
              "      <td>-0.720941</td>\n",
              "      <td>-0.194676</td>\n",
              "      <td>-0.663372</td>\n",
              "      <td>-0.714922</td>\n",
              "      <td>-0.853870</td>\n",
              "      <td>-0.807510</td>\n",
              "      <td>-0.837513</td>\n",
              "      <td>-0.360836</td>\n",
              "      <td>-0.560288</td>\n",
              "      <td>-0.560288</td>\n",
              "      <td>-0.792254</td>\n",
              "      <td>-0.600419</td>\n",
              "      <td>-0.508069</td>\n",
              "      <td>-0.623026</td>\n",
              "      <td>-0.349791</td>\n",
              "      <td>-0.189199</td>\n",
              "      <td>-0.189199</td>\n",
              "      <td>-0.666421</td>\n",
              "      <td>-0.189199</td>\n",
              "      <td>-0.189199</td>\n",
              "      <td>-0.566789</td>\n",
              "      <td>-0.516308</td>\n",
              "      <td>-0.848439</td>\n",
              "      <td>-0.193651</td>\n",
              "      <td>-0.193651</td>\n",
              "      <td>-0.193651</td>\n",
              "      <td>-0.193651</td>\n",
              "      <td>-0.193651</td>\n",
              "      <td>-0.193651</td>\n",
              "      <td>-0.193651</td>\n",
              "      <td>-0.693386</td>\n",
              "      <td>-0.193651</td>\n",
              "      <td>-0.820575</td>\n",
              "      <td>-0.842615</td>\n",
              "      <td>-0.542675</td>\n",
              "      <td>-0.704686</td>\n",
              "      <td>-0.733858</td>\n",
              "      <td>-0.210927</td>\n",
              "      <td>-0.220766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.990951</td>\n",
              "      <td>-0.989762</td>\n",
              "      <td>-0.997945</td>\n",
              "      <td>460.235478</td>\n",
              "      <td>-0.992646</td>\n",
              "      <td>0.364792</td>\n",
              "      <td>-0.938280</td>\n",
              "      <td>-0.990007</td>\n",
              "      <td>-0.981447</td>\n",
              "      <td>-0.993273</td>\n",
              "      <td>-0.996718</td>\n",
              "      <td>-0.252545</td>\n",
              "      <td>-0.985143</td>\n",
              "      <td>-0.983954</td>\n",
              "      <td>-0.935212</td>\n",
              "      <td>-0.861851</td>\n",
              "      <td>-0.985297</td>\n",
              "      <td>-0.988181</td>\n",
              "      <td>-0.935212</td>\n",
              "      <td>-0.048653</td>\n",
              "      <td>-0.048653</td>\n",
              "      <td>-0.988892</td>\n",
              "      <td>-0.940470</td>\n",
              "      <td>-0.940470</td>\n",
              "      <td>-0.917559</td>\n",
              "      <td>-0.988977</td>\n",
              "      <td>-0.930514</td>\n",
              "      <td>-0.986390</td>\n",
              "      <td>-0.860393</td>\n",
              "      <td>-0.971139</td>\n",
              "      <td>-0.979066</td>\n",
              "      <td>-0.981261</td>\n",
              "      <td>-0.981261</td>\n",
              "      <td>-0.975051</td>\n",
              "      <td>-0.979785</td>\n",
              "      <td>-0.969355</td>\n",
              "      <td>-0.977203</td>\n",
              "      <td>-0.935713</td>\n",
              "      <td>-0.977652</td>\n",
              "      <td>-0.954538</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.808236</td>\n",
              "      <td>-0.749512</td>\n",
              "      <td>-0.239738</td>\n",
              "      <td>-0.695684</td>\n",
              "      <td>-0.739056</td>\n",
              "      <td>-0.871362</td>\n",
              "      <td>-0.827892</td>\n",
              "      <td>-0.856024</td>\n",
              "      <td>-0.401459</td>\n",
              "      <td>-0.594890</td>\n",
              "      <td>-0.594890</td>\n",
              "      <td>-0.816885</td>\n",
              "      <td>-0.633293</td>\n",
              "      <td>-0.545280</td>\n",
              "      <td>-0.654859</td>\n",
              "      <td>-0.391430</td>\n",
              "      <td>-0.233375</td>\n",
              "      <td>-0.233375</td>\n",
              "      <td>-0.698705</td>\n",
              "      <td>-0.233375</td>\n",
              "      <td>-0.233375</td>\n",
              "      <td>-0.601639</td>\n",
              "      <td>-0.552052</td>\n",
              "      <td>-0.864969</td>\n",
              "      <td>-0.238547</td>\n",
              "      <td>-0.238547</td>\n",
              "      <td>-0.238547</td>\n",
              "      <td>-0.238547</td>\n",
              "      <td>-0.238547</td>\n",
              "      <td>-0.238547</td>\n",
              "      <td>-0.238547</td>\n",
              "      <td>-0.722535</td>\n",
              "      <td>-0.238547</td>\n",
              "      <td>-0.842071</td>\n",
              "      <td>-0.859996</td>\n",
              "      <td>-0.578001</td>\n",
              "      <td>-0.731893</td>\n",
              "      <td>-0.760069</td>\n",
              "      <td>-0.258574</td>\n",
              "      <td>-0.269946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.974451</td>\n",
              "      <td>-0.975281</td>\n",
              "      <td>-0.987169</td>\n",
              "      <td>-0.992646</td>\n",
              "      <td>502.678560</td>\n",
              "      <td>0.369408</td>\n",
              "      <td>-0.928937</td>\n",
              "      <td>-0.994424</td>\n",
              "      <td>-0.982059</td>\n",
              "      <td>-0.984134</td>\n",
              "      <td>-0.998081</td>\n",
              "      <td>-0.215842</td>\n",
              "      <td>-0.991516</td>\n",
              "      <td>-0.996364</td>\n",
              "      <td>-0.940764</td>\n",
              "      <td>-0.863106</td>\n",
              "      <td>-0.997725</td>\n",
              "      <td>-0.998680</td>\n",
              "      <td>-0.940764</td>\n",
              "      <td>-0.035823</td>\n",
              "      <td>-0.035823</td>\n",
              "      <td>-0.997468</td>\n",
              "      <td>-0.949040</td>\n",
              "      <td>-0.949040</td>\n",
              "      <td>-0.956194</td>\n",
              "      <td>-0.999352</td>\n",
              "      <td>-0.950076</td>\n",
              "      <td>-0.998108</td>\n",
              "      <td>-0.886095</td>\n",
              "      <td>-0.986638</td>\n",
              "      <td>-0.995471</td>\n",
              "      <td>-0.995002</td>\n",
              "      <td>-0.995002</td>\n",
              "      <td>-0.991562</td>\n",
              "      <td>-0.996440</td>\n",
              "      <td>-0.985407</td>\n",
              "      <td>-0.994315</td>\n",
              "      <td>-0.959419</td>\n",
              "      <td>-0.995227</td>\n",
              "      <td>-0.978291</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.873575</td>\n",
              "      <td>-0.821994</td>\n",
              "      <td>-0.348788</td>\n",
              "      <td>-0.774228</td>\n",
              "      <td>-0.814820</td>\n",
              "      <td>-0.924227</td>\n",
              "      <td>-0.889442</td>\n",
              "      <td>-0.912148</td>\n",
              "      <td>-0.504871</td>\n",
              "      <td>-0.684892</td>\n",
              "      <td>-0.684892</td>\n",
              "      <td>-0.879154</td>\n",
              "      <td>-0.719831</td>\n",
              "      <td>-0.638857</td>\n",
              "      <td>-0.739288</td>\n",
              "      <td>-0.494776</td>\n",
              "      <td>-0.343465</td>\n",
              "      <td>-0.343465</td>\n",
              "      <td>-0.776820</td>\n",
              "      <td>-0.343465</td>\n",
              "      <td>-0.343465</td>\n",
              "      <td>-0.690724</td>\n",
              "      <td>-0.645900</td>\n",
              "      <td>-0.919346</td>\n",
              "      <td>-0.347797</td>\n",
              "      <td>-0.347797</td>\n",
              "      <td>-0.347797</td>\n",
              "      <td>-0.347797</td>\n",
              "      <td>-0.347797</td>\n",
              "      <td>-0.347797</td>\n",
              "      <td>-0.347797</td>\n",
              "      <td>-0.798977</td>\n",
              "      <td>-0.347797</td>\n",
              "      <td>-0.900440</td>\n",
              "      <td>-0.915399</td>\n",
              "      <td>-0.669409</td>\n",
              "      <td>-0.807806</td>\n",
              "      <td>-0.831953</td>\n",
              "      <td>-0.364102</td>\n",
              "      <td>-0.372986</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 748 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0           1           2    ...       745       746       747\n",
              "0  419.387861   -0.970991   -0.984727  ... -0.716943 -0.230874 -0.247645\n",
              "1   -0.970991  442.203316   -0.996869  ... -0.699586 -0.153627 -0.161382\n",
              "2   -0.984727   -0.996869  452.615620  ... -0.733858 -0.210927 -0.220766\n",
              "3   -0.990951   -0.989762   -0.997945  ... -0.760069 -0.258574 -0.269946\n",
              "4   -0.974451   -0.975281   -0.987169  ... -0.831953 -0.364102 -0.372986\n",
              "\n",
              "[5 rows x 748 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BY0plC58yhG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "4d1b40f1-1997-4d69-d543-8f34dcbfc3e3"
      },
      "source": [
        "W_t = corrMatrix.transpose()\n",
        "X_t = X.transpose()\n",
        "print(W_t.shape)\n",
        "print(X_t.shape)\n",
        "print(laplacianMatrix.shape)\n",
        "print(X.shape)\n",
        "print(corrMatrix.shape)\n",
        "A_reg = W_t.dot(laplacianMatrix)\n",
        "B_reg = X_t.dot(X)\n",
        "C_reg = W_t.dot(laplacianMatrix)\n",
        "print(A_reg.shape)\n",
        "print(B_reg.shape)\n",
        "print(C_reg.shape)\n",
        "reg = A_reg.dot(B_reg.dot(C_reg.dot(corrMatrix)))\n",
        "print(reg.shape)\n",
        "# print(reg)\n",
        "regTerm = np.trace(reg) # R(W)\n",
        "print(regTerm)\n",
        "ho2=0.001\n",
        "print(regTerm*ho2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(748, 748)\n",
            "(4, 748)\n",
            "(748, 748)\n",
            "(748, 4)\n",
            "(748, 748)\n",
            "(748, 748)\n",
            "(4, 4)\n",
            "(748, 748)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c225311d05a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# print(reg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (4,4) and (748,748) not aligned: 4 (dim 1) != 748 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR6RzmkH20vo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "52a3790f-30f3-4beb-d557-42c27dd2a6fe"
      },
      "source": [
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(corrMatrix.shape)\n",
        "print(X.dot(corrMatrix).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(748, 4)\n",
            "(748,)\n",
            "(4, 4)\n",
            "(748, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFKpdrD-xMr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "8919c727-818b-4089-840d-eb340b534bb4"
      },
      "source": [
        "from sklearn import linear_model\n",
        "reg = linear_model.Lasso(alpha=0.01)\n",
        "reg.fit(X.dot(corrMatrix), X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5603.820574483131, tolerance: 4.895496657754011\n",
            "  positive)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 50012.949679602396, tolerance: 44.38854799465241\n",
            "  positive)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,\n",
              "      normalize=False, positive=False, precompute=False, random_state=None,\n",
              "      selection='cyclic', tol=0.0001, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEH9lCW4xXRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4dcf52fb-5cd7-47f4-c113-8387375eac86"
      },
      "source": [
        "# print(reg.coef_.shape)\n",
        "print(regTerm)\n",
        "# print(reg.coef_)\n",
        "print(ho2*regTerm)\n",
        "# print(reg.coef_ - ho2*regTerm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "229674207.52050352\n",
            "229674.20752050352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWwbhlng7SEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(reg.coef_)\n",
        "# target = reg.coef_- ho2*regTerm #Eq.7\n",
        "\n",
        "df = pd.DataFrame(data=reg.coef_)\n",
        "# print(target.shape)\n",
        "\n",
        "# df = pd.DataFrame(data=target)\n",
        "# print(target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5vb4dyywc7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "28a4d876-8829-48b1-badc-f98e5808fea0"
      },
      "source": [
        "# Visualize data \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.529300</td>\n",
              "      <td>0.087803</td>\n",
              "      <td>1.862594e-17</td>\n",
              "      <td>0.010738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.006134</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>9.811713e-20</td>\n",
              "      <td>0.000722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.545094</td>\n",
              "      <td>0.593034</td>\n",
              "      <td>3.899551e-15</td>\n",
              "      <td>0.182219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.164960</td>\n",
              "      <td>0.130650</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.141323</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1             2         3\n",
              "0  0.529300  0.087803  1.862594e-17  0.010738\n",
              "1 -0.006134  0.002385  9.811713e-20  0.000722\n",
              "2 -1.545094  0.593034  3.899551e-15  0.182219\n",
              "3  1.164960  0.130650  0.000000e+00  0.141323"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrvDmQcuDZ7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04cb0189-455c-4761-e78b-85e2fa8f7bee"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers, layers\n",
        "import numpy as np\n",
        "\n",
        "# number_of_inputs = 4*748\n",
        "# number_of_inputs = 15\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(4)),\n",
        "    layers.Dense(1, use_bias=False, kernel_regularizer=regularizers.l1(0.01))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='mse')\n",
        "\n",
        "# a = np.array([1,0,3,0,5,0,7,0,0,0,0,0,0,0,0])\n",
        "# N = 1000\n",
        "# x = np.random.uniform(size=(N, number_of_inputs))\n",
        "# print(x.shape)\n",
        "# # print(a.shape)\n",
        "# y = x @ a + np.random.normal(scale=1e-1, size=(N,)); \n",
        "# print(y.shape)\n",
        "\n",
        "XW = X.dot(corrMatrix)\n",
        "print(XW.shape)\n",
        "print(X[:,0].shape)\n",
        "X_fake = X[:,0]\n",
        "\n",
        "# XW = X.dot(corrMatrix).flatten(order='F')\n",
        "# X_flat = X.flatten(order='F')\n",
        "# model.fit(XW, X_flat, epochs=200)\n",
        "\n",
        "model.fit(XW, X_fake, epochs=200)\n",
        "\n",
        "print(model.layers[0].get_config())\n",
        "print(model.layers[0].get_weights())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(748, 4)\n",
            "(748,)\n",
            "Epoch 1/200\n",
            "24/24 [==============================] - 0s 860us/step - loss: nan   \n",
            "Epoch 2/200\n",
            "24/24 [==============================] - 0s 864us/step - loss: nan\n",
            "Epoch 3/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 4/200\n",
            "24/24 [==============================] - 0s 934us/step - loss: nan\n",
            "Epoch 5/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 6/200\n",
            "24/24 [==============================] - 0s 793us/step - loss: nan\n",
            "Epoch 7/200\n",
            "24/24 [==============================] - 0s 804us/step - loss: nan\n",
            "Epoch 8/200\n",
            "24/24 [==============================] - 0s 966us/step - loss: nan\n",
            "Epoch 9/200\n",
            "24/24 [==============================] - 0s 881us/step - loss: nan\n",
            "Epoch 10/200\n",
            "24/24 [==============================] - 0s 826us/step - loss: nan\n",
            "Epoch 11/200\n",
            "24/24 [==============================] - 0s 925us/step - loss: nan\n",
            "Epoch 12/200\n",
            "24/24 [==============================] - 0s 879us/step - loss: nan\n",
            "Epoch 13/200\n",
            "24/24 [==============================] - 0s 874us/step - loss: nan\n",
            "Epoch 14/200\n",
            "24/24 [==============================] - 0s 812us/step - loss: nan\n",
            "Epoch 15/200\n",
            "24/24 [==============================] - 0s 812us/step - loss: nan\n",
            "Epoch 16/200\n",
            "24/24 [==============================] - 0s 755us/step - loss: nan\n",
            "Epoch 17/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 18/200\n",
            "24/24 [==============================] - 0s 945us/step - loss: nan\n",
            "Epoch 19/200\n",
            "24/24 [==============================] - 0s 949us/step - loss: nan\n",
            "Epoch 20/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 21/200\n",
            "24/24 [==============================] - 0s 845us/step - loss: nan\n",
            "Epoch 22/200\n",
            "24/24 [==============================] - 0s 893us/step - loss: nan\n",
            "Epoch 23/200\n",
            "24/24 [==============================] - 0s 845us/step - loss: nan\n",
            "Epoch 24/200\n",
            "24/24 [==============================] - 0s 867us/step - loss: nan\n",
            "Epoch 25/200\n",
            "24/24 [==============================] - 0s 836us/step - loss: nan\n",
            "Epoch 26/200\n",
            "24/24 [==============================] - 0s 939us/step - loss: nan\n",
            "Epoch 27/200\n",
            "24/24 [==============================] - 0s 837us/step - loss: nan\n",
            "Epoch 28/200\n",
            "24/24 [==============================] - 0s 852us/step - loss: nan\n",
            "Epoch 29/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 30/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 31/200\n",
            "24/24 [==============================] - 0s 839us/step - loss: nan\n",
            "Epoch 32/200\n",
            "24/24 [==============================] - 0s 849us/step - loss: nan\n",
            "Epoch 33/200\n",
            "24/24 [==============================] - 0s 835us/step - loss: nan\n",
            "Epoch 34/200\n",
            "24/24 [==============================] - 0s 856us/step - loss: nan\n",
            "Epoch 35/200\n",
            "24/24 [==============================] - 0s 937us/step - loss: nan\n",
            "Epoch 36/200\n",
            "24/24 [==============================] - 0s 935us/step - loss: nan\n",
            "Epoch 37/200\n",
            "24/24 [==============================] - 0s 861us/step - loss: nan\n",
            "Epoch 38/200\n",
            "24/24 [==============================] - 0s 809us/step - loss: nan\n",
            "Epoch 39/200\n",
            "24/24 [==============================] - 0s 826us/step - loss: nan\n",
            "Epoch 40/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 41/200\n",
            "24/24 [==============================] - 0s 844us/step - loss: nan\n",
            "Epoch 42/200\n",
            "24/24 [==============================] - 0s 937us/step - loss: nan\n",
            "Epoch 43/200\n",
            "24/24 [==============================] - 0s 941us/step - loss: nan\n",
            "Epoch 44/200\n",
            "24/24 [==============================] - 0s 954us/step - loss: nan\n",
            "Epoch 45/200\n",
            "24/24 [==============================] - 0s 874us/step - loss: nan\n",
            "Epoch 46/200\n",
            "24/24 [==============================] - 0s 842us/step - loss: nan\n",
            "Epoch 47/200\n",
            "24/24 [==============================] - 0s 873us/step - loss: nan\n",
            "Epoch 48/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 49/200\n",
            "24/24 [==============================] - 0s 811us/step - loss: nan\n",
            "Epoch 50/200\n",
            "24/24 [==============================] - 0s 902us/step - loss: nan\n",
            "Epoch 51/200\n",
            "24/24 [==============================] - 0s 862us/step - loss: nan\n",
            "Epoch 52/200\n",
            "24/24 [==============================] - 0s 847us/step - loss: nan\n",
            "Epoch 53/200\n",
            "24/24 [==============================] - 0s 850us/step - loss: nan\n",
            "Epoch 54/200\n",
            "24/24 [==============================] - 0s 936us/step - loss: nan\n",
            "Epoch 55/200\n",
            "24/24 [==============================] - 0s 928us/step - loss: nan\n",
            "Epoch 56/200\n",
            "24/24 [==============================] - 0s 904us/step - loss: nan\n",
            "Epoch 57/200\n",
            "24/24 [==============================] - 0s 879us/step - loss: nan\n",
            "Epoch 58/200\n",
            "24/24 [==============================] - 0s 792us/step - loss: nan\n",
            "Epoch 59/200\n",
            "24/24 [==============================] - 0s 992us/step - loss: nan\n",
            "Epoch 60/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 61/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 62/200\n",
            "24/24 [==============================] - 0s 809us/step - loss: nan\n",
            "Epoch 63/200\n",
            "24/24 [==============================] - 0s 927us/step - loss: nan\n",
            "Epoch 64/200\n",
            "24/24 [==============================] - 0s 864us/step - loss: nan\n",
            "Epoch 65/200\n",
            "24/24 [==============================] - 0s 848us/step - loss: nan\n",
            "Epoch 66/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 67/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 68/200\n",
            "24/24 [==============================] - 0s 879us/step - loss: nan\n",
            "Epoch 69/200\n",
            "24/24 [==============================] - 0s 828us/step - loss: nan\n",
            "Epoch 70/200\n",
            "24/24 [==============================] - 0s 882us/step - loss: nan\n",
            "Epoch 71/200\n",
            "24/24 [==============================] - 0s 847us/step - loss: nan\n",
            "Epoch 72/200\n",
            "24/24 [==============================] - 0s 881us/step - loss: nan\n",
            "Epoch 73/200\n",
            "24/24 [==============================] - 0s 806us/step - loss: nan\n",
            "Epoch 74/200\n",
            "24/24 [==============================] - 0s 840us/step - loss: nan\n",
            "Epoch 75/200\n",
            "24/24 [==============================] - 0s 779us/step - loss: nan\n",
            "Epoch 76/200\n",
            "24/24 [==============================] - 0s 792us/step - loss: nan\n",
            "Epoch 77/200\n",
            "24/24 [==============================] - 0s 859us/step - loss: nan\n",
            "Epoch 78/200\n",
            "24/24 [==============================] - 0s 886us/step - loss: nan\n",
            "Epoch 79/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 80/200\n",
            "24/24 [==============================] - 0s 959us/step - loss: nan\n",
            "Epoch 81/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 82/200\n",
            "24/24 [==============================] - 0s 992us/step - loss: nan\n",
            "Epoch 83/200\n",
            "24/24 [==============================] - 0s 933us/step - loss: nan\n",
            "Epoch 84/200\n",
            "24/24 [==============================] - 0s 860us/step - loss: nan\n",
            "Epoch 85/200\n",
            "24/24 [==============================] - 0s 844us/step - loss: nan\n",
            "Epoch 86/200\n",
            "24/24 [==============================] - 0s 835us/step - loss: nan\n",
            "Epoch 87/200\n",
            "24/24 [==============================] - 0s 978us/step - loss: nan\n",
            "Epoch 88/200\n",
            "24/24 [==============================] - 0s 838us/step - loss: nan\n",
            "Epoch 89/200\n",
            "24/24 [==============================] - 0s 841us/step - loss: nan\n",
            "Epoch 90/200\n",
            "24/24 [==============================] - 0s 883us/step - loss: nan\n",
            "Epoch 91/200\n",
            "24/24 [==============================] - 0s 926us/step - loss: nan\n",
            "Epoch 92/200\n",
            "24/24 [==============================] - 0s 928us/step - loss: nan\n",
            "Epoch 93/200\n",
            "24/24 [==============================] - 0s 952us/step - loss: nan\n",
            "Epoch 94/200\n",
            "24/24 [==============================] - 0s 844us/step - loss: nan\n",
            "Epoch 95/200\n",
            "24/24 [==============================] - 0s 856us/step - loss: nan\n",
            "Epoch 96/200\n",
            "24/24 [==============================] - 0s 922us/step - loss: nan\n",
            "Epoch 97/200\n",
            "24/24 [==============================] - 0s 890us/step - loss: nan\n",
            "Epoch 98/200\n",
            "24/24 [==============================] - 0s 911us/step - loss: nan\n",
            "Epoch 99/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 100/200\n",
            "24/24 [==============================] - 0s 770us/step - loss: nan\n",
            "Epoch 101/200\n",
            "24/24 [==============================] - 0s 959us/step - loss: nan\n",
            "Epoch 102/200\n",
            "24/24 [==============================] - 0s 890us/step - loss: nan\n",
            "Epoch 103/200\n",
            "24/24 [==============================] - 0s 954us/step - loss: nan\n",
            "Epoch 104/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 105/200\n",
            "24/24 [==============================] - 0s 933us/step - loss: nan\n",
            "Epoch 106/200\n",
            "24/24 [==============================] - 0s 917us/step - loss: nan\n",
            "Epoch 107/200\n",
            "24/24 [==============================] - 0s 846us/step - loss: nan\n",
            "Epoch 108/200\n",
            "24/24 [==============================] - 0s 975us/step - loss: nan\n",
            "Epoch 109/200\n",
            "24/24 [==============================] - 0s 880us/step - loss: nan\n",
            "Epoch 110/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 111/200\n",
            "24/24 [==============================] - 0s 887us/step - loss: nan\n",
            "Epoch 112/200\n",
            "24/24 [==============================] - 0s 916us/step - loss: nan\n",
            "Epoch 113/200\n",
            "24/24 [==============================] - 0s 793us/step - loss: nan\n",
            "Epoch 114/200\n",
            "24/24 [==============================] - 0s 891us/step - loss: nan\n",
            "Epoch 115/200\n",
            "24/24 [==============================] - 0s 932us/step - loss: nan\n",
            "Epoch 116/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 117/200\n",
            "24/24 [==============================] - 0s 925us/step - loss: nan\n",
            "Epoch 118/200\n",
            "24/24 [==============================] - 0s 935us/step - loss: nan\n",
            "Epoch 119/200\n",
            "24/24 [==============================] - 0s 968us/step - loss: nan\n",
            "Epoch 120/200\n",
            "24/24 [==============================] - 0s 783us/step - loss: nan\n",
            "Epoch 121/200\n",
            "24/24 [==============================] - 0s 824us/step - loss: nan\n",
            "Epoch 122/200\n",
            "24/24 [==============================] - 0s 996us/step - loss: nan\n",
            "Epoch 123/200\n",
            "24/24 [==============================] - 0s 862us/step - loss: nan\n",
            "Epoch 124/200\n",
            "24/24 [==============================] - 0s 967us/step - loss: nan\n",
            "Epoch 125/200\n",
            "24/24 [==============================] - 0s 903us/step - loss: nan\n",
            "Epoch 126/200\n",
            "24/24 [==============================] - 0s 863us/step - loss: nan\n",
            "Epoch 127/200\n",
            "24/24 [==============================] - 0s 955us/step - loss: nan\n",
            "Epoch 128/200\n",
            "24/24 [==============================] - 0s 814us/step - loss: nan\n",
            "Epoch 129/200\n",
            "24/24 [==============================] - 0s 837us/step - loss: nan\n",
            "Epoch 130/200\n",
            "24/24 [==============================] - 0s 989us/step - loss: nan\n",
            "Epoch 131/200\n",
            "24/24 [==============================] - 0s 967us/step - loss: nan\n",
            "Epoch 132/200\n",
            "24/24 [==============================] - 0s 890us/step - loss: nan\n",
            "Epoch 133/200\n",
            "24/24 [==============================] - 0s 827us/step - loss: nan\n",
            "Epoch 134/200\n",
            "24/24 [==============================] - 0s 916us/step - loss: nan\n",
            "Epoch 135/200\n",
            "24/24 [==============================] - 0s 817us/step - loss: nan\n",
            "Epoch 136/200\n",
            "24/24 [==============================] - 0s 761us/step - loss: nan\n",
            "Epoch 137/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 138/200\n",
            "24/24 [==============================] - 0s 853us/step - loss: nan\n",
            "Epoch 139/200\n",
            "24/24 [==============================] - 0s 869us/step - loss: nan\n",
            "Epoch 140/200\n",
            "24/24 [==============================] - 0s 944us/step - loss: nan\n",
            "Epoch 141/200\n",
            "24/24 [==============================] - 0s 873us/step - loss: nan\n",
            "Epoch 142/200\n",
            "24/24 [==============================] - 0s 884us/step - loss: nan\n",
            "Epoch 143/200\n",
            "24/24 [==============================] - 0s 842us/step - loss: nan\n",
            "Epoch 144/200\n",
            "24/24 [==============================] - 0s 961us/step - loss: nan\n",
            "Epoch 145/200\n",
            "24/24 [==============================] - 0s 937us/step - loss: nan\n",
            "Epoch 146/200\n",
            "24/24 [==============================] - 0s 858us/step - loss: nan\n",
            "Epoch 147/200\n",
            "24/24 [==============================] - 0s 817us/step - loss: nan\n",
            "Epoch 148/200\n",
            "24/24 [==============================] - 0s 889us/step - loss: nan\n",
            "Epoch 149/200\n",
            "24/24 [==============================] - 0s 834us/step - loss: nan\n",
            "Epoch 150/200\n",
            "24/24 [==============================] - 0s 831us/step - loss: nan\n",
            "Epoch 151/200\n",
            "24/24 [==============================] - 0s 827us/step - loss: nan\n",
            "Epoch 152/200\n",
            "24/24 [==============================] - 0s 829us/step - loss: nan\n",
            "Epoch 153/200\n",
            "24/24 [==============================] - 0s 891us/step - loss: nan\n",
            "Epoch 154/200\n",
            "24/24 [==============================] - 0s 882us/step - loss: nan\n",
            "Epoch 155/200\n",
            "24/24 [==============================] - 0s 996us/step - loss: nan\n",
            "Epoch 156/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 157/200\n",
            "24/24 [==============================] - 0s 845us/step - loss: nan\n",
            "Epoch 158/200\n",
            "24/24 [==============================] - 0s 890us/step - loss: nan\n",
            "Epoch 159/200\n",
            "24/24 [==============================] - 0s 863us/step - loss: nan\n",
            "Epoch 160/200\n",
            "24/24 [==============================] - 0s 850us/step - loss: nan\n",
            "Epoch 161/200\n",
            "24/24 [==============================] - 0s 935us/step - loss: nan\n",
            "Epoch 162/200\n",
            "24/24 [==============================] - 0s 838us/step - loss: nan\n",
            "Epoch 163/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 164/200\n",
            "24/24 [==============================] - 0s 868us/step - loss: nan\n",
            "Epoch 165/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 166/200\n",
            "24/24 [==============================] - 0s 881us/step - loss: nan\n",
            "Epoch 167/200\n",
            "24/24 [==============================] - 0s 861us/step - loss: nan\n",
            "Epoch 168/200\n",
            "24/24 [==============================] - 0s 829us/step - loss: nan\n",
            "Epoch 169/200\n",
            "24/24 [==============================] - 0s 871us/step - loss: nan\n",
            "Epoch 170/200\n",
            "24/24 [==============================] - 0s 893us/step - loss: nan\n",
            "Epoch 171/200\n",
            "24/24 [==============================] - 0s 868us/step - loss: nan\n",
            "Epoch 172/200\n",
            "24/24 [==============================] - 0s 823us/step - loss: nan\n",
            "Epoch 173/200\n",
            "24/24 [==============================] - 0s 950us/step - loss: nan\n",
            "Epoch 174/200\n",
            "24/24 [==============================] - 0s 856us/step - loss: nan\n",
            "Epoch 175/200\n",
            "24/24 [==============================] - 0s 817us/step - loss: nan\n",
            "Epoch 176/200\n",
            "24/24 [==============================] - 0s 858us/step - loss: nan\n",
            "Epoch 177/200\n",
            "24/24 [==============================] - 0s 844us/step - loss: nan\n",
            "Epoch 178/200\n",
            "24/24 [==============================] - 0s 996us/step - loss: nan\n",
            "Epoch 179/200\n",
            "24/24 [==============================] - 0s 916us/step - loss: nan\n",
            "Epoch 180/200\n",
            "24/24 [==============================] - 0s 804us/step - loss: nan\n",
            "Epoch 181/200\n",
            "24/24 [==============================] - 0s 956us/step - loss: nan\n",
            "Epoch 182/200\n",
            "24/24 [==============================] - 0s 906us/step - loss: nan\n",
            "Epoch 183/200\n",
            "24/24 [==============================] - 0s 939us/step - loss: nan\n",
            "Epoch 184/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 185/200\n",
            "24/24 [==============================] - 0s 813us/step - loss: nan\n",
            "Epoch 186/200\n",
            "24/24 [==============================] - 0s 956us/step - loss: nan\n",
            "Epoch 187/200\n",
            "24/24 [==============================] - 0s 871us/step - loss: nan\n",
            "Epoch 188/200\n",
            "24/24 [==============================] - 0s 882us/step - loss: nan\n",
            "Epoch 189/200\n",
            "24/24 [==============================] - 0s 870us/step - loss: nan\n",
            "Epoch 190/200\n",
            "24/24 [==============================] - 0s 861us/step - loss: nan\n",
            "Epoch 191/200\n",
            "24/24 [==============================] - 0s 868us/step - loss: nan\n",
            "Epoch 192/200\n",
            "24/24 [==============================] - 0s 780us/step - loss: nan\n",
            "Epoch 193/200\n",
            "24/24 [==============================] - 0s 863us/step - loss: nan\n",
            "Epoch 194/200\n",
            "24/24 [==============================] - 0s 752us/step - loss: nan\n",
            "Epoch 195/200\n",
            "24/24 [==============================] - 0s 760us/step - loss: nan\n",
            "Epoch 196/200\n",
            "24/24 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 197/200\n",
            "24/24 [==============================] - 0s 813us/step - loss: nan\n",
            "Epoch 198/200\n",
            "24/24 [==============================] - 0s 802us/step - loss: nan\n",
            "Epoch 199/200\n",
            "24/24 [==============================] - 0s 867us/step - loss: nan\n",
            "Epoch 200/200\n",
            "24/24 [==============================] - 0s 928us/step - loss: nan\n",
            "{'name': 'dense_21', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': False, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': {'class_name': 'L1', 'config': {'l1': 0.009999999776482582}}, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
            "[array([[nan],\n",
            "       [nan],\n",
            "       [nan],\n",
            "       [nan]], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uLgDb5ewc76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2b35f117-19e2-449a-b2cd-1d59fffbaf83"
      },
      "source": [
        "# Training\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td_u0MFdwc7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4YjI9-awc8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "3891fda9-612c-4011-cd2b-365bd849ac87"
      },
      "source": [
        "# Evaluating the algorithm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14  1  0]\n",
            " [ 0  9  0]\n",
            " [ 0  2  4]]\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "    Iris-setosa       1.00      0.93      0.97        15\n",
            "Iris-versicolor       0.75      1.00      0.86         9\n",
            " Iris-virginica       1.00      0.67      0.80         6\n",
            "\n",
            "       accuracy                           0.90        30\n",
            "      macro avg       0.92      0.87      0.87        30\n",
            "   weighted avg       0.93      0.90      0.90        30\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGf00EO3wc8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extra: Comparing Error Rate with the K Value\n",
        "error = []\n",
        "\n",
        "# Calculating error for K values between 1 and 40\n",
        "for i in range(1, 40):\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(X_train, y_train)\n",
        "    pred_i = knn.predict(X_test)\n",
        "    error.append(np.mean(pred_i != y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCPkmXmuwc8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "542b8bc7-8d88-4572-a033-d21f5daf0898"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n",
        "         markerfacecolor='blue', markersize=10)\n",
        "plt.title('Error Rate K Value')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Mean Error')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Mean Error')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAGDCAYAAADgeTwhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU1f3/8dcnASIBAiKLIiiCgsoiSmBi7bdat2pbpS5t3fdqRdufdKXtt61av9Yu1mrVVutSrV93236xgtZqtbU4A8EdBQ0oiKhsghBIgOT8/jiTEkKWSWbulryfj8d9JHPvPfd8ApPJZ86c+znmnENERERERPJXFHUAIiIiIiKdhZJrEREREZECUXItIiIiIlIgSq5FRERERApEybWIiIiISIEouRYRERERKRAl1yIiEitm9oyZXRB1HCIiHaHkWkQkB2b2jpltMrMNjbYbQ47hGTOryfa9ysz+ZGa75dj2MDNblkff27U3sx7Z/v9tZmVNzp1uZv9s5hoDzGyzmY3taBwiInGn5FpEJHfHOed6N9oube4kM+vWzL7i9nTUyvmXOud6A3sDvYFftue6hWBmJcCfgH7A0c65j5uccg/wCTPbq8n+U4BXnXOvhRCmiEgklFyLiOTJzM7JjuBeZ2argcvN7A9m9lszm2lm1cCnzWy/7OjzWjObb2bHN7rGDue31qdzbi3wF2BCo2uca2ZvmNl6M1tsZhdl9/cCZgFDGo26DzGzouwo8yIzW21mD5pZ/zZ+1lLgUaAb8DnnXHUzsS0DngbObHLoLOBuM9vZzP5qZivN7KPs90Nb6O9yM7un0ePhZuYa3sCYWV8zu93M3jez98zsqva+kRERKSQl1yIihZECFgODgf/J7jst+30fIINPSv8GDAK+BvyvmY1udI3G5z/XWmdmtgtwIlDVaPcK4PNAGXAucJ2ZHZRNgI8FljcadV+ejeELwKHAEOAj4KZWui3BJ+k1wBTn3KZWzr2LRsl19uecANyL/9tzJ7AnsAewCejoFJs/AFvxI/kHAkcDmq8tIpFRci0ikru/ZEedG7avNDq23Dn3G+fc1kZJ5/855/7tnKvHJ5a9gWucc5udc08DfwVObXSN/5zvnKtpIYYbzGwdsAoYgE+QAXDOPeacW+S8Z/GJ/H+18vN8FfiBc26Zc64WuBw4ublpLVl9gIOBu7Lnt+bPwGAz+0T28VnALOfcSufcaufcI865jc659fg3FIe2cb0dmNlg4LPAZc65aufcCuA6/PQTEZFIKLkWEcndF5xz/Rptv2907N1mzm+8bwjwbjbRbrAE2L2NazT1dedcX2A8sDPwn+kUZnasmaXNbI2ZrcUnngNaudaewJ8b3iwAbwB1+NH35qzCJ653mdlnWgvSObcReAg4y8wMOB24OxtnqZndYmZLzOxj4J9Avw5M59gT6A683+hnuAX/yYCISCSUXIuIFIZrY99yYJiZNX7d3QN4r41rNN+Zc68CVwE3mVcCPIK/wXGwc64fMBOwVq79LnBskzcMOznn3mvm3IZ+/wR8BXjYzFqdF46fGvIl4Cj8qPej2f3fBEYDKedcGfCp7H7b4QpQDZQ2erxrk/hrgQGN4i9zzo1pIy4RkcAouRYRCUcG2Ah8x8y6m9lhwHHA/Xlc8y78KPPxQA/8nOiVwFYzOxY//7jBh8AuZta30b7fAf9jZnsCmNlAM5vSVqfOufuAS4H/M7NDWjn1X8Ba4Fbgfufc5uz+Pvh51muzN1D+uJVrvAR8ysz2yMb+vUZxvI+f+nKtmZVlb9AcaWbtnmIiIlIoSq5FRHL3qG1f5/rPuTbMJpbH4W8sXAXcDJzlnFvQ0WCy17we+GF27vLXgQfxNyaeBsxodO4C4D5gcXYKxZBs2xnA38xsPZDG35iZS9934UegHzOzyS2c4/BTQfbMfm3wa6An/t8hDTzeSj9PAg8ArwDz8PPUGzsL/8bi9ezP/TCQU+1vEZEgmH/tExERERGRfGnkWkRERESkQAJNrs3sGDNbaGZVZja9meOfMrMXzGyrmZ3czPEyM1tmIS8xLCIiIiLSEYEl19mSSjfh5xfuD5xqZvs3OW0pcA5+UYHm/ARfoklEREREJPaCHLmeDFQ55xZnb7q5H9juLnTn3DvOuVeA+qaNzWwi/i74vwUYo4iIiIhIwQSZXO/O9gsiLGP7xRJalK0Dey3wrQDiEhEREREJREtL3EZtKjDTObfML+zVPDO7ELgQoFevXhP33XffkMITERERka5q3rx5q5xzA5s7FmRy/R4wrNHjoWy/EllrDgb+y8ymAr2BHma2wTm33U2Rzrlb8YsTUF5e7iorK/OPWkRERESkFWa2pKVjQSbXc4F9zGwvfFJ9Cn5RgzY5505v+N7MzgHKmybWIiIiIiJxE9ica+fcVvzyuE8AbwAPOufmm9mVZnY8gJlNMrNlwBeBW8xsflDxiIiIiIgErdOs0KhpISIiIiISBjOb55wrb+6YVmgUERERESkQJdciIiIiIgWi5FpEREREpECUXIuIiIiIFIiSaxEREUmORYuonTqNTWWDqS8qZlPZYGqnToNFi7pG/4WIoau3D5iSaxEREUmGWbOoHl/BDbf1ZOz62fRwtYxdP5sbbutJ9fgKmDWrc/dfiBi6evswOOc6xTZx4kQnIiIinVRVldtQOsBVMNuB22GrYLbbUDrAuaqqztl/IWLo6u0LCKh0LeSkGrkWERGR2Ku99kZu3vIV0hzc7PE0B/PbLRdQe91NnbL/QsTQ1duHRYvIiIiISOxtKhvM2PWzWczIFs8ZwSJeLTuE0nUfdLr+2xVDaYrS6lV+x9y5sMp/v+mkMxi7aU7b7XtVULphpd/x3HOwfn3H2//977BlS/va9z6Y0vUroL4ennhi28+fa/sA/w8atLaIjJJrERERib36omJ6uFrq6NbiOd3YQm1RT4rqtna6/tsVAztR5Or8js98Bv72N98eoweb29d+/Hh49dWOtx8wAFav7lj7zZuhpGTbz59r+wD/DxpohUYRERFJtNreA9iTJa2eswdLqek9oFP2364YevXftuP66yGdhnSa2tL+Of4Mu2zbcc89+bV/4omOt+/W7T9t29c+uP+DXCi5FhERkdgrOuM0vtr99lbPubj7bRSfeVqn7L9dMZxz5rYd++4LqRSkUhSdfWZu7c8+Y9uO8ePzaz9xYsfbFxX9p2272gf4f5CTlu50TNqmaiEiIiKdWNSVIqLuvxAxdPX2BUQr1UIiT4oLtSm5FhER6eRmznQbSge4X3T7jhtBlevGZjeCKveLbt/1SdXMmSH1/91o+m+Ioecu7hd8Y/sYuk/PLYaGn6H79K7ZvkCUXIuIiEjnUFXlag492lXT09VZsaump6s5Zkooo5X/6f/Iz27f/5GfC69/55y76SZXQ3dX3WuAqysqdtVlg13NJdNyj6GqytVcMs1Vlw3umu0LoLXkWtVCREREJFnWrIHnn4cjjoCyMvjGN+Caa8Lr/+ab4Wc/g7fegv794bzz4IYbwuv/u9+FX/8aPv54u2oaEp7WqoW0XMtEREREJI7694fPfc5/P3kyrF0bbv9Tp8LFF4MZPPggjB4dbv9HHw0DByqxjimNXIuIiEhyfPwx/Pa38KUvwV57+YVGilT8TMKlOtciIiLSOcydC9On+ykZEH5inU7DmDHw4ov+8bp1cPvtUFUVTv8rVsALL8DWYBdJkY5Tci0iIiLJkU77r5Mn+69r18Jhh8Hdd4fT//PPw+uvw667+sfV1XDBBfDYY+H0/+c/+9rRS5eG05+0m5JrERERSY5Mxi+M0q+ff9y3L7z2Gjz7bHj977EH7LabfzxkCAwd6veH1f+AAX5KjMSSkmsRERFJBud8cplKbdtn5h+Hmdw27h+gomLbiHpY/ZuF05+0m5JrERERSYYPPoCPPtoxuU2l/FSNjz8Otv8PP4R33mm+/7ffhpUrg+1/3Tp44w2fzEtsKbkWERGRZNhtN59An3XW9vtTKT+qPXdusP1v2gRnnunneDftH+Cll4Ltf+5c/3M2Te4lVlTnWkRERJJjp5123Dd5Mhx7LPToEWzfw4c3f+NkKuWreAwcGGz/FRXw5JMauY45JdciIiKSDBdfDOPH+6+N7bwzzJwZfP8ffgiDBu0437lHj+ATa4DeveHII4PvR/KiaSEiIiISf5s3w513wqJFLZ+zbp2fNhGEujrYZx/49rebP/7EE3DKKX5RmyA455d4f+WVYK4vBaPkWkREROLv5ZehtrblKRH/+79+BHvJkmD6X7AA1q/3I+fNef99eOABf14Q3nkHvvc9mD07mOtLwSi5FhERkfhrKHXX0s18++23rVRfFP037A+6f823jj0l1yIiIhJ/mYyvFjJ0aPPHx43zNzsGVW86k/Ej4/vs0/zx0aP9gjZBJdeZDJSWwtixwVxfCkbJtYiIiMRfnz7w2c+2vHhK9+5QXh5scjt5MhS1kDoVFfnjQSb3EydCN9WiiDv9D4mIiEj8/fa3bZ+TSsGNN/qbHwtdlu9HP/LVOlpz6KEwa5a/+bG4uHB919XBm2/CeecV7poSGHNB3VUbsvLycldZWRl1GCIiIlJozuW23PecOTBvHpx9tp9C0Zls3QobN0JZWdSRCGBm85xz5c0d07QQERERibcrroAxY2DLltbPmzzZ18AudGI9b55P3KPUrZsS64RQci0iIiLx9vzzfppF9+5tn/vOO4UvV3fVVXD66bmde+qpcMYZhe3/xz+Gyy8v7DUlMEquRUREJL7q6/2oca4l6KZN89NCCqWhvF9LJfiaMoN//KNw/YOv4f3qq4W9pgQm0OTazI4xs4VmVmVm05s5/ikze8HMtprZyY32TzCz581svpm9YmZfDjJOERERiam33oK1a3NPblMpqKqC1asL0/+yZX6BmFyT+4oKWL7ctyuEVav8qpS5/vwSucCSazMrBm4CjgX2B041s/2bnLYUOAe4t8n+jcBZzrkxwDHAr82sX1CxioiISEy1tXhLUw1JcKHmSLe3/0IvJtNwHS0ekxhBjlxPBqqcc4udc5uB+4EpjU9wzr3jnHsFqG+y/03n3FvZ75cDK4CBAcYqIiIicTRiBFx4oV+BMRfl5b7mdKHqTWcyUFICBxyQ2/kTJvgygIVMrouLfY1rSYQgk+vdgXcbPV6W3dcuZjYZ6AEsaubYhWZWaWaVK1eu7HCgIiIiElP/9V9wyy25143u3duvYlio5PbHP4bnnsu9bnZJCXztazB+fGH6LymBo4+GXr0Kcz0JXKwXkTGz3YA/Amc75+qbHnfO3QrcCr7OdcjhiYiISJBqa2HpUth779zqXDe44w4YNKgwMfTp40fD2+OXvyxM3wA/+EHhriWhCHLk+j1gWKPHQ7P7cmJmZcBjwA+ccwGtJSoiIiKxNXcujBoFf/1r+9pNnAjDhrV9XlvefBN++EN/g2J7rVkD69bl138nWeivqwkyuZ4L7GNme5lZD+AUYEYuDbPn/xm42zn3cIAxioiISFw1TO2YPLl97Wpq4Prr4V//yq//p57yNa5ra9vXbulS2GUXuO++/Pq/6y7Yc094L+exSYmBwJJr59xW4FLgCeAN4EHn3Hwzu9LMjgcws0lmtgz4InCLmc3PNv8S8CngHDN7KbtNCCpWERERiaF02ieXgwe3r1337vDf/w33359//4MGwfDh7Ws3bBgMGJD/vO9Mxpch3G23/K4joQp0zrVzbiYws8m+HzX6fi5+ukjTdvcA9wQZm4iIiMRcJgOf+ET72xUXw6RJ+VcMaVg8pj3zvcGfn0oVpv/Jk331E0kM/W+JiIhI/CxfDu++2/HFUyoq4JVXYNOmjrX/6CNYuDC//hcs8CPPHbFxo49fi8ckjpJrERERiZ++feGRR2DKlLbPbU4qBVu3wgsvdKx9VZWvFNLRxVsakuK5czvWft48qKtTcp1AsS7FJyIiIl1Ur15w4okdb59K+ekUb74JhxzS/vaTJvnR63z6v+UWGDOmY+3794eLL9bKjAlkrpOUeSkvL3eVlZVRhyEiIiKF8OCDMHp07isjNqe6WouvSCDMbJ5zrtkC6JoWIiIiIvFSVwfnnw+33ZbfdTqaWDsHn/40/PGP+fX/3ntw770dq1f9+uv+30ESR8m1iIiIxMvrr8OGDfnPN549G446Ct5/v33tFi2CZ57p+M2QDR59FE4/Hd5+u33t3nvPTye58cb8+pdIKLkWERGReGkoYVeIm/n+/vf215tuOD/f/hvmS3e0//YuniOxoORaRERE4iWT8Tf07b13ftc58EDo1q39yW06DaWlHb8ZscHYsf467a13ncn4hXAOPDC//iUSSq5FREQkXubO7djiLU317OlviOxIcjtpkk/M89GtG0yc2LGR6wkTYKed8utfIqHkWkREROJl9mxfxq4QKiqgsjL3mwOdg/32g89/vnD9v/gi1Nbmdv7WrdveXEgiqc61iIiIxEuvXoUroXfYYX6lxTVrYODAts83g7vuKkzfAJdd5reSktzbPPwwDBlSuBgkVKpzLSIiIvFx//1+2e+rrvKLwIStpkbTMaRNqnMtIiIiyXD//X7Z80In1lu35nbeuecWfkrGHXfAb36T27mPPQb//ndh+5dQKbkWERGReHDO33xY6CW/L7nE36CYi0wG9tijsP0/9hhcf31u537nO3D11YXtX0Kl5FpERETiYelS+PDDwo8cDxwIL78M69e3ft6KFX7Bl0L3n0r5hWlWrmz9vHXr4I03dDNjwim5FhERkXgo1OItTVVU+FHxtu7NCrJ/gDlzWj9v7lwfp5LrRFNyLSIiIvGwZg3sthuMH1/Y6zasdNhWvetMBoqLfW3qQpo40V+3rXrXWpmxU1ApPhEREYmHr34VLroo/8VjmurfH/bZp+3k9tOfht69/aqKhdSrl19tcfXq1s+rrITRo2HnnQvbv4RKybWIiIjER6ET6waXXdZ2ib0jjvBbENJpP3rdmgcegOXLg+lfQqNpISIiIhK9F16AcePanhfdUVOnwnnntXx81Sq/kmKuJfvaq63EGqBHDxg+PJj+JTRKrkVERCR6zz8Pr70GgwYFc33n4N13Ydmy5o8/+igcdBBUVQXT/+rVftrJ/fc3f/ypp2DaNF8xRBJNybWIiIhEL5OBXXeFYcOCuf7WrTBqFFx3XfPH02no29efE4Sdd/Yj488+2/zxv/4Vfve7ws/3ltApuRYREZHoNSweE9Sc6+7d/ch0Szc1ZjK+BF5QS64XFfmFbFrrv7zcxymJpuRaREREorVmDbz1VvD1nSsqYN482LJl+/3V1fDqq8H3n0rBK6/Axo3b79+82c85V33rTkHJtYiIiESruhpOP93PSQ5SKgU1NT7BbWzePKivDye5r6vz/TX28stQW6vkupNQKT4RERGJ1rBhcM89wffTkLxmMtsvFHPQQfDEE+GMXH/mMztOPfnwQxg8eNtKjpJo5pyLOoaCKC8vd5VBle8RERGR4KxcCQMGBDffuoFz8NBD8MlPwpAhwfbVXg35WND/BlIQZjbPOVfe3DFNCxEREZHoOOdXJZw2Lfi+zOBLX9oxsb722h2nigRp/fod95kpse4klFyLiIhIdN56Cz76yC8gE4bly+G3v4W1a/3jZcvgW99quUReod19ty/59957/vHq1X5p9kcfDad/CZySaxEREYlOQ2m6sG7mW7jQr9aYTkfT/6hRfrS+od85c/zCNX36hNO/BE7JtYiIiEQnk4HevWG//cLpr7zcT79oSG7Tab/s+AEHhNP/gQf6/hr3X1Tk45JOQcm1iIiIRCed9ourFBeH01+fPjBmzLbkNpPx1UJKSsLpv6QEJkzYvv+xY/0bDOkUlFyLiIhIdKZPh29+M9w+Uymf1NbVwYIF4deXTqVg7ly/mM2cOapv3cmozrWIiIhE5+STw++zogJuvx3efdff4FhdHW7/p5ziK6SsWwcnnQSf/Wy4/UugNHIdlUWLqJ06jU1lg6kvKmZT2WBqp06DRYuS0X/U8YuIJFXUr79xa997ALVTLwv371/6RTb1GUj9iJFs6r87td+7PNy/X4MHUzu/ik0jxlB/+x1sOusi/Q3tRAJNrs3sGDNbaGZVZja9meOfMrMXzGyrmZ3c5NjZZvZWdjs7yDhDN2sW1eMruOG2noxdP5serpax62dzw209qR5fAbNmxbv/qOMXEUmqqF9/49i+OsMNt5WG+/fv7r6MXf98NH+/GmL4/U76G9pZOecC2YBiYBEwAugBvAzs3+Sc4cB44G7g5Eb7+wOLs193zn6/c2v9TZw40SVCVZXbUDrAVTDb+Vo8228VzHYbSgc4V1UVz/6jjl9EJKmifv1Nevt8Rd1/XGKQggAqXQs5aZAj15OBKufcYufcZuB+YEqTxP4d59wrQH2Ttp8BnnTOrXHOfQQ8CRwTYKyhqb32Rm7e8hXSHNzs8TQH89stF1B73U3R9b/5fGp/dJW/yWPBAlixwh+sq6P2v3/CzZsviCx+EZGkyvn1t+H18+23t70OL1iQ2+tv4/Zvvtnx9vX127XNuf2WbPstW5pvvyW6vx9R//2NSwwSgpay7nw34GTgtkaPzwRubOHcP7D9yPW3gP9u9PiHwLda6y8pI9cb+wxyI6hq9h1rwzaCKlddNjja/um5bcf06b7x6tVuIztFGr+ISFK1+/V/7NjtDrb79bd//463r6nZ4WC72r/9dn7t4/Dv30ljkMKglZHrRFcLMbMLgQsB9thjj4ijyU3JhlUsYc9Wz1nKHuy0YVW0/dtmuPc+v6OhsH+vXpSwOdL4RUSSqt2v/z//ua8m0dD+1NPb1/73v4fNmzvWvls3uO++7eNvT/uBA/NrH4Co//7GJQYJXpDJ9XvAsEaPh2b35dr2sCZtn2l6knPuVuBWgPLycteRIMNW23sAe65fwmJGtnjOHiylpvcASqPsv88ASk85ZfsDJSXU9ok2fhGRpGr36/+xx27f/sL/1772J57Y8fbFxb5cXEfb9+qVX/sWz+i4qP/+xiUGCV6Qc67nAvuY2V5m1gM4BZiRY9sngKPNbGcz2xk4Orsv8YrOOI2vdr+91XMu7n4bxWeeFsv+o45fRCSpon79TXr7fEXdf1xikBC0NF+kEBvwWeBNfNWQH2T3XQkcn/1+ErAMqAZWA/MbtT0PqMpu57bVV1LmXEd+p3DS7/YWEUmqqF9/k94+X1H3H5cYpCBoZc51oMl1mFtikmvnnJs5020oHeB+0X26G0GV68ZmN4Iq94vu0/0v1cyZ4fRf/J2O9R91/CIiSdXw+ln07Whef5PePl9R9x+XGCRvSq7j6K23XM0l01x12WBXR5GrpqermXpZeO9Wq6q29V9U7KrLBruaS6bl3n/j9g3xX/Q1vdsWEWlLVZWr2Wu0qy7qlf/rb1dsn6+o+49LDJKX1pJr88eTr7y83FVWVkYdRu7uuQd++EP45z/hqafg3HNh/nzYf/9w+n/zTfjwQzj4YH9XeD7+8hc44QSYPdtfT0REWuYc7L47HH64/1sgIoljZvOcc+XNHQt0+XNpRSYDK1fCkCFQUeH3pdPh9X/77XDEEbB1a/7XSqVgl13ggw/yv5aISGe3bBm8/75/7RSRTifRda4TLZ2G8nIoLoZRo+D002Ho0HD7P/BA2Gmn/K+1227+jYJZ/tcSEensuneHK6+Eo46KOhIRCYCS6yjU1MDLL8M3vuEfFxWF+9Hg1q1QWQnnn1+4ayqxFhHJza67+mmBItIpaVpIFF58EbZs2fEjwQ8+2G41rcDMnw8bNxb2I8lZs2Dfff08bhERaVkmAx99FHUUIhIQJddRKCuDiy7a/ua/J5/00yvCmHfd0EfDXO9C6NMHFi70fzRERKR5W7bAYYf5aSEi0ikpuY7CmDHwu9/5jwYbTJjgv4aRXJ91lq/sMWJE4a550EF+/riSaxGRlr36qp8aqJsZRTotJddRWLAA6uq23zdwIIwcGU5y2rOnHzUv5Dzp0lIYP17JtYhIaxoGUJRci3RaSq7DtmIF7LcfXH/9jsdSqeCT03Xr4LvfhTfeKPy1UymYM2fHNw4iIuJlMjBoEAwfHnUkIhIQJddha0ieJ0/e8VgqBe+952ugBmXuXPj5z+Hddwt/7WOPhRNPhA0bCn9tEZHOIJPxr/WqsCTSaakUX9jSaT83+aCDdjz2uc9B377+5sCgtJbc5+v44/0mIiLNu/9+qK+POgoRCZCS67BlMn5ucmnpjsdGjvRbkNJpXzKvX79gru8crFnjV2wUEZHtNdy8LiKdlqaFhKmuzs9Jbq0E3ltvwaOPBtO/c9s+kgzKySfD4YcHd30RkaR6/HG4776ooxCRgGnkOkzOwb33wu67t3zOTTfB73/vbzzsVuD/npUr/eqMQSbXY8bAX/7i51337h1cPyIiSXPDDbBkCZx6atSRiEiANHIdpm7d4POfhwMPbPmcVMqvnvjaa4Xvf9AgWL26sMueN1VR4ecTVlYG14eISNI41/YnlyLSKSi5DtPjj8O//936OQ0vvEEtJmMGPXoEc23YdqOk6l2LiGyzaJEf3FB9a5FOT8l1mKZPb3vJ2+HD/YIyQSSnZ5wBP/tZ4a/b2IAB4S2GIyKSFFo8RqTL0JzrsGzY4Je9nTKl9fPMgllMprYWHn4YdtutsNdtzk9+AjvvHHw/IiJJMX8+9Orl70sRkU5NyXVY5s3zc5FzGbX49a99vetCevlln2CHMWqim3VERLb305/CN79Z+BvVRSR29FselvYs3hJEreuG/sNIrhtKDu6yC4waFXx/IiJJMGBA1BGISAg05zosc+f6pDmXF1fn/Nzohx4qXP+ZDAwZAkOHFu6aLamrg09/Gm69Nfi+RETi7qWX4JRT/E2NItLpKbkOyz33wN/+ltu5ZnD33XDXXYXrf9gw+OIX/bWD1qOHLzeomxpFRODZZ+GBB6Bnz6gjEZEQaFpIWEpKYMSI3M9PpfxKjc4VJiH+6U/zv0Z7pFJ+5HrLFujePdy+RUTiJJ32nxoOGRJ1JCISAo1ch+HZZ/2NLGvX5t4mlYJVq+Dtt/Pvv7bWJ+lhqqiATZt8hRQRka4sk9HiMSJdiJLrMJ6B52oAACAASURBVPz1r35Z89LS3NsUcjGZq6/2IyabN+d/rVw13DipqSEi0pWtWOEHSVTfWqTL0LSQMGQyfg5ye1ZGHDMG+veH99/Pv/902i99HuTKjE0NHw7/+hccdFB4fYqIxM0HH8ABB8DBB0cdiYiERMl10LZsgcpKuOii9rXr1s2PeBQX59d/fb0vi/elL+V3nfYyg09+Mtw+RUTiZvx4Xy1ERLoMTQsJ2muv+bnHHflIMN/EGuCtt/xc7yg+kly4EL7//fbNNRcR6UzCvt9FRCKn5DpoH3zgp2R0JLl99VU45BA/8t1RYS4e09S77/oqJXPmhN+3iEjU6uthjz38qrsi0mUouQ7ascf6BHv48Pa37d8fZs/2W0eNHQvf/S7su2/Hr9FRkyb56SG6qVFEuqIFC2DZMth556gjEZEQKbkOg1nHalXvvruvjZpPxZCDDoJrrinMFJP26tsX9tuvMBVPRESSJspPDkUkMkqug7R2LYweDf/3fx2/RirV8ZHfmhrftra24/3nqyF+zTsUka4mnfaDDKNGRR2JiIRIyXWQ5syBN9+E3r07fo1UChYvhpUr29+2stLXy8512fUgpFK+YsoHH0QXg4hIFDIZ/xpYpD+1Il2JfuODlMn46SCTJnX8GoceCscfDx9/3LH+IdqPJM89Fz76CHbbLboYRETC5hyccAKccUbUkYhIyFTnOkiZjJ9zXFbW8WtMntzxaSXptL+RctCgjvefrzAXrhERiQsz+PGPo45CRCIQ6Mi1mR1jZgvNrMrMpjdzvMTMHsgez5jZ8Oz+7mZ2l5m9amZvmNn3gowzEM755LZQo8bV1e1vk8lsW0Y9Sr/6FZx5ZtRRiIiEZ9ky2LAh6ihEJAKBJddmVgzcBBwL7A+camb7NzntfOAj59zewHXAz7L7vwiUOOfGAROBixoS78TYuNFP5/jc5/K/1g9/6CuH1Nfn3mb5cl9nOg53qX/4ITz4YLQ3VoqIhOn//T9frUlEupwgR64nA1XOucXOuc3A/cCUJudMAe7Kfv8wcISZGeCAXmbWDegJbAY6MOk4Qr16wR13wEkn5X+tkSNh3Tq/4mGudtkFnnmmMP3nK5WCzZvhxRejjkREJBzpdH7324hIYgWZXO8OvNvo8bLsvmbPcc5tBdYBu+AT7WrgfWAp8Evn3JqmHZjZhWZWaWaVKztSTSNIq1cXrvxcw+hze0rylZT4myGHDStMDPlomJqixWREpCtYtsx/ehiHaXkiErq4VguZDNQBQ4C9gG+a2YimJznnbnXOlTvnygcOHBh2jK377GfhuOMKc63Ro32t1PYsxnLbbfDcc4XpP19DhvjFcJRci0hXEIdKTSISmSCT6/eAxsOmQ7P7mj0nOwWkL7AaOA143Dm3xTm3Avg3UB5grIVVU+OnQIwZU5jrFRX5qiG5Jqd1dXDZZfDAA4XpvxBOOsnPGxcR6ezSaV8p6YADoo5ERCIQZCm+ucA+ZrYXPok+BZ80NzYDOBt4HjgZeNo558xsKXA48Ecz6wVUAL8OMNbCeuklv3BKIT8SnDoV1uwwM6Z58+f76iJx+kjy18n57xMRycu558LEiX56noh0Oa0m19mKHz9zzn2rvRd2zm01s0uBJ4Bi4A7n3HwzuxKodM7NAG7HJ9BVwBp8Ag6+ysidZjYfMOBO59wr7Y0hMkF8JPiFL0TbfyE45990qPa1iHRm++/vNxHpklpNrp1zdWb2yY5e3Dk3E5jZZN+PGn1fgy+717Tdhub2J0Ym4+cYDxlS2OsuXAhbt7Y93SST8dVCRo4sbP/52LoV9toLzj4brroq6mhERIKxdKmfFnLssdCnT9TRiEgEcpkW8qKZzQAewlfwAMA596fAokq6887zNzQW2mc/CxMmwCOPtH7ea6/5UWuzwsfQUd26wcCBuqlRRDq3mTPh4oth8WIl1yJdVC7J9U74mwwPb7TPAUquW3LkkcFcN5WCf/6z7fNmz4a1a4OJIR+pFNx7r18MpyiuhWpERPKQyfiBhOHDo45ERCLSZnLtnDs3jEA6jUWL4IMPfCLZrcD3i1ZUwH33wXvvtV55o6gI+vcvbN+FkErB734HCxZoPqKIdE7pdPw+ORSRULU5fGhmQ83sz2a2Irs9YmZDwwguke680y/esnlz4a+dy2Iyd94JF13UvqXSw6LFZESkM1u71g8exKlSk4iELpfP5u/El8wbkt0eze6T5mQyMH48lJYW/toTJvhKG60tJvOXv8Czz8Zz2sWoUfDtb8PYsVFHIiJSePPm+a9xq9QkIqHKZd7CQOdc42T6D2Z2WVABJVp9PcyZA6c1LeddICUl8PjjLU+pcM4n3sccE0z/+Soqgp//POooRESCcfjhfmrgbrtFHYmIRCiX4c3VZnaGmRVntzPwNzhKUwsWwMcfB/uR4Kc/DYMHN39syRJYsSLeH0lu2QKVlbBpU9SRiIgUlhmMGAE9e0YdiYhEKJfk+jzgS8AHwPv4lRR1k2Nzwli8ZcUK+OUv/ehIFP3n629/g0mTYO7cqCMRESkc5+CrX4Wnn446EhGJWKvJdXaFxqudc8c75wY65wY5577gnFsaUnzJ8uUvw7/+5ecWB2X9ej9v+e9/3/HY5s2w334wblxw/edr8mT/tbV54yIiSbN4MdxyC1RVRR2JiESs1eTaOVcH7GlmWq86F6Wl8MlPBnsz4YgRMGBA88npmWfC669D9+7B9Z+vgQP9ypGqGCIinUkSPjkUkVDkckPjYuDf2VUaG6/Q+KvAokqi6mr4yU/grLOCreFs5l+8myanziWnrmoq5SuaiIh0FpmMH2AZMybqSEQkYrkMsS4C/po9t0+jTRqrrISf/Qzefjv4vlIpf/PkunXb97/77n51xrhLpfxCOMuWRR2JiEhhpNNQXl74xcNEJHFafRXIzrke5Zw7PaR4kqthJLlhTnGQUikoLoY33th+YZbly2HYsOD7z9cJJ/jRnQEDoo5ERCR/9fW+EtLhh0cdiYjEQKvJtXOuzsz2NLMezrkAlhzsRDIZPx964MDg+zrsMF/yr3G5p0zG11YdmoDFM4cNS8abABGRXBQVwQsv+Ol5ItLlac51oaTTPukNQ49m7i9Np/2IdlLmXT//vL/58vzzo45ERKQwkvL6KyKB0pzrQlizBjZuDPcu8YcegilT/EjJ6tW+/FOcF49p6oEH4Otfh61bo45ERCQ/U6fCuVr+QUS8NkeunXNXNN1nZrpjo7H+/X2Cu2VLeH2uXg0zZvgbKHv29InqkUeG13++Uim4/np47TWYMCHqaEREOu7xx+Ggg6KOQkRiosWRazN7rtH3f2xyeE5gESVVURGUlITXX8MoecNc6+uvh4kTw+s/Xw3xazEZEUmylSv9IEeSPjkUkUC1Ni2kV6PvxzY5polljZ13HlxzTbh9jhvnR6zTab8Uepij5oWw117+5k8tJiMiSabFY0SkidaSa9fC98097noWLaJ26jQ2lQ2m/s4/sOlHP6V26jSf6IZhyRJq+w1i0423U7/3PmzqNSDc/vO1eDG1pf3YdPdD1BcVs6lscLLib/z/n8T4RfKR7/O/M7U/bgqb2InaPz6g338RAVpPrvuZ2QlmdlL2+xOz20lA35Dii6dZs6geX8ENt/Vk7PrZ9GAzY7e8wA239aR6fAXMmhVO/x9+mbH1L4fff74a4n/vJB+/q2Xs+tnJi7/h/z9p8YvkI9/nf2drTy1jeY0b/lCm338R8ZxzzW7Ana1tLbWLaps4caILRVWV21A6wFUw2/lSHdtvFcx2G0oHOFdV1Tn7z5fiF0mufJ//Xb29iHQaQKVrISdtceTaOXdua1vwaX881V57Izdv+QppDm72eJqD+e2WC6i97qZO2X++FL9IcuX2/D+f2l9cDzU127Zsyc3aX/6Gm7dc0P72dXXZ9jd0rH19fWHa/yKX9vr9F+nyWsq6k7aFNXK9sc8gN4KqZkctGrYRVLnqssGdsv98KX6R5Mr5+U/P7Xdee61v32tAx9rfd59vX7pLx9o/+6xvv9POHWv/5pu+fUk//f6LiHOu9ZFr88eTr7y83FVWVgbeT31RMT1cLXWtlAjvxhZqi3pSVFf4BVKi7j9fil8kuXJ+/ttOFF39P9t2Hn44TJ7c8fbHHw/779/x9qedBnvs0fH2F14I/fvr919E/sPM5jnnyps7psVg2qm29wD2XL+ExYxs8Zw9WEpN7wGUdsL+86X4RZIr5+d/n4GUTp/eddvr91+kS8tl+XPM7BNmdpqZndWwBR1YXBWdcRpf7X57q+dc3P02is88rVP2ny/FL5Jc+T7/u3p7EekiWpov0rABfwRmAzcDv8luN7TVLuxN1UJC6j9fil8kuaKutpH09iLSadDKnOtckus3wM/NjvMWWnLtnHMzZ7oNpQPcL7pPdyOoct3Y7EZQ5X7Rfbp/YZ05s3P3n6/OEr99K5nxi+Qj39/frt5eRDqFfJPrh4Dd2jov6i3U5No556qqXM0l01x12WBXV1TsqssGu5pLpoU3YhF1//lqLv6plyUn/jfecDXFPV11j76uzopddfcyV3P+1OTEL5KPfF9/unp7EUm81pLrNquFmNk/gAnAHKC20XSS4ws7QSU/YVULkQBkMvDFL8JDD0EqFXU0uXnhBZg4Ee6/H3bZBY46Cv72N/9VpLOrq4MjjoCvfQ1OOinqaEREQpdvtZDLCxuOSBO77w7vvuuT7KQk1+m0/5pKwc47g5nfp+RauoL58+HZZ+ErX4k6EhGR2GkzuXbOPRtGINKFDR3qE+x0Gr7+9aijyU0mA4MHw557+sR6v/38PpGuoPGbSxER2U6bybWZVeArhOwH9ACKgWrnXFnAsUlXkkolKzn91Kdg9GifWIOP/9FHfdGAhn0inVUm46dDjWy53rOISFeVS53rG4FTgbeAnsAFwE1BBiVdUEUFLF4MK1dGHUluzj8fvv/9bY8PPhgGDYI1a6KLSSQs6bR/Q6k3kiIiO8hpERnnXBVQ7Jyrc87dCRyTSzszO8bMFppZlZntsNyVmZWY2QPZ4xkzG97o2Hgze97M5pvZq2a2U24/kiTSEUfA1KmweXPUkbRt1Sq/NXbBBX4e6i67RBOTSFjq6vw0qGNy+jMgItLl5HJD40Yz6wG8ZGY/B94nh6TczIrxI9xHAcuAuWY2wzn3eqPTzgc+cs7tbWanAD8Dvmxm3YB7gDOdcy+b2S7Alnb9ZJIsBx3ktyS4+Wa44gpYuxb69PH7NIInXUVxMTz8cNRRiIjEVi4j12dmz7sUqAaGAbnUXpoMVDnnFjvnNgP3A1OanDMFuCv7/cPAEWZmwNHAK865lwGcc6udc3U59ClJVlcHb78ddRRtS6f9yF1DYt1g+nQ49thoYhIJSxI+XRIRiVCbybVzbglg+IVkrnDOfSM7TaQtuwPvNnq8LLuv2XOcc1uBdcAuwCjAmdkTZvaCmX0nh/4k6b7+dZgwAerro46kZc7BnDnNV0lwDp5+Gmprdzwm0lmcfDIcfXTUUYiIxFYu0zuOA14CHs8+nmBmMwKOqxvwSeD07NcTzOyIZmK70MwqzaxyZVJuhJOWlZfDxx/DwoVRR9KyRYtg9Wp/A2ZTqZQf1XvppfDjEgmDc75SyJAhUUciIhJbuUwLuRw/xWMtgHPuJWCvHNq9h59C0mBodl+z52TnWfcFVuNHuf/pnFvlnNsIzAR2mJDrnLvVOVfunCsfOHBgDiFJrDWMBse5JF9r9X0b9jWcI9LZLFkCK1aovrWISCtySa63OOfWNdnX+prp3lxgHzPbK3tD5ClA0xHvGcDZ2e9PBp7Ortf+BDDOzEqzSfehwOtI57bvvlBWFu/k9LDD4LbbYMyYHY/tvrtfECfObw5E8qHFY0RE2pRLtZD5ZnYaUGxm+wBfB2a31cg5t9XMLsUnysXAHc65+WZ2JVDpnJsB3A780cyqgDX4BBzn3Edm9it8gu6Amc65xzrw80mSFBXB5MnxTk6HDvU1rltywQXQu3d48YiEKZOBnj1h3LioIxERiS3zA8WtnGBWCvwAX8HD8MnyT5xzNcGHl7vy8nJXWVkZdRiSr6efhq1b43nDVE0N3H8/fOYzsNtuUUcjEr7HHvP13L+je8xFpGszs3nOufJmj7WVXCeFkmsJ3PPPwyc+AX/6E5xwQsvnbdrkK4b06xdebCIiIhKa1pLrFqeFtFURxDl3fL6BiezAOXjqKT+1ormKHFFqmK7S2nzT2lq/SuM3vgFXXRVOXCJhWLECVq70Nd6LclrcV0SkS2ptzvXB+BrU9wEZ/JQQkWCZwYUX+tUa47YKXDrt51y3VoaspARGj473vHGRjnjwQfja12DpUhg2rO3zRUS6qNaGH3YFvg+MBa7HL2O+yjn3rHPu2TCCky6qoiKeFUMymdxG0ysq/EIzcV4MR6S90mn/xnLo0KgjERGJtRaTa+dcnXPucefc2UAFUAU8k60AIhKcVAree89vcbFyJbzzTm4lyFKp+C+GI9JemYx/bps+xBQRaU2rE+fMrMTMTgTuAS4BbgD+HEZg0oXFcTGZgQPh/ffh3HPbPleLyUhns3o1VFWpvrWISA5au6HxbvyUkJnAFc6510KLSrq2CROge3efXJ94YtTRbLPrrrmdN3o0XHcdHHJIsPGIhCWXm3lFRARopRSfmdUD1dmHjU8ywDnnygKOrV1Uiq+TeeMN2Htvn2THwRVXwMiRcMYZUUciEr61a+G55+DTn4ZevaKORkQkcq2V4mttznWRc65PditrtPWJW2ItndB++8Unsa6rg2uvhdltLky6zUcfwV/+Ahs3BheXSFj69YPPf16JtYhIDlSsVOJpyRK47DJ4882oI4EFC2D9+vZ9JP7vf/uFZvRpiiRdfb1/c7lgQdSRiIgkgpJriaetW+H66+GZZ6KOpGPzTeN4U6ZIR7z1FnzrW+375EZEpAtTci3xNGIEDBgQj4ob6TT07QujRuXeZuBA/zPEIX6RfOhmRhGRdlFyLfFk5v+Yx2Hkd+NG+OQn27/kc1ziF8lHOg19+sC++0YdiYhIIii5lvhKpXzVkHXroo3jnnvg0Ufb366iwi+Es2xZ4WMSCUsmA5MnQ3Fx1JGIiCSCkmuJr1QKBg/2KyNGrSOr0p1yil+lcffdCx+PSBg2b/bPYU0JERHJWYuLyIhE7sgjYfnyaJdb/s1v4JFH4IknoKSkfW0HDfKbSFL16OFXZ6ypiToSEZHE0Mi1xFdRUbSJNcA//uGndbQ3sW4wYwZcc01hYxIJU0mJv6FXRERyouRa4u2WW/zc5RZWEg2Uc/5mroqKjl/jqafgJz/xpQVFkubyy+HnP486ChGRRFFyLfFWX+9vqIpi3vWyZfD++/nNN02lfLWR114rXFwiYbnjDnjhhaijEBFJFCXXEm8No8ZRlLQrRH1fLSYjSfX++/Duu/l9ciMi0gUpuZZ4GzcOevaMJjnt2xeOOw4OOKDj14jTYjgi7aHFY0REOkTVQiTeunWDiROjSU6POspv+WhYDOfDDwsTk0hY0mno3h0OPDDqSEREEkXJtcTfSSfBokXh9llXB+vXQ79++V/rz3/2SYpIkhQXwxFHwE47RR2JiEiimIuiCkMAysvLXWVlZdRhSGfx4otw0EG+lN5xx0UdjYiIiMSImc1zzpU3d0xzriUZ6uvh44/D669hvunYsflfq64OvvAFvyCNSBJ0kkEXEZEoKLmWZBg3DqZODa+/TAYGDoThw/O/VnGxX0L6iSfyv5ZIGG6/HUaPhpUro45ERCRxlFxLMoweHW7FkHTa34hYqBUiKyp8/BoRlCRIp/2y5wMGRB2JiEjiKLmWZKiogKoqWLUq+L7WroUFCwpb3zeV8rG//XbhrikSlEK/uRQR6UKUXEsyNNTanTMn+L6KiuDmm2HKlMJdU4vJSFJ8/DG8/rrqW4uIdJCSa0mGiRN90htGclpWBhdfXJibGRuMGweHHaayZhJ/lZV++pKSaxGRDlGda0mG3r3h2mvD+YP/j3/4lRX33LNw1+zWzV9XJO769YNzzoHJk6OOREQkkVTnWqQx53yVkOOPhzvuKPz1N2/21UOKiwt/bREREQmF6lxL51BTA//8J6xYEVwfixf7KgmFvJmxwdNP+ykn8+YV/toiheCcv3G4kwy6iIhEQcm1JMfixXDooTBrVnB9NMzpDmL6yahRUFurmxolvpYsgX32gVtvjToSEZHEUnItybHvvn7kN8jkNJOB0lIYM6bw1x46FIYM8WXOROKo4XervNlPOkVEJAdKriU5iopg0qRgk+t02icW3QK617dhMRmROMpkfEWb8eOjjkREJLECTa7N7BgzW2hmVWY2vZnjJWb2QPZ4xsyGNzm+h5ltMLNvBRmnJEhFBbz8MmzcGMz1H34YbrwxmGuDn26yaJGWlZZ4Sqd92cvu3aOOREQksQJLrs2sGLgJOBbYHzjVzPZvctr5wEfOub2B64CfNTn+KyDACbaSOKkU1NXBCy8Ec/1hw3xN6qAceyxcfbVWvpP42bzZ/14FcTOviEgXEmSd68lAlXNuMYCZ3Q9MAV5vdM4U4PLs9w8DN5qZOeecmX0BeBuoDjBGSZpDD4XZs+Gggwp/7ccfh4UL4dJLgyuVN25csMm7SD7uvRdGjow6ChGRRAtyWsjuwLuNHi/L7mv2HOfcVmAdsIuZ9Qa+C1zRWgdmdqGZVZpZ5Up9zN41lJXBwQdDSUnhr3333fDLXwZfg3rVKs27lvjp0QNOPBEOOCDqSEREEi2uNzReDlznnNvQ2knOuVudc+XOufKBAweGE5lEb84cuPLKwl83nQ5nBcgf/AA+8xmorw++L5FcPf64arCLiBRAkMn1e8CwRo+HZvc1e46ZdQP6AquBFPBzM3sHuAz4vpldGmCskiSzZ8OPfwzLlxfumitXwttvhzPftKIC1q2DN98Mvi+RXH396/CTn0QdhYhI4gWZXM8F9jGzvcysB3AKMKPJOTOAs7Pfnww87bz/cs4Nd84NB34NXO2cC7CEgyRKQwJcyKkVQS4e01RDH5oaInGxejW89ZZuZhQRKYDAkuvsHOpLgSeAN4AHnXPzzexKMzs+e9rt+DnWVcA3gB3K9YnsYMIEXyqskMnp22/7edwTJxbumi0JYzEckfaYM8d/DePNpYhIJ2fOuahjKIjy8nJXWVkZdRgSlsmT/UqKzzxTuGvW1PgFNMJw5JGwZk1wJQVF2uPyy/19DOvWQZ8+UUcjIhJ7ZjbPOdfscrZBluITCU5FBcyYAc4VrmZ0WIk1wDXXhNufSGvmzIGxY5VYi4gUQFyrhYi07ppr/FSOQiTWCxf6keQXX8z/WrkqL/fJjEgcPPww/OlPUUchItIpKLmWZCotLdyI9ezZ8NRT0LNnYa6XC+fgD3+Av/89vD5FWlJaCnvvHXUUIiKdgpJrSa5vfMOX5MtXJgN9+8KoUflfK1dmfo7rLbeE16dIc558Er73PdjQ6rICIiKSIyXXkkyLFlH7yKNsuupa6ouK2VQ2mNqp02DRotzbT53GprLB1N9yK5s21FF76Tdzb5+vRYuoLe7Jpkdmdix+6doaP3/zff4ffQybrrme2m//QM8/EZECUHItyTNrFtXjK7hh2YmMrX+ZHq6Wsetnc8NtPakeXwGzZuXW/raejF0/mx5sZmzdS7m3L1T8iz/PWPdK++OXrq3p8zfv538tY3mVG27vpeefiEgBqBSfJMuiRVSPr+DIjTNIc/AOhyt4nr+XHk+vV9IwcmTh20cdv3RtSX/+i4h0Eq2V4tPItSRK7bU3cvOWrzSbGACkOZjfbrmA2utuCqR9vqLuX5It6c9/EZGuQCPXkiibygYzdv1sFtPyqNoIFvGqjad0cJmvgPCvf/kDX/gCm2Y8yVj3Stvtyw6hdN0HhQ4/9/gD6l+SLefnT/EESreu9zu++EV47jnf/sOPI33+i4h0FlpERjqNkg2rWMKerZ6zlD3YydXA8WfArrtuO3DIIZT836O5td+wqhDh7iDn+APqX5It5+dP3cZtOz7xCejf37e/9TY9/0REAqZpIZIotb0HsCdLWj1nD5ZSUzbQl7m74optB779bWr75Ni+94BChLuDnOMPqH9JtnY9/xtMm+Z/F265JfLnv4hIV6DkWhKl6IzT+Gr321s95+Lut1F85mmBtM9X1P1LsiX9+S8i0hVozrUkS9KrJUTdvyRb0p//IiKdhKqFSOcxciS9Hr6bv5cezy+6f48RLKIbWxjBIn7R/Xs+MXj47pYTg3zbRx2/dG2Nnz9F307e819EpAvQyLUk06JF1F53E3V/vJedNqyipvcAis88jZJpl+SWGOTbvpDxr19JjSuh+ILzKJk+TYmNtG3RImo/eQR1K1ezk9uUvOe/iEjCtTZyreRaJGpPPw3Tp8M998CoUVFHI0mwZQv07QsXXQTXXRd1NCIiXY5K8YnE2eGHw5w5UUchSVJTA5ddBkcdFXUkIiLShJJrkbhwDsyijkKSoE8fuPrqqKMQEZFm6IZGkTj46U/9XNdOMk1LAvbWW7BxY9vniYhI6JRci8RBv37w9tvwzjtRRyJJcNxxcMopUUchIiLNUHItEgcVFf5rJhNtHBJ/H30ECxdCKhV1JCIi0gwl1yJxMG4c9Oyp5FraNneu/9rwhkxERGJFybVIHHTrBhMnQjoddSQSd+m0v/F10qSoIxERkWaoWohIXJx7LqxcGXUUEneZDOy/P5SVRR2JiIg0Q8m1SFycd17UEUgSXHklrFkTdRQiItICJdcicbJ2LWzYAEOHRh2JxNXEiVFHICIirdCca5E4GTfOL4Uu0pwXXoBHHoHNm6OOREREWqDkWiROJk1SxRBp2R/+AGedBUV66RYRiSu9QovESSoFVVWwalXUkUgcpdNQXu6ry4iISCwpuRaJk4baxXPmRBuHxE9NDbz0kupbi4jEnJJryz/h/gAAET9JREFUkTiZONF/5K+pIdLUSy/Bli1amVFEJOb02aJInPTuDffco4oQsqPKSv9VybWISKyZcy7qGAqivLzcVTb88RER6Wycg8WLYeTIqCMREenyzGyec668uWOaFiISNx99BP/7v7B8edSRSJyYKbEWEUkAJdcicbN8OZxxBjz5ZNSRSFysXAnnnOPnXYuISKwFmlyb2TFmttDMqsxsh5UxzKzEzB7IHs+Y2fDs/qPMbJ6ZvZr9eniQcYrEyr77Qp8+uqlRtkmn4a67YP36qCMREZE2BJZcm1kxcBNwLLA/cKqZ7d/ktPOBj5xzewPXAT/L7l8FHOecGwecDfwxqDhFYqe4GCZPVnIt22Qy/nmhG11FRGIvyJHryUCVc26xc24zcD8wpck5U4C7st8/DBxhZuace9E51zDhdD7Q08xKAoxVJF5SKXj5Zdi4MepIJA7SaRg3DkpLo45ERETaEGRyvTvwbqPHy7L7mj3HObcVWAfs0uSck4AXnHO1TTswswvNrNLMKleuXFmwwEUiV1EBdXWaYytQXw9z52rxGBGRhIh1nWszG4OfKnJ0c8edc7cCt4IvxRdiaCLBOuIIWLoUhg6NOhKJ2ooVsNtuSq5FRBIiyOT6PWBYo8dDs/uaO2eZmXUD+gKrAcxsKPBn4Czn3KIA4xSJn9JSTQEQb9ddYcECX+daRERiL8hpIXOBfcxsLzPrAZwCzGhyzgz8DYsAJwNPO+ecmfUDHgOmO+f+HWCMIvH1+ONwySVRRyFxYRZ1BCIikoPAkuvsHOpLgSeAN4AHnXPzzexKMzs+e9rtwC5mVgV8A2go13cpsDfwIzN7KbsNCipWkVh64w24+WYtJtPVHXUU/OhHUUchIiI5CnTOtXNuJjCzyb4fNfq+BvhiM+2uAq4KMjaR2Eul/NdMBk44IdpYJBrV1fD005pvLSKSIFqhUSSuDjwQundXveuurLLSVwtRci0ikhhKrkXiqmdPOOAAJdddWcP//eTJ0cYhIiI5U3ItEmeHHAJbtqhSRFeVycCIETBwYNSRiIhIjmJd51qky7vuOlWJ6MomTfLTg0REJDGUXIvEmRLrrm369LbPERGRWNG0EJG4+/KXYdq0qKOQsK1dC7W1UUchIiLtpORaJO7Wr4cnn4w6Cgnb1Vf7udZbt0YdiYiItIOSa5G4S6Xg9dfh44+jjkTClMnA/vtDN83eExFJEiXXInGXSvlqIXPnRh2JhGXrVl/jumEhIRERSQwl1yJx11DjWPWuu47XXoONG7V4jIhIAim5Fom7/v3hnHNgzz2jjkTC0vBGSiPXIiKJo8l8Iklw551RRyBh+tSn4Fe/gr32ijoSERFpJyXXIkmxYQMUF/tl0aVz228/v4mISOJoWohIEsyfD337wqOPRh2JBG39epgxw9e5FhGRxFFyLZIEo0ZBjx6QTkcdiQQtnYYpU3y1EBERSRwl1yJJ0L07TJyoiiFdQSbjl72fNCnqSEREpAOUXIskRSoF8+bB5s1RRyJBymRg3339NCAREUkcJdciSZFKQW0tvPJK1JH8//buPsiuujzg+PfJZpOwhmA1GGogJBE7NmEwQjChUIZi0QRRcaZaQKlOqdRKp5C+JuhMxUE6jqWBDGChIHGYyovVVoYhpUxkRsdAaIBQqBi6Ce9DElKEJnFZ8vL0j3vS3ix7Nzd7783Zs/v9zNzZe875PXufffKbvU/O/u456pTM2rIQL8EnSZVlcy1VxemnwzXXwLvfXXYm6pRnnoFt27x5jCRVmJfik6riqKPg0kvLzkKdNHNm7cow73pX2ZlIkobJM9dSlWzeDPfcU3YW6pRx42DOHJg6texMJEnDZHMtVcnKlfCxj8Grr5adiTrhqqtg1aqys5AktcDmWqqSfR90e/jhcvNQ+/X3wxVXwAMPlJ2JJKkFNtdSlcyfX1s64M1kRp/162uXWfTDjJJUaTbXUpUcfjjMnevNZEajff+mXoZPkirN5lqqmgULao1YZtmZqJ3WroXp02sPSVJl2VxLVbNsGaxbV3YWarfNm10SIkmjgNe5lqpm9uyyM1AnrF4Nu3aVnYUkqUWeuZaqaOVKuO22srNQu3V3l52BJKlFNtdSFd12G1x7bdlZqF2WL4dPfcp19JI0CthcS1W0YAE8/jj09ZWdidph1Sro7YWIsjORJLXI5lqqogULYPdueOyxsjNRq/burd0UyEvwSdKoYHMtVdG+RsybyVTfhg3w+uteKUSSRgmba6mKjjoKZs2CF14oOxO1ypvHSNKoYnMtVdHGjfSfdQ59t3yXveO66Jsyjf4vLYGNG5uP/9IS+qZMM77s+N+/iL6uyfSvuLH5eEnSiGVzLVXNqlXsPGEhK26dzPHb1zAh+zl++xpW3HwYO09YWPtwXDPxNx9m/EiJ37OeFbf0NBcvSRrZMrNjD2ARsAHoBZYOcnwicGdxfC0ws+7YsmL/BuAjB3qtk046KaVRr7c3d/RMzYWsydp12/Z/LGRN7uiZmtnba/xojJckjQjAumzQk3bszHVEdAHXA4uBOcD5ETFnwLCLgF9k5nHAcuAbRewc4DxgbtGg31B8P2lM67/6Om7Y9QUe4pRBjz/EKXxr1x/Qv/x640dhvCRp5Ivs0E0LIuIU4KuZ+ZFiexlAZv5N3Zj7ijEPRsR4YDNwJLC0fmz9uEavN3/+/Fy3bl1HfhZppOibMo3jt69hE+9pOGY2G3liwkn0nPjr+x+YNYu+e1Y3F999Ij0nDfi/8Mkn07fyzubix3+Anvlz9z+weDF9f3t9c/Fd8+g5+fj9D1x4IX1Lrxh+/GWX0feFPxl+/JVX0vfJC4Yff+ON9J12VnPxU06l5/XNDcdIksoVEY9k5vzBjo3v4OtOB+ovZfAiMPDj8P83JjN3R8TrwDuL/Q8NiJ0+8AUi4mLgYoAZM2a0LXFppJq4YxvPceyQY55nBpPe3A5Tpux/YPLk5uN37XhrfE9P8/G7d741ftKk5uP3/PKt8RMnthY/YUJr8d3drcV3dTUfv2PbkGMkSSNXJ5vrjsvMm4CboHbmuuR0pI7rnzyVY7c/N+SZzxk8zxtTjqTnvvveGn/HD1uLv+HW1uKvvLq1+CWXtxbfav0OVfzkqfQ0HCFJGsk6ebWQl4Bj6raPLvYNOqZYFnIE8N9NxkpjzrjPXsAXu28Zcswfdd9M14UXGD8K4yVJFdDok46tPqidFd8EzAImAI8DcweMuQT4++L5ecBdxfO5xfiJRfwmoGuo1/NqIRoTyr5ahfFeLUSSNOTVQjrWXNdel7OBp4GNwJeLfV8DPl48nwR8j9ol9x4GZtfFfrmI2wAsPtBr2VxrzLj33tzRMzW/2b00Z9Ob43kzZ9Ob3+xeWmvM7r3X+NEcL0kqXWnN9aF82FxrTOntzTcuWZI7p0zLPeO6cueUafnGJUuaP+NpfLXjJUmlGqq57til+A41L8UnSZKkQ2GoS/F5+3NJkiSpTWyuJUmSpDaxuZYkSZLaxOZakiRJahOba0mSJKlNbK4lSZKkNrG5liRJktrE5lqSJElqk1FzE5mIeAV4bpjhU4FtbUxnrLF+rbF+rbF+rbF+rbF+rbF+rbOGrRlu/Y7NzCMHOzBqmutWRMS6RnfZ0YFZv9ZYv9ZYv9ZYv9ZYv9ZYv9ZZw9Z0on4uC5EkSZLaxOZakiRJahOb65qbyk6g4qxfa6xfa6xfa6xfa6xfa6xf66xha9peP9dcS5IkSW3imWtJkiSpTcZ0cx0RiyJiQ0T0RsTSsvOpooh4NiKeiIj1EbGu7HxGuoj4dkRsjYgn6/a9IyLuj4j/Kr7+Spk5jmQN6vfViHipmIPrI+LsMnMcySLimIh4ICJ+FhH/GRGXFvudg00Yon7OwSZExKSIeDgiHi/qd0Wxf1ZErC3ei++MiAll5zoSDVG/lRHxTN38m1d2riNZRHRFxGMRcU+x3fb5N2ab64joAq4HFgNzgPMjYk65WVXWb2XmPC8F1JSVwKIB+5YCqzPzvcDqYluDW8lb6wewvJiD8zLz3kOcU5XsBv4sM+cAC4FLit97zsHmNKofOAeb0Q+cmZnvB+YBiyJiIfANavU7DvgFcFGJOY5kjeoH8Bd18299eSlWwqXAU3XbbZ9/Y7a5Bj4I9Gbmpsx8E7gD+ETJOWmUy8wfA68O2P0J4DvF8+8A5x7SpCqkQf3UpMx8OTMfLZ5vp/YGMx3nYFOGqJ+akDU7is3u4pHAmcA/Ffudfw0MUT81KSKOBj4K3FxsBx2Yf2O5uZ4OvFC3/SL+khyOBP4tIh6JiIvLTqaipmXmy8XzzcC0MpOpqD+OiP8olo24pKEJETET+ACwFufgQRtQP3AONqX4k/x6YCtwP7AReC0zdxdDfC8ewsD6Zea++ff1Yv4tj4iJJaY40l0D/CWwt9h+Jx2Yf2O5uVZ7nJaZJ1JbXnNJRJxedkJVlrXL93gm4uB8C3gPtT+TvgxcXW46I19ETAa+D1yWmf9Tf8w5eGCD1M852KTM3JOZ84Cjqf0F+X0lp1QpA+sXEccDy6jV8WTgHcBflZjiiBUR5wBbM/ORTr/WWG6uXwKOqds+uting5CZLxVftwL/TO2XpQ7Oloj4VYDi69aS86mUzNxSvOHsBf4B5+CQIqKbWmP4j5n5g2K3c7BJg9XPOXjwMvM14AHgFODtETG+OOR7cRPq6reoWK6UmdkP3Irzr5FTgY9HxLPUlgKfCVxLB+bfWG6u/x14b/Ep0QnAecDdJedUKRHxtog4fN9z4MPAk0NHaRB3A58rnn8O+GGJuVTOvqaw8Emcgw0V6wtvAZ7KzL+rO+QcbEKj+jkHmxMRR0bE24vnhwFnUVu3/gDwO8Uw518DDer387r/GAe19cLOv0Fk5rLMPDozZ1Lr+X6UmZ+hA/NvTN9Eprhc0jVAF/DtzPx6ySlVSkTMpna2GmA88F1rOLSIuB04A5gKbAH+GvgX4C5gBvAc8OnM9EN7g2hQvzOo/Tk+gWeBP6xbP6w6EXEa8BPgCf5/zeHl1NYNOwcPYIj6nY9z8IAi4gRqHxjronZy767M/FrxXnIHtSUNjwGfLc7Cqs4Q9fsRcCQQwHrgi3UffNQgIuIM4M8z85xOzL8x3VxLkiRJ7TSWl4VIkiRJbWVzLUmSJLWJzbUkSZLUJjbXkiRJUpvYXEuSJEltYnMtSRUVETvqnp8dEU9HxLF1+2ZGxIsRMW5A3PqIWNDge86MCK+TK0nDZHMtSRUXER8CVgCLM/O5ffsz81ngeeA368a+Dzg8M9ce6jwlaSywuZakCouI06ndcvuczNw4yJDbqd2NbJ/zgDuKM9Q/iYhHi8dvDPK9Px8R19Vt31PcfIGI+HBEPFjEfi8iJrf1B5OkirK5lqTqmkjtDp/nZubPG4y5Czg3IsYX279LreHeCpyVmScW+1Y0+6IRMRX4CvDbRfw64E+H9yNI0ugy/sBDJEkj1C5gDXARcOlgAzJzS7GG+kMRsQXYnZlPRsQRwHURMQ/YA/zaQbzuQmAO8NOIAJgAPDj8H0OSRg+ba0mqrr3Ap4HVEXF5Zl7VYNy+pSFbiucAS4rt91P7K+Ybg8TtZv+/cE4qvgZwf2ae31r6kjT6uCxEkiosM38JfBT4TERc1GDYD4CzqS3/uKPYdwTwcmbuBS4EugaJexaYFxHjIuIY4IPF/oeAUyPiOICIeFtEHMyZb0katTxzLUkVl5mvRsQi4McR8Upm3j3g+GsR8SBwVGZuKnbfAHw/In4P+Fdg5yDf+qfAM8DPgKeAR4vv90pEfB64PSImFmO/Ajzd5h9NkionMrPsHCRJkqRRwWUhkiRJUpvYXEuSJEltYnMtSZIktYnNtSRJktQmNteSJElSm9hcS5IkSW1icy1JkiS1ic21JEmS1Cb/C8aUQN6dnsW3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZhlXfFasE_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6873ac45-b5b4-4803-8c34-e1a2f5876b4b"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn import tree\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(X, y)\n",
        "\n",
        "tree.plot_tree(clf) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(209.25, 203.85, 'X[20] <= 16.795\\ngini = 0.468\\nsamples = 569\\nvalue = [212, 357]'),\n",
              " Text(136.01250000000002, 176.67000000000002, 'X[27] <= 0.136\\ngini = 0.159\\nsamples = 379\\nvalue = [33, 346]'),\n",
              " Text(78.46875, 149.49, 'X[12] <= 6.597\\ngini = 0.03\\nsamples = 333\\nvalue = [5, 328]'),\n",
              " Text(68.00625000000001, 122.31, 'X[13] <= 38.605\\ngini = 0.024\\nsamples = 332\\nvalue = [4, 328]'),\n",
              " Text(41.85, 95.13, 'X[14] <= 0.003\\ngini = 0.012\\nsamples = 319\\nvalue = [2, 317]'),\n",
              " Text(20.925, 67.94999999999999, 'X[14] <= 0.003\\ngini = 0.245\\nsamples = 7\\nvalue = [1, 6]'),\n",
              " Text(10.4625, 40.77000000000001, 'gini = 0.0\\nsamples = 6\\nvalue = [0, 6]'),\n",
              " Text(31.387500000000003, 40.77000000000001, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
              " Text(62.775000000000006, 67.94999999999999, 'X[21] <= 33.27\\ngini = 0.006\\nsamples = 312\\nvalue = [1, 311]'),\n",
              " Text(52.3125, 40.77000000000001, 'gini = 0.0\\nsamples = 292\\nvalue = [0, 292]'),\n",
              " Text(73.2375, 40.77000000000001, 'X[21] <= 33.56\\ngini = 0.095\\nsamples = 20\\nvalue = [1, 19]'),\n",
              " Text(62.775000000000006, 13.590000000000003, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
              " Text(83.7, 13.590000000000003, 'gini = 0.0\\nsamples = 19\\nvalue = [0, 19]'),\n",
              " Text(94.16250000000001, 95.13, 'X[13] <= 39.15\\ngini = 0.26\\nsamples = 13\\nvalue = [2, 11]'),\n",
              " Text(83.7, 67.94999999999999, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
              " Text(104.625, 67.94999999999999, 'X[25] <= 0.082\\ngini = 0.153\\nsamples = 12\\nvalue = [1, 11]'),\n",
              " Text(94.16250000000001, 40.77000000000001, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
              " Text(115.0875, 40.77000000000001, 'gini = 0.0\\nsamples = 11\\nvalue = [0, 11]'),\n",
              " Text(88.93125, 122.31, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
              " Text(193.55625, 149.49, 'X[21] <= 25.67\\ngini = 0.476\\nsamples = 46\\nvalue = [28, 18]'),\n",
              " Text(156.9375, 122.31, 'X[23] <= 810.3\\ngini = 0.332\\nsamples = 19\\nvalue = [4, 15]'),\n",
              " Text(136.01250000000002, 95.13, 'X[24] <= 0.179\\ngini = 0.124\\nsamples = 15\\nvalue = [1, 14]'),\n",
              " Text(125.55000000000001, 67.94999999999999, 'gini = 0.0\\nsamples = 14\\nvalue = [0, 14]'),\n",
              " Text(146.475, 67.94999999999999, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
              " Text(177.8625, 95.13, 'X[23] <= 844.65\\ngini = 0.375\\nsamples = 4\\nvalue = [3, 1]'),\n",
              " Text(167.4, 67.94999999999999, 'gini = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
              " Text(188.32500000000002, 67.94999999999999, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
              " Text(230.175, 122.31, 'X[6] <= 0.097\\ngini = 0.198\\nsamples = 27\\nvalue = [24, 3]'),\n",
              " Text(219.7125, 95.13, 'X[1] <= 19.435\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]'),\n",
              " Text(209.25, 67.94999999999999, 'gini = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
              " Text(230.175, 67.94999999999999, 'gini = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
              " Text(240.63750000000002, 95.13, 'gini = 0.0\\nsamples = 21\\nvalue = [21, 0]'),\n",
              " Text(282.4875, 176.67000000000002, 'X[21] <= 19.91\\ngini = 0.109\\nsamples = 190\\nvalue = [179, 11]'),\n",
              " Text(261.5625, 149.49, 'X[27] <= 0.145\\ngini = 0.498\\nsamples = 17\\nvalue = [8, 9]'),\n",
              " Text(251.10000000000002, 122.31, 'gini = 0.0\\nsamples = 9\\nvalue = [0, 9]'),\n",
              " Text(272.02500000000003, 122.31, 'gini = 0.0\\nsamples = 8\\nvalue = [8, 0]'),\n",
              " Text(303.4125, 149.49, 'X[24] <= 0.088\\ngini = 0.023\\nsamples = 173\\nvalue = [171, 2]'),\n",
              " Text(292.95, 122.31, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
              " Text(313.875, 122.31, 'X[26] <= 0.18\\ngini = 0.012\\nsamples = 172\\nvalue = [171, 1]'),\n",
              " Text(303.4125, 95.13, 'X[15] <= 0.016\\ngini = 0.375\\nsamples = 4\\nvalue = [3, 1]'),\n",
              " Text(292.95, 67.94999999999999, 'gini = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
              " Text(313.875, 67.94999999999999, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
              " Text(324.33750000000003, 95.13, 'gini = 0.0\\nsamples = 168\\nvalue = [168, 0]')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfXxU5Z34/c93CMwMCU8RSQxEm+ADwRBSIUmLoLRatz61WvukFh9bXff3a3/d7brdh3u9995d+3Tvb7elD7Y/Fq0aK5TYWhdRaWslbZHYbUv1FmMUQoNQCAiJGiaklu/9x3WiYTKTzCQz58xMvu/Xi5cmmbnO91xzzneuc53rXJeoKsYYY/wRCjoAY4yZSCzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGjMG0Wh0v4hoJv9Fo9H9Qe+XyT6xScyNSZ+IaKbPHRFBVSWjhZqcYy1dY4zxUVHQARiTz1paWqisrKS7u5uysjJUlXA4zMDAALNmzUJEqKqq4uGHH+b48eMsW7aM3/3ud0yZMoUpU6agqpx//vlB74bxkXUvGDMGQ7sXdu3axcaNG2loaGD37t0UFxfT1NREKBRiz5499PX1MWfOHPbv308sFuO5557j9ttvT1SmdS9MANa9YMw4bd26lUmTJjEwMEBPTw9Hjhxh3759PPnkk/T19VFRUUFxcTGLFy/mT3/6ExdddBFPPPEEd911V9ChmwBYS9eYMbAbaWasrE/XmDSIyALgeoB169ZRVFREdXU1XV1dxGIx6uvrKS0tJRQK0dnZyYEDB5gzZw79/f289tprLFu2jF/96ldEo1EikQhHjhyhtLSUxsbGwfJPU9XfB7mPJruspWvMKESkFPg4LtmeCjRHIpEb+/v7T8rkdiKRSKy/v/8o8BxwL9Ciqm9kchsmeJZ0jUlARCYDf4ZLtO8DHsclwh+r6ptZ3G4YuMzb7nnAI952f6aqx7O1XeMfS7rGDCEii3EJ7xpgF/Bd4Puq2hNALGVeHNcDpcD9wL2q2uF3LCZzLOmaCS8uuc3CJbf7cim5xX0ZdOK+DNYH8WVgxseSrpmQvMv4y3GJbAXwI9xl/FO5fBmfoNvjCVzcm7PZ7WEyx5KumTBERIBGXML6KPAsLmE9lI83rLwbfB/D7c9pwAO4FvqzgQZmRmRJ1xQ8EZkHrMIlp0m4RHt/IQ3N8oayXYfbz0O4ffyeqnYHGpgZxpKuKUgiUgxciUu0S4ANuET0dMafasghIjIJeA9uvy8HtuD2+1FVPRZkbMaxpGsKhoiEcP2z1+MS7tO4hPOIqsaCjC0IIjIN+DCuPs4G1uPq478L+Ysn11nSNXlPRObjLq2vA97AJZYHVPUPgQaWQ0Skire7WI7h6qhZVfcGGtgEZEnX5CURmQF8BJdEzgIexCWS31orLjnvZuK5uHq7CvgVrt4eVtWjQcY2UVjSNXnD66+8EJcwLgGexCWMx1R1IMjY8pGIRIErcPXZCPwAV5+/sC+u7LGka3KeiCzEJYZPAPtwiWGdqh4KNLACIiIVuPq9HogA9+GGn3UGGlgBsqRrcpKInARcjUsCFUAzLgk8H2hgBc7rfliCq/ePAzt4e/Kd14KMrVBY0jU5w3va6hLcCf9eYBPuhP+Jqv4pyNgmIhGZAlyK+zxWAhtxn8eT9nmMnSVdEyivZfVO3Il9NfAi7sTeoKq9QcZm3iYiJ/P2lUcZb0++0x5oYHnIkq4JhIicAlyLO4lLeLsPcWeggZlRiUgtb/exd/F2H/vhQAPLE5Z0jW+8p8SeBl4F6oEf4k7Yn+fyJDMmMREpwk26cz3wfuA3wBTgfOt+SM4WpjR+ugxYBHQA81T1JlXdYgk3P6nqm6r6mKp+HHgH8DpuDPCSQAPLcdbSNQlFo9H9/f39ZZkoKxKJHIjFYuWZKMvkFzuOhrOkaxLK5Gq3tsrtxGXH0XDWvWCMMT6yJdhNUi0tLVRWVtLd3U19fT379u0jHA4zMDDArFmzEBGqqqpYs2YNq1atYvPmzbz22mvMnDmTmpoazjzzzKB3weSIocfSnDlziEQiqOqwY+nhhx9m8uTJzJ8/n61btzJnzhwuvPBCiouLg96FjLHuBZPQ4GXhrl272LhxIw0NDezevZvi4mKampoIhULs2bOHvr4+jh49Snl5OT09PezYsYPbbrstvqyCuCw06RvavZDqsTRv3jwOHTo07FgqlOPIkq5JaPBkaW5upre3l9raWnbs2MHUqVOpq6ujo6ODiooKKioqCIfDlJSU0NHRQTgcHpZ8C+VkMekby3HU1tZGeXk5r7zyCocOHeL6668fLKsgjiNLuiYhuwFiMsGOo+GsT9cM463AwLp16ygqKqK6upquri5isRj19fWUlpYSCoXo7Ozk8OHDTJ06FVVl+fLlrF27ltNOO42SkhKKiooIh8NB744JWCrH0cGDB6mtrWXfvn10dXVx4YUXsm3bNk4//XT6+/sppMahtXTNW7wJZ64GPh8Oh888duxYRr6UI5HI0f7+/hpV7cpEeSZ/2Djd4WzImEFEporIp4GXgRuAzx47dmyKqsp4/wFz+/v7vwVsF5F7vFVrzQQRi8XK446HM4GDQFOS4yUCPAPcHv+3Qki4YEl3QhORmSLyD0AncAHwMVV9r6r+OFMdcaq6T1VvB04HdgGtIvKQiCzNRPkmf4jIVKAF+CdVfSbRa7wViz8K/LWInOdnfH6x7oUJSETKgb8EPgk8CnzZr8nBvUlvPgl8DjcHwxdx87PagVjAvCk87wEmA58Y7fMWkfcDa4ElqrrfhxB9Yy3dCUREqkXkLuAFoBh3QF/n52oMqtqnql/DtXwfAL4JbBORKwdv4JmC9EmgAbgllS9YVX0c+E9gvTebWcGwlu4EICKLgL/FTb/3HeBrqnog2Kgcb7HJK4C/A6YCXwa+p6p/DDQwkzEicg7wBLAinUnPvWNjE7BdVT+frfj8Zkm3gInIMlwyWwp8Ffh2rq7G4F1+XoCL93Tg34C1tix4fhORWcCvgb9V1e+P4f2zcfP0flpVf5Tp+IJgSbfAeMnrz3DJ61TgK8B3VTUWaGBpEJEmXMt8GbAa+Kaq9gQblUmX1130I2CXqv6vcZTzLuAR4N2FsLKIJd0C4V2KXYVLVpOBLwHrVfXNQAMbB2/p9c/jJj//T+CrqvqHYKMyqRKRvwMuB1aq6sA4y/o0cDMu8eZNAyIRS7p5TkTCwHXA3+DGP34ReLSQVmMQkdOAv8atqfZ94CuquivYqMxIROQ9wPeABlV9JQPliVfeUVW9ebzlBcnuFucpESkRkc/hxr5ehbs7fK6q/lchJVwAVf29qn4aWIBbX+0ZEfmeiNQFHJpJQETm4kamrMpEwgXwRjx8Cni3iNyUiTKDYi3dPCMiJwGfAf4C+BnwJVX9TbBR+UtEpgO3AZ/F3aT5oqr+MtioDLz1KPnPgMdV9V+zUH4N0Aq8T1W3Z7p8P1hLN0+IyDwR+Q/gJaACWKaqH51oCRdAVV9T1S8DVcBG4H4RaRWRi73LUBOcLwG9wBeyUbiqvgB8GmgRkZnZ2Ea2WUs3x4nIWbj+2itxT/T8u6ruDTaq3OINnv8o7ibin3AnfovaMuC+EpGrcEP9lqjq4Sxv6+tAJXBlvj3NaEk3R4nIElwSOR/31NY3VPXVYKPKbV4r91LccLky3HC5e73n+U0WiciZwC+AS1T1v33Y3hRcN8MPVPUr2d5eJlnSzSFe0liJSxoLgf8NrFHVN4KMK9949bgCV491wH8A31HV1wMNrECJyMXA3cA/q+pdPm73VNyDE0+o6rV+bXe8LOnmABFZjHtq7FNAKe5R2GZroY2fiLwTd8XwXuAuYAdu/LId+BkiItuAJuBkVT3k87afBN6TTytKWNINmIjMAHqAI8CtuMsl64vMMBE5A7gT+AjwdVX9TMAhFQxvNEn/eB+AGMf2Z/ud7MfDkm7AvEvhG4A2Vd0RcDgFzZtW8kPAr9KZeMWYTLKka4wxPrJxumMQjUb3i4iO9180Gi2oyZnzkX2WJ8pEffhRF/kSZyLW0h0DydCy0lIgS0rnM/ssT5SJ+vCjLvIlzkQKakZ2P7W0tFBZWcmhQ4eYPXs2AOFwmIGBAWbNmoWIUFVVxTe+8Q1uuOEG1q9fj4jQ1NTEtGnTqKqqCngPzKDBz7K7u5s5c+YQiURQ1WGf5b333kskEuHcc89l8+bNNDY2Mn369IL7LIfWR319Pfv27Ut4bK9Zs4ZVq1axefNm+vr6mDZtGrW1tYHEWVdXR3t7O2VlZUnjbGlp4fDhw8yfP5+5c+f6Fmc8a+mOweC37K5du9i4cSMNDQ3s3r2b4uJimpqaCIVC7Nmzh76+PkKhEG+++SZvvPEGl156aXw5BdE6ymfpfJZz5sxh/353Rdre3s6tt946tJyC+CzTrY8DBw7wxhtv8Pzzz3P77bcPluFbSzfVc7CkpISenh527drFjTfe6FuciVif7hg1Nzfz2GOPsXjxYrZv387AwACVlZU89dRTtLe3M2PGDKqqqjj77LOJRqOUlJTwox/9iHvuuSfo0E0CW7duZdKkSQwMDNDT08ORI0fYt28fTz75JH19fVRUVFBcXMzixYspLy9nwYIFPPXUU0GHnTWp1kddXR2TJk3illtuYcuWLb7GmM45uH//furr66murg78HLSW7hhYP2DhsM/yRPnSV5ovcSZifbpjtG7dOoqKiqiurqarq4tYLEZ9fT2lpaWEQiE6Ozt5+eWXueCCC4hGo2zatImjR48SjUaZP38+jY2NQe+C8aTyWR44cICZM2dy+PBhLrvsMh5++GEGBgYK9rNMpU527txJdXU1p556Kq2trVRVVflaF6nEePDgQWpra3nllVd4/fXXaWhoYOvWrZSVlfkWZzxr6Y5BNBrd39/fP+5PLRKJHIjFYuWZiMmMjX2WJ8pEffhRF/kSZyLWpzsGsVisXFVl6D9gEm4Vh4YhP3cO/pzoXyGcpPmuv7//g8Bu4OtAJNlnleDzfjfwe9zCmZFC+SwTHdve/j4NfGDIz9uAy4M6rpOcg1OAA8CCuJ/PyqXzz5Ju5pwH9OFWMkDdkjn3AHm9tEihEudzwH8Bn1PVz6QzwZCqbgPeiVtxeauInJ6lUAMnbrWGdwCPDfn13eTesX0J8JKqvgigqn8E7gduDDSqOJZ0M+dm4O643v3vAh8TkWgwIZlExC159Ahu4vMmVf3BWMpR1SO4uRy+CzwtIh/LWJC55SbgPj1xZen1wHtFJLjO0eFuxn0ZDHU3cL24ie5zgiXdDBC3bMjlQPPQ36vqHuBXuBPT5AARWQ78FmgHVqhq53jKU+frwPuBO0XkO4X0JStuzbNVxCUzVX0NeBj4RBBxxRORU3BzKG8Y+nt1y/vsBi4OIKyELOlmxseBHyeZXm4t7hvYBEhEQiLy90ALcJuq3p7JqQhV9dfAOcB0oM27JC8ElwIdqtqR4G9rgZu9mfKCdh3wUJIJ/3OqK8SSbmbcjDsAE3kEWCQi1T7GY4bwLoEfx7VGl6rqo9nYjtf6uwZ3c61VRK7PxnZ8luiSfdAvcMNOm/wLZzgv6d9E8nNwPfCeXOkKsaQ7TiKyCCgHfpzo797NmQfIsc78iUJELsAt6dIGvFdVX8nm9rzuhv/ErVTxeRG5V0RKsrnNbPEu2ZcTd8k+yLt/cTfBX8mdCxzHjagYRt0yTT/EtYYDZ0l3/G4Gvqsjr/ZwN3CDiEzyKaYJT0SKROSfcXevr1fVf4y7EZRVqvoc0IBbnfhXIlLn17Yz6Hrcqsp9I7zmPuDD4iaID8pNwNpRHlFbC9yUC10hlnTHQUTCwLW4oWFJqeqzwH7gfX7ENdGJyFzgp8Ay4BxV/UkQcahqn6reBHwB+KmI3JILJ30qhlyyJ+taAEBV9+G6GT7iR1zxRGQacCXuy3Ukv8Tlu3dlPahRWNIdnw8Az6nqrhRem1Od+YVKRC7BjZXeDPyZqgY+ubiq3o+7TP8fwIPi1hTLdcuBN0lyyR4nyGP7Y8BTqnpgpBflUFeIJd1xGrUlMMSDwEUiMjuL8UxYIjJZRL4CfAf4iKreOUqXj6+8AfvvAg4DvxGRJQGHNJpULtkHbQTOFJEzsxxTIumcg/cBVwXdx25Jd4xEpBJ31/ahVF6vqj24p59yYlxjIRGRdwCtwELgnar680ADSkJVY6r6F8DfAY+JyGdysbvBa4lfweiX7MAJT3752tpN8qRcUqr6B+DnBNQVMsiS7thdD6xX1Vga77mbHOnMLxQiciXwDG787QeSjJXOKaq6AdfqXQX8UERKAw4p3seAn6lqdxrvuRu4zucnvxI9KTeawLv5LOmOgYiEGHlcYDJbgGJgacaDmmBEJCwiq4F/x0288r+9+S7ygncf4FzcJEm/EZF3BxzSUOlcsgP+P/mV7Em5FDwKnCEiZ2U+qtRY0h2bR4F5wLPpvMkmwckMb3KZrcBc3OiEtoBDGhNVHVDVvwI+AzwsIp/3vtADIyILgdNwD5Oky89W5EhPyiXldYX8HvcYeCAs6Y5ND7BtjI+R3gt8QkTOz3BMBc+bGewFXHfC3cCHvUln8pqqPoIb0/sBYI+IfDyIOLzW42rc2NyxjGleD1woIh/ObGQn8m5G/xOwboxFfA/I6kMyI7GkOwaqerWqnjfGtx8DSoDPZTCkiaIGWADco6rfTPHOel5Q1S7gg0AFcHtAYRQDFwCJ5i9IxQDu2P7HjEWUWD2wGEinz/ktqvo1Va3MbEips5UjAiAiTUDP4LyfxuQKEbkGeHCsX2gicjYQVdX/zmxkJ2xjMnClqn4/W9vIJku6xhjjI+te8ESj0f0iouP9F41GA38CKpdlop4LvY6tjjInF89ra+l6JItLcY9nEb1CWfBwUCbqOVEdF5Js1VE2jsNcLzOb5/WYy7Kk64iIbtiwgcrKSg4dOsS8efPo7e1l+vTpDAwMMGvWLESEqqoq1q5dy7Rp01i4cCHPPPMMCxcupKSkhJ6eHlauXDnswxn84FevXk1DQwN79+5l7ty5VFdXEwqF2LNnD319fYRCIaZPn86iRYuGvregEoyI6LZt2+ju7mbWrFkcP348YR3feeedLFmyhNmzZzMwMEA4HCYcDrN48eKCq5N4Q4/Frq4uysrKWLFiBY8//jgLFy5ERDh+/Djt7e1vHa87duzgtNNO46KLLqKoqChhHQ1NQOkei8nqPBvHdibLHCyrpaWFyspKuru7mT59OpFIhHA4POy4W7NmDatWrWLz5s3s3r2bT33qU5SUlFjSzYbBD2fXrl1s3LiRhoYGdu/eTXFxMU1NTcM+7MEku2/fPq6++uqh5Yx4sI8hroJKMCKiO3fuHLWOw+EwsViMN954g97eXq655pqhZRRUncQbPF7uuOMOLr74Yn7zm99wzjnnDEs6R48epby8nL179/L8889z++23Dy3Dl+Mw18scWtZgfe7du5eTTz6ZGTNmMHfu3BPO6zfffJNYLMbhw4ezdszlzGJtuWLr1q1MmjSJgYEBenp6GBgYYN++fXR0dFBRUUFFRQXhcJiSkhKOHTtGRUUFjz766LDEEK+5uZne3l5qa2vZsWMHU6dOpa6uLmG5bW1tLF++nOLiIKcozZ506ritrY2zzz6bTZs2cckllwQdum+am5spKytjYGCAUChER0cHkUgkYR319vZy0UUXsWXLFs4/f+Th3+kch9u3b2fmzJnU19dnrMytW7dSVlbGkiUjz/eT6fNlaH0ePHiQvr4+6urqePLJJxOWd+mll7Jlyxb27t074nk9FtbS9WSz72fwcrG6upquri5isRj19fWUlpYSCoXo7Ozk4MGDLF26lFgsRnt7O6WlpTQ2NhZcq876dEeXrTpK5Tjs7e2ltLSUiooKWltbqaqqGvE4TKXMnTt3Ul1dTVlZWUrHdjpx1tTU0NramrRM69PNYSKiDz74IEVFRSN+2EeOHKG2tpZp06axadMmjh49SjQaZf78+UkPJLuR9rZ063nWrFm0trbS0dHBu971roL8IoqXah0dOHCAuro6tm/fzmWXXcbatWupr6/39TjM9TJTrcuDBw9SW1vLK6+8wuuvv05DQwPbtm1j9uzZNDU1WdLNhvF80EMVWpLMtEzUc6HXsdVR5uTieW3jdD2xWKxcVcX7Nvsobnmdswd/F/8P1x++Hjf5TXjw93agjyyunj8BvATMTFC/IdyCnvcCoaF/K/Q6HlpHQ+pjHnAEKPZ+XgG8QFzdTJQ6SlWiuvTq71PAD4f8/BXg/012vmeyPi3pxhGRy4BvAO9X1R3JXqduVYJVuCVNHhB/5xHNe+IeF/0qbtKa3vi/ex1xt+Kmwfykz+HlouuBDap61Pv5l8AkcmDNrzwVv7T84HzAk7O9YUu6Q4jI+3CVf7mq/m6016ubJu5jwDTgHrHVflMibjHBh4C/VrdoZ0LqVqG9CviCiJzjV3y5RhLM3+x9KeXEml/5RhJMX6luHpSXgawPkbGk6xGR83BTvn1IVZ9J9X2qegz4EO7y79sitirESLz6WQu0quq9o73eOxn+AmgRkVnZji9HrQD6gV/F/T4n1vzKQzcB9+rw6SvX4sOXmCVd3pr1qwX4uKr+It33e5d8lwNnA1+1xDuizwDzvf+mRN3yNo8A90nAk3wH5GYSLBKpObLmVz6RkVec2ACsEJFTshnDRDyATyAi78Sd0Deo6k/HWo6qvoFbquRc4IuWeIcTtyTN3+P6cfvTfPvfACcBn894YDlMRGbgJjdvTvKStdhKJOm4DHhRVV+K/4N3Dj8EXJfNACZ00vVu5mwCblPVTeMtz7sh9Ge4fqE7xlteIRGRk3GjPW5W1c50369ulY6PAp8RkfdmOr4c9nHgJ6p6MMnfNwGnS4BrfuWZ+Bto8bK+eOyETboiciawGficqv4gU+Wq6qvA+4CrRSSoFQByineD8UGgWVU3jrUcVX0FN8zsARGZm6n4ctyIC6Dq28uf3+hbRHlKRCpwV6IbRnjZ04B6r8uKCZl0RaQK+Anwj6r6vUyXr6oHcMue/LmI/M9Ml5+H/gl3rI279e91AX0TWO/H8J4giUgtbvHNzaO89G7gehu2OKrBYXd9yV7g9Ztn9YbahEu6IjIP+CnwZVVNd/nmlKnqXlzivV1EJuywHhG5BLgBuDrB3eKx+gLQC3w5Q+XlqpuB73pjwpNS1XbcUu6+LH+ej7zuglSXlr8PuNIb2phxEyrpikgZLuF+S1W/me3tqepu4ELg/xGRa7O9vVwjIu/ALTn/ca/1nxHqlrJfhTsxsrrybFBEZApwLa7+UuHLcKc8tgK3cGbbaC/0jtWncGPwM27CJF0ROQnXpfCAqv6bX9v17pJeBPybiFzl13aDJiIR3DC8L6nqLzNdvqoexg2Vusvrny80HwCeV9WdKb5+A3C+iNjjv4ndBNydxpRjWRsVMiGSrojMxPWLbQT+xe/te48TXwx8S0Qu9Xv7Afkq0On9NyvUrTj7fwEPiUihTT484g20eKr6OvAD3BWAGUJEpgNX4G44puoxoEpEajIdT8EnXa9f5jHgF8DfZ2RyzTFQ1e24ByjuEZELg4jBLyKyCngPbnhYtuv7/wC/xbV4C2JstHff4V24JJqOu4GbC6UeMuhjwJOq2p3qG7z7D/eShdZuQSddr0vhEeA54LNBJdxB3uPFVwHfE5ELCnGuBi/h/jvuAYjXsr097zP9c+CdwL+KSDTb2/RBK7B9yOQ2qdrq/XdZhuPJW95Ipf8Atozh7VmZBKdgk6530+wQ8Efgz4NOuINU9ee4Gx4/Ab4TcDgZ5dX5fcBPVfU5v7brJac7cU+7/S+/tptFVUDSGe6S8Y7xe4C/LpAvn0woAYpxs7KlRVU7gA7gLzMZUCGP6zsF16f4997d7lyyGXgCNwi7kLyO+yIJ4lHdDcDpvN3ay2eTxnHM9uH6LxtwLeYJTVWfE5Hx1OdJuKGJX8lUTLZyhDEFxJsQaDXwD4nmKTbp8frX/1JVP5exMi3pGmOMf3K+Tzcaje4XER3vv2g0uj/ofRmUiX3yY3/yIc58OT7yoS7zST7XZ863dCUHl1Aer0zskx/7kw9x5svxka26HO/Ci/m6gGU26tOvBUHz4kZaS0sLlZWVHDp0iNraWtrb2ykrK2NgYIBZs2YhIlRVVbFmzRpWrVpFS0sLRUVFzJs3j5kzZ7J48eKgd2GYofs0b948ent7mT59+rB9Wrt2LeXl5cyfP5+XXnqJyZMns2DBAt/ibGtro7u7m7q6ulHrffPmzYTDYaZMmcLs2bOpr6/3JcbBuuzu7mbWrFkcP348YV3eeeedNDU1sWDBAnbs2IGqMm/ePI4cOeJrnIcOHWL27Nk0Njby+OOPs3DhQkSE2bNn09raSnt7OzfeeCObNm1iYGCAmTNnUltbm7DM/v7+svEkH2/ESV4aWp8lJSVEIhHC4fCIeWHGjBlMmTKF008/fVh5461LSK0+86alu2vXLjZu3EhDQwO7d++muLiYpqYmQqEQe/bsoa+vj1AoxJtvvkksFuPw4cNcc801Q8vJuZZuKvukqsyYMYMjR46wcuXKoWX40tLduXPnqDEePXqUefPmcejQIfbt28fVV1/tW5zp1GU4HCYcDtPT08OLL77ILbfcEkic1dXV3HHHHVx88cXs3buXuXPnUl1dfUKsr732GgsWLGD+/Pkjxji0xbd69WoaGhqSlhkKhZg+fTqLFi3ybb+zJROfe/y+x7ees1WfeZN0M1BOzhxc+XDZ7m0j5+PMl+MjW3UpInr//ffT29tLbW0tO3bsYOrUqdTV1dHR0UFFRQUVFRWEw2FKSkp49tlnCYfDxGIxVq5cmVPnRTqyUZ/p1mVbWxvl5eX84Q9/4LnnnuP2229PqT7zonsBoLm5OeXK6OjoIBaL0d/fz65du7jtttuCDn+YdPanra2N2tpatm/fzqWX+jt1Q7pxlpeX09PTw/nnn+9rnOnGeuDAAV599VWWLfP/4a104ty2bRvV1dWceWbyOX0ikQgLFy6kqxgseuAAABRkSURBVKuLmTNnUl9fT2lpKfPmzaOzs5MdO3YwZ84cIpEI5557LmvWrOGccwpjceVMf+aj1WV7ezslJSVvlbd06VK6u7s577zzUo45L5LuunXrUjqwGhsbCYVC7Nq1i8mTJ3PyySezZMmSoMNPqKioiLKyMqZNm8Ypp5xCLBYjEomwcuVKQqEQnZ2dHDhwgMbGRlasWEFrayv9/f0888wzNDY25lScBw8eZOnSpaxYsYItW7bQ3d1NJBKhqanJtzhTOUZeeuklampqmDJlCp2dnZSWlvoW31jiDIVC9PT00NPTk7S8SCRy4CMf+ci4bqSN9b25IJXj8/Dhw9TU1FBZWUlnZ2fSc2i8dTlYxmivyfnuhUzcUYTcukvr113S8cqHOPPl+MiHuswn+VyfOT9ONxaLlauqDP7DPeLYBZw89Pfe34qB/w+4Nf5vuXSwxu+TF/tJuNUQTvJ+/hDQGv86P/cnSZzvxNV/kffz3+KWBw8kzkQxenENziw1+PM9wN/kYJx3AHcN+fmXwBVBfeb5YoT6fAa41Pv/ctw5NSOX6jPnk+5QIjIfWAN8VFUPxf9d3cQnV+Fmm8rNfoXkrgEeUzc5N7i5f88SkTMCjCmRmzhxCZn7gKtEpCTAmBKJX5ol66u8pkvcI7s3kiDOYCLKb/L2mnJPwFsrQPwMt4p0zsibpCtu1qSHgH9W1aRLbqibGegvgA0i4n+n3didkCTUrfLaTA6dgOJWg7iGIUvIqOofgJ/jVnHICSJyKm7Cl6Hz0f4Sd7y/O5CgEluJmyTo10N+933gPBE5JZCI8ttNwL164ppyObeMUd4kXeAbwAu4lWBHpKotwMPAfV5rIqeJyDtx3Qs/jftTrq3y+kHcPK+7436fa62z64H1qhob/IU3vijX4hy2hIyqvoGtAJE2cWvKfYLha8o9DpwmIgv9jyqxnE9IACJyE66F8qk0Bud9HpiJ63PMdTcB92jc9HPqlvn5PfD+QKIa7mYSLyHzKHCGiJzlczzDeF+yyZa6yZmuEHFLSF2Gu5qJtxZbASJdlwM7VPXlob/ULK4AMVY5n3RFpB43n+VVXisgJd7l+ceA/ykiF2QrvvHyLtmvJvmqrznROhOR04AluCuIE3h1fT+ufzJoK4HXgN/E/2FIV0gu9PFdDWxW1VcT/O1p3FzLtgJE6kZaU+4eYJVkeAWIscrppOu1BlqAT6vqC+m+X1X34i45mkVkbqbjy5ArgN+q6u+T/H098N4ceEb+BuDBoZfscXKlK+Rm3GiKZFdEWVvlNU3JrhoGu0Jyri8yV3nn9rtx93yG8e7zvIi7sghcziZd79Lqu7g7+uvGWo6qPgl8HVifK990ceLvsp9A3TpjD+O+PAKR5C77CbwvxU7gEr/iiicis4BLgQdGeNkm4HQR8W/WoDgiUgeU4ZZsSuZ+4EpxC6uakV0PbNCR15S7mxz5EsvZpAvcjhtnl4kZ278EHMF1U+QM75L9HOCHo7w06D6+9wJHVHXYJXucoFuRVwNPJLlkB97qCrmPYLtCbsb14f8p2QtUdT9uMcVc6ArJWaP04Q+1AVgmIhXZj2pkOZl0ReR84K9w43EHxlued4PqOuAKEcmZoU24E/9BVe0f5XW/wD2y7d9ztSdK5aAGN9zpfBEJahB/0kv2OFlZ5TUVIhImbtjdCIL+EssH5wEx4FcjvUhV+3Bdldf5EdRIci7peuMTvwdcp6pdmSpXVY/gxpJ+K4fuso94yT5oyHAn3y+PvEv2S3CfyYhU9XVcq9334U7eDdeTGT7sbhhVbQd2ARdnO64EPgg8q6qdKbz2MaBKRGqyHFM+GzbsbgQ58YBMTiVdr+WxHvg/qro50+Wr6q+BfwAeEpHiTJefpguAV1X1tym+/l7gwwHEfS0nPik3mqC6QgaH3SW9ZI8T1I2qVFvjg8Od7sNauwmJyAzgA7j+71S0AX8EVmQtqBTkVNIFvgAcBf4li9tYg3sC6NsBf+OlfPLBW8OdfoH/T36NeKMvgcEl0H178ivRk3Ip8P3JL+9JuaWM3oc/VE4Nd8oxVwM/TjQlQCK58oBMziRdEbkSd9PgE/EPCWSSV/G3AYuBW7O1nZF4jye/nxQu2eP42job4Um5pALqCkn2pFxSAT35dQOwboRhd8Oo6ovAS7hRGeZE6TYIwLWKrxCR6VmIJyU5kXRF5F3Ad4CPpPqtNR7e0JIPA/8iIhdle3sJXAts8vqZ0/EoUCcif5WFmBJJ+KRcCu4DPuTjk1+p3uiL51tXSDp9+AkE3jrLNSKyCKgA0uqGVNVu4Encg1OBCDzpisjpuCdwfqiqz/i1XW/A9NeAJ/x8Ys3rk/0kYzv5jgPTgT/PaFAJeCMQRnpSLilvuFMrPiQKrzWe8Em5FAw++eXH2OIrcNMMjjbsLpENwAoROTuzIeUnEZkE/A9OnO0uHWuBT4nI1MxGlprAky4QxvWx/msA2/42rp/Uz77drwF1QEe6b/QOsNn4MxfDZlzXwr4xvv814Gs+tHafAaakc8k+yOsKOYabRjPbHgKOpTF3yFu8rpAIowyLmkCW4boGx3psduBmoftixiJKQ9CPbKKqz+NuLgSx7UP4fydzK+4E2jOWN3sD/5MO/s+gR4H/8h4mGIt/AM4G+jIXUkIbSPMSM85ngb/LUCwjeQpXJ2P1WWwuhkFduJEI3x3j+1/GHTe+XVkPlfPL9RhjTCHJePdCNBrdLyI6nn/RaHR/puPKdIz5Eme2YzTGpCfjLV3Jwnr0mZaJGL1ycj7ObMdojElPVvp029raOHToELW1tbS3t1NWVsbAwACzZs1CRKiqqmLNmjWsWrWKlpYWioqKmDdvHjNnzmTx4sXZCGmYlpYWKisr6e7uZs6cOUQiEVR1WJxr167l1FNPpaamhtbWVhYvXsy0adOoqqoaVuZ4VyhNtDrp0DhnzpzJlClTCIfDI9bn4cOHWbJkCRUVw+f2yNYqquMpN9mqrFZmZsvMF5ne92ycl+ORlZbuzp07qa6uBuDYsWMcOnSIuXNTn87Wrxbkrl27Mhrn0Jbp6tWraWhoYO/evcydO5fq6mpCoRB79uyhr6+PUCjE9OnTWbRo0ajljSfOkWLMVJzj3fdkn7eVmdky88XgvufCsZmszPHISku3urp62M4VFRWdsHOqSigU4tVXX+XAgQPccsst2Qhl3HGGQiFKSkro7e3lhRde4NZbR3+Irbm5mUmTJjEwMMDBgwfp6+sjEonQ0dFBRUUFFRUVhMNhSkpKeOKJJ1i+fDnFxcmnVEglzqNHj1JeXk5PTw+7du3ixhtHnrmwubmZ3t5eFi9ezPbt25k6dSqRSISnnnrqrRjnzJlDSUkJbW1tlJaW8uyzz3LxxcnniEm3zOrq6hH3O90yOzo6+N3vfjfq1VIhxplqmfkinXPomWeeIRwOp1SfmTwvxyprfbqDB0xtbS07duxg6tSp1NXVJdzBrq6uEy7Z/Wjp3n///SnH19HRQSwWIxaLcfjwYa655pqEcY63DzZZeenUZVtbG/PmzePQoUOcf/75GY8xUZzjLTeV1p6VOf4y80Wm9z3T5+V42Y208ZUzLKE9+OCDFBUVUV1dTVdXF7FYjPr6ekpLSwmFQnR2dnLw4EGWLl1KLBajvb2d0tJSGhsbfUmQqcbY29tLaWkpNTU1bNmyhe7ubhYsWEBTU1PSA3us+97U1JQ08aRS5uHDh6mpqUFVM1bm0P1vbW3NaJlnnHEGmzZtorq62td9zxciohs2bEh7vxOdQ4PljfUzT1bmuPYvG0k3nZOvv7+f7du3c8opp9DY2DhYRtaTbjoxRqNRNm3axOTJk4lEIsyePTth8sl0h326cU6bNo3W1lb6+vqorKxMeMDYjbSJW2a+sBtpacrWSZ1JmYgR8iPOfD8BjSk0vjyRJiLfAvap6r96P6/AzXtQm5Hr/AwRkZ8C31bVDd7PnwQuVdUrg43sbeIm++gELlfV33m/+wpuKoHPBxqcMWZUWU+6IhIFXgHqVXWP9zvBLYl8napuy2oAKRKRatzz3PNU9Zj3u2m4ORLOUtUDQcY3SNxUlF9U1SVDfrcA+Blw6jjmSjDG+MCPWcauAp4ZTLgQ7JpfI7gReGAw4UKwa36NYNiKExrsml/GmDT40dJ9Erhr8JJ9yO8rgOdxLctsz0Q1Iu+SfTeuK+HZuL8txy3xszDorhAROQnYCVRp3AToInIT8AFVvSKQ4IwxKclqS1dE5gOLgEfi/6aq+whmza9E3gfsj0+4nl/i6sm3Nb9GcC3waHzC9Wwg2OXPjTEpyHb3wg3EXbLHyZVlSJKutZQrXSFeP/jNJI8zF7tCjDFxsta9MNIl+5DXTMbdqDpP3fI5vhOR2bhJjd+hqj1JXnMKsAOoVDeLv+9EZAnQAszXJGuWici5uP7emqC7QowxiWWzpTvSJTsA3p32ZtxNrKBcC2xMlnDhreXPf06wXSGpLBLp+/Lnxpj0ZDPpDrvLnsRa4HoR8X3poCGX7KnGGUgXgzfs7uOMsjxJrnSFGGOSy0rSFZGTcS3dB0d7raq+gOuGCGK40xKgBNiSwms3AaeLyFnZDSmhK4H/VtWuFF47uPz5tCzHZIwZg2y1dK/FLWrYm+Lrg7qhdjNw9yiX7MBbXSH3EVycqbTGhy5/ngujQowxcbIx4Y0AzwKfVtWnUnyP709+iVvz/hVg8dAHN0Z5j+9PfolIFW7V0nkjjAKJf88HgM+r6rlZDc4Yk7ZstHQbgKm41lZKhgx3ui4L8STzIaAt1YQLgT35dSPwvVQTrucxoFpEarIUkzFmjLKRdG8ixUv2OGuBm7yWsh+Sjs0dxf3A34pIaYbjGcYbdncDacY5pCskyFEhxpgEMpp0vUv2jwL3juHtvj355T0pV0uCJ+VScAQXox+X7hcC3YOziaXpbuA6byy0MSZHZLql+1sAVX0l3Td6w51ewiXfbGsH9qZ5yQ6Aqq4HPovr280aL1k+jpuNbSxeAqbjT30aY1KU6bGx3cCvx/H++4BlGYplJHuB9WN9s6p+LYOxJPMm8AZju2pAVY+LyM/wZyY5Y0yKfJnE3BhjjGOtoFFEo9H9IqLj+ReNRvcHvR/GmNxgLd1R5MPqxsaY/JFyS3c8Lb5kLb1Ml5lPrdKJvO/GTGQpt3SHtvhWr15NQ0MDe/fuZe7cuVRXVxMKhdizZw99fX2EQiGmT5/OokWLBt+bsKWX6TLjW6XplpmoXBHRDRs2UFlZSXd3N3PmzCESiaCqDAwMMGvWLESEqqoq1q5dy6mnnkpNTQ0bN27kHe94B2eccQZnnXXWiLHm6r4bYzIvraR7//3309vbS21tLTt27GDq1KnU1dXR0dFBRUUFFRUVhMNhSkpK2LZtG9XV1Zx55pkjJt10ymxra2P58uUUFxcnTTzplPfss89SXl7Oyy+/TE9PD9dcc03CpKuq3HHHHVx88cXs3buXk08+mRkzZjB37twTklhJSQm7d+/mwIED3HLLLUPjGnesmd73trY2amtreemll1i5cmXSOI0xmTWmlm7aG0mhpZuJMrPR/zpYZnNzc1rJfObMmfT09HDeeedlPFa/9t0Yk3lpJd0HH3yQoqIiqqur6erqIhaLUV9fT2lpKaFQiM7OTnp7eyktLWX27Nls376dU045haampqRJN5UyDx48yNKlS4nFYrS3t1NaWpqwzMGugFTiO+2003j66afZuXMn8+fPp6ysjMbGxqRJd1yVnCRJZnrf0/l8ampqaG1tpa+vj8rKyoT7bozJvJSTbjQa3d/f3182lo1EIpEDsVhs2IKJmS5zPOUlKzfd5Dht2jRaW1tpb29n2bJlSZNZPuy7MSbzbMjYKCyZGWMyyR6OGEUsFitXVRn6D2jETfE4yft5GtADVMS/VlXFEq4xZpAl3bE5YZFIb4XgH2DLnxtjRmHdC2mSt1ecqBs6m5qILAPuARbY8ufGmGSspZu+wRUn4qevfBpQ/JklzRiTpyzppi/hIpFe6zawZdqNMfnBuhfSIG7FiaeBykQToItIOfACbuHK1/2OzxiT+6ylm54bgQeSrTjhLX++BbdkkTHGDGNJN0WS+iKR1sVgjEnKkm7qLgL2qepzo7zuMaBKbPlzY0wClnRTl/AGWjxVfRO3rtlNWY/IGJN37EZaCkTkZNzquqepam8Krz8L17dbqap/zHZ8xpj8YS3d1HwCeCSVhAugqi/ikvSlWY3KGJN3LOmOQkSEFLsW4tgNNWPMMJZ0R9cARIDWNN/XAiwXkYrMh2SMyVeWdEd3M3B3uvMpeJPgtADXZSUqY0xesqQ7AhH5IG4UwuNjLOKnwP8tIvWZi8oYk88s6Y6sHigCusb4/l/juibmZiwiY0xesyFjI/BuosngvLljLCM0nvcbYwqLJV1jjPGRdS8YY4yPJmzSjUaj+0VEx/ovGo3u96NMY0xhmbDdCyLy1iiw1atX09DQwN69e5k7dy7V1dWEQiH27NlDX18foVCI6dOns2jRoqHvH7asejbKNMYUlqKgAwhac3MzkyZNYmBggIMHD9LX10ckEqGjo4OKigoqKioIh8OUlJTwxBNPsHz5coqLi0csr7e3l8WLF7N9+3amTp1KJBLhqaeeequ8OXPmUFJSQltbG8ePH2fv3r1ccsklPu61MSYo1tId+/tHbOlmqkxjTGGZ0C3ddevWUVRURHV1NV1dXcRiMerr6yktLSUUCtHZ2cnBgwdZunQpsViM9vZ2SktLaWxsTFpmS0vLiOX19vZSWlpKTU0Nra2ttLe3s2zZshHLNMYUjgnb0o1Go/v7+/vLxvr+SCRyIBaLlWe7TGNMYZmwSdcYY4IwYYeMGWNMECzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMjyzpGmOMj/5/ir6iLxFUqAMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SWKNqw0wmPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2827394d-dbca-47cb-fc6b-6436530b1aed"
      },
      "source": [
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X, y)\n",
        "print(lasso_reg.sparse_coef_.shape)\n",
        "# lasso_reg.predict([[1.5]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQraSn6Bm1Z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Importing libraries\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Lasso Regression\n",
        "# class LassoRegression:\n",
        "#     def __init__(self, learning_rate, iterations, l1_penality):\n",
        "#         self.learning_rate = learning_rate\n",
        "#         self.iterations = iterations\n",
        "#         self.l1_penality = l1_penality\n",
        "\n",
        "#     # Function for model training\n",
        "#     def fit(self, X, Y):\n",
        "#         # no_of_training_examples, no_of_features\n",
        "#         # self.m, self.n = X.shape\n",
        "#         self.n, self.m = X.shape\n",
        "#         # weight initialization\n",
        "#         self.W = np.zeros(self.n)\n",
        "#         self.b = 0\n",
        "#         self.X = X\n",
        "#         self.Y = Y\n",
        "#         # gradient descent learning\n",
        "#         for i in range(self.iterations):\n",
        "#             self.update_weights()\n",
        "\n",
        "#         return self\n",
        "\n",
        "#     # Helper function to update weights in gradient descent\n",
        "#     def update_weights(self):\n",
        "#         Y_pred = self.predict(self.X)\n",
        "#         # calculate gradients\n",
        "#         dW = np.zeros(self.n)\n",
        "#         print(f'dW shape: {dW.shape}')\n",
        "\n",
        "#         for j in range(self.n):\n",
        "#             if self.W[j] > 0:\n",
        "#                 dW[j] = (\n",
        "#                     -(2 * (self.X[:, j]).dot(self.Y - Y_pred)) + self.l1_penality\n",
        "#                 ) / self.m\n",
        "#             else:\n",
        "#                 dW[j] = (\n",
        "#                     -(2 * (self.X[:, j]).dot(self.Y - Y_pred)) - self.l1_penality\n",
        "#                 ) / self.m\n",
        "\n",
        "#         db = -2 * np.sum(self.Y - Y_pred) / self.m\n",
        "\n",
        "#         # update weights\n",
        "#         self.W = self.W - self.learning_rate * dW\n",
        "#         self.b = self.b - self.learning_rate * db\n",
        "\n",
        "#         return self\n",
        "\n",
        "#     # Hypothetical function h( x )\n",
        "#     def predict(self, X):\n",
        "#         print(f'X shape: {X.shape}')\n",
        "#         print(f'W shape: {self.W.shape}')\n",
        "#         return X.dot(self.W) + self.b\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L37UwSKfbims",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Importing libraries\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Lasso Regression\n",
        "# class LassoRegression:\n",
        "#     def __init__(self, learning_rate, iterations, l1_penality):\n",
        "#         self.learning_rate = learning_rate\n",
        "#         self.iterations = iterations\n",
        "#         self.l1_penality = l1_penality\n",
        "\n",
        "#     # Function for model training\n",
        "#     def fit(self, X, Y):\n",
        "#         # no_of_training_examples, no_of_features\n",
        "#         self.d, self.n = X.shape\n",
        "#         # weight initialization\n",
        "#         self.W = np.zeros(self.n)\n",
        "#         # self.W = np.zeros(self.n)\n",
        "#         # self.W = np.zeros((self.n,self.n))\n",
        "#         self.b = 0\n",
        "#         self.X = X\n",
        "#         self.Y = Y\n",
        "#         # flatten() and reshape(self.d,self.n)\n",
        "#         # gradient descent learning\n",
        "#         for i in range(self.iterations):\n",
        "#             self.update_weights()\n",
        "\n",
        "#         return self\n",
        "\n",
        "#     # Helper function to update weights in gradient descent\n",
        "#     def update_weights(self):\n",
        "#         Y_pred = self.predict(self.X)\n",
        "#         # calculate gradients\n",
        "#         dW = np.zeros(self.n)\n",
        "#         db = 0\n",
        "#         # dW = np.zeros((self.n,self.n))\n",
        "#         # print(f'dW shape: {dW.shape}')\n",
        "#         # print(f'self.Y shape: {self.Y[:,0].shape}')\n",
        "#         # print(f'Y_pred shape: {Y_pred.shape}')\n",
        "#         # print(f'x: {self.X[0,:].shape}')\n",
        "#         # print(f'y: {Y_err[0,:].shape}')\n",
        "#         # print(f'n: {self.n}')\n",
        "#         # print(f'd: {self.d}')\n",
        "#         # print(f'res:{(-(2 * (self.X[0, :]).dot(Y_err[0,:])) + self.l1_penality) / self.n}')\n",
        "\n",
        "#         for j in range(self.n):\n",
        "#           if self.W[j] > 0:\n",
        "              \n",
        "#               if j == 747:\n",
        "#                   print(f'j_if: {j}')\n",
        "              \n",
        "#               dW[j] = (\n",
        "#               -(2 * (self.X[:,j]).dot(self.Y[:,j] - Y_pred)) + self.l1_penality\n",
        "#               ) / self.n\n",
        "#               db += -2 * np.sum(self.Y[:,j] - Y_pred) / self.n\n",
        "#           else:\n",
        "#               if j == 747:\n",
        "#                   print(f'j_else: {j}')\n",
        "              \n",
        "#               dW[j] = (\n",
        "#                   -(2 * (self.X[:,j]).dot(self.Y[:,j] - Y_pred)) - self.l1_penality\n",
        "#               ) / self.n\n",
        "#               db += -2 * np.sum(self.Y[:,j] - Y_pred) / self.n\n",
        "\n",
        "#         # db = -2 * np.sum(self.Y - Y_pred) / self.n\n",
        "\n",
        "#         # update weights\n",
        "#         self.W = self.W - self.learning_rate * dW\n",
        "#         self.b = self.b - self.learning_rate * db\n",
        "\n",
        "#         return self\n",
        "\n",
        "#     # Hypothetical function h( x )\n",
        "#     def predict(self, X):\n",
        "#         # X = X.reshape(self.d,self.n)\n",
        "#         # self.W = self.W.reshape(self.n,self.n)\n",
        "#         # print(f'X shape: {X.shape}')\n",
        "#         # print(f'W shape: {self.W.shape}')\n",
        "#         return X.dot(self.W) + self.b\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WGm1D2b2CTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lasso Regression\n",
        "class LassoRegression:\n",
        "    def __init__(self, learning_rate, iterations, l1_penality):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iterations = iterations\n",
        "        self.l1_penality = l1_penality\n",
        "\n",
        "    # Function for model training\n",
        "    def fit(self, X, Y):\n",
        "        # no_of_training_examples, no_of_features\n",
        "        self.d, self.n = X.shape\n",
        "        # weight initialization\n",
        "        # self.W = np.zeros(self.n)\n",
        "        # self.W = np.zeros(self.n)\n",
        "        self.W = np.zeros((self.n,self.n))\n",
        "        self.b = 0\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        # flatten() and reshape(self.d,self.n)\n",
        "        # gradient descent learning\n",
        "        for i in range(self.iterations):\n",
        "            self.update_weights()\n",
        "\n",
        "        return self\n",
        "\n",
        "    # Helper function to update weights in gradient descent\n",
        "    def update_weights(self):\n",
        "        Y_pred = self.predict(self.X)\n",
        "        # calculate gradients\n",
        "        # dW = np.zeros(self.n)\n",
        "        db = 0\n",
        "        dW = np.zeros((self.n,self.n))\n",
        "        # print(f'dW shape: {dW.shape}')\n",
        "        # print(f'self.Y shape: {self.Y[:,0].shape}')\n",
        "        # print(f'Y_pred shape: {Y_pred.shape}')\n",
        "        # print(f'x: {self.X[0,:].shape}')\n",
        "        # print(f'y: {Y_err[0,:].shape}')\n",
        "        # print(f'n: {self.n}')\n",
        "        # print(f'd: {self.d}')\n",
        "        # print(f'res:{(-(2 * (self.X[0, :]).dot(Y_err[0,:])) + self.l1_penality) / self.n}')\n",
        "\n",
        "        i = 0\n",
        "        for j in range(self.n):\n",
        "          if self.W[i][j] > 0:\n",
        "              dW[i][j] = (\n",
        "              -(2 * (self.X[i,j]).dot(self.Y[i,j] - Y_pred[i][j])) + self.l1_penality\n",
        "              ) / self.n\n",
        "              db += -2 * np.sum(self.Y[i,j] - Y_pred[i][j]) / self.n\n",
        "          else:\n",
        "              dW[i][j] = (\n",
        "                  -(2 * (self.X[i,j]).dot(self.Y[i,j] - Y_pred[i][j])) - self.l1_penality\n",
        "              ) / self.n\n",
        "              db += -2 * np.sum(self.Y[i,j] - Y_pred[i][j]) / self.n\n",
        "\n",
        "        # db = -2 * np.sum(self.Y - Y_pred) / self.n\n",
        "\n",
        "        # update weights\n",
        "        self.W = self.W - self.learning_rate * dW\n",
        "        self.b = self.b - self.learning_rate * db\n",
        "\n",
        "        return self\n",
        "\n",
        "    # Hypothetical function h( x )\n",
        "    def predict(self, X):\n",
        "        # X = X.reshape(self.d,self.n)\n",
        "        # self.W = self.W.reshape(self.n,self.n)\n",
        "        # print(f'X shape: {X.shape}')\n",
        "        # print(f'W shape: {self.W.shape}')\n",
        "        print(f'X.dot(self.W) shape: {X.dot(self.W).shape}')\n",
        "        return X.dot(self.W) + self.b\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPwSxyc52Tp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c775353-cae5-49f2-d2ec-f35d54965178"
      },
      "source": [
        "d, n = X_t.dot(corrMatrix).shape\n",
        "print(n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxELsyHHwfL6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "64ec7f32-3499-4a4b-dd21-d506a3ef0cd9"
      },
      "source": [
        "# Model training\n",
        "model = LassoRegression(iterations=1, learning_rate=0.01, l1_penality=500)\n",
        "X_t = X.transpose()\n",
        "print(X_t.shape)\n",
        "print(corrMatrix.shape)\n",
        "# Quanto a implementação original mudaste\n",
        "# o suficiente pra que o Y fosse passado\n",
        "# com a mesma dimensionalidade de X\n",
        "model.fit(X_t.dot(corrMatrix), X_t)\n",
        "print(\"W shape:\", model.W.shape)\n",
        "# print(\"W:\", model.W)\n",
        "# print(\"Predicted values \", np.round(Y_pred[:3], 2))\n",
        "# print(\"Real values\t \", Y_test[:3])\n",
        "# print(\"Trained W\t \", round(model.W[0], 2))\n",
        "# print(\"Trained b\t \", round(model.b, 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 748)\n",
            "(748, 748)\n",
            "X.dot(self.W) shape: (4,)\n",
            "W shape: (748,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CGFPk5LzTw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "eps = np.finfo(float).eps\n",
        "from numpy import log2 as log\n",
        "\n",
        "outlook = 'overcast,overcast,overcast,overcast,rainy,rainy,rainy,rainy,rainy,sunny,sunny,sunny,sunny,sunny'.split(',')\n",
        "temp = 'hot,cool,mild,hot,mild,cool,cool,mild,mild,hot,hot,mild,cool,mild'.split(',')\n",
        "humidity = 'high,normal,high,normal,high,normal,normal,normal,high,high,high,high,normal,normal'.split(',')\n",
        "windy = 'FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE'.split(',')\n",
        "play = 'yes,yes,yes,yes,yes,yes,no,yes,no,no,no,no,yes,yes'.split(',')\n",
        "\n",
        "dataset ={'outlook':outlook,'temp':temp,'humidity':humidity,'windy':windy,'play':play}\n",
        "df = pd.DataFrame(dataset,columns=['outlook','temp','humidity','windy','play'])\n",
        "\n",
        "##1. claculate entropy o the whole dataset\n",
        "\n",
        "entropy_node = 0  #Initialize Entropy\n",
        "values = df.play.unique()  #Unique objects - 'Yes', 'No'\n",
        "\n",
        "for value in values:\n",
        "    fraction = df.play.value_counts()[value]/len(df.play)  \n",
        "    entropy_node += -fraction*np.log2(fraction)\n",
        "\n",
        "def ent(df,attribute):\n",
        "    target_variables = df.play.unique()  #This gives all 'Yes' and 'No'\n",
        "    variables = df[attribute].unique()    #This gives different features in that attribute (like 'Sweet')\n",
        "\n",
        "\n",
        "    entropy_attribute = 0\n",
        "    for variable in variables:\n",
        "        entropy_each_feature = 0\n",
        "        for target_variable in target_variables:\n",
        "            num = len(df[attribute][df[attribute]==variable][df.play ==target_variable]) #numerator\n",
        "            den = len(df[attribute][df[attribute]==variable])  #denominator\n",
        "            fraction = num/(den+eps)  #pi\n",
        "            entropy_each_feature += -fraction*log(fraction+eps) #This calculates entropy for one feature like 'Sweet'\n",
        "        fraction2 = den/len(df)\n",
        "        entropy_attribute += -fraction2*entropy_each_feature   #Sums up all the entropy ETaste\n",
        "\n",
        "    return(abs(entropy_attribute))\n",
        "\n",
        "a_entropy = {k:ent(df,k) for k in df.keys()[:-1]}\n",
        "a_entropy\n",
        "\n",
        "def ig(e_dataset,e_attr):\n",
        "    return(e_dataset-e_attr)\n",
        "\n",
        "#entropy_node = entropy of dataset\n",
        "#a_entropy[k] = entropy of k(th) attr\n",
        "IG = {k:ig(entropy_node,a_entropy[k]) for k in a_entropy}\n",
        "\n",
        "\n",
        "def find_entropy(df):\n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "    entropy = 0\n",
        "    values = df[Class].unique()\n",
        "    for value in values:\n",
        "        fraction = df[Class].value_counts()[value]/len(df[Class])\n",
        "        entropy += -fraction*np.log2(fraction)\n",
        "    return entropy\n",
        "  \n",
        "  \n",
        "def find_entropy_attribute(df,attribute):\n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "    target_variables = df[Class].unique()  #This gives all 'Yes' and 'No'\n",
        "    variables = df[attribute].unique()    #This gives different features in that attribute (like 'Hot','Cold' in Temperature)\n",
        "    entropy2 = 0\n",
        "    for variable in variables:\n",
        "        entropy = 0\n",
        "        for target_variable in target_variables:\n",
        "            num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n",
        "            den = len(df[attribute][df[attribute]==variable])\n",
        "            fraction = num/(den+eps)\n",
        "            entropy += -fraction*log(fraction+eps)\n",
        "        fraction2 = den/len(df)\n",
        "        entropy2 += -fraction2*entropy\n",
        "    return abs(entropy2)\n",
        "\n",
        "\n",
        "def find_winner(df):\n",
        "    Entropy_att = []\n",
        "    IG = []\n",
        "    for key in df.keys()[:-1]:\n",
        "        #Entropy_att.append(find_entropy_attribute(df,key))\n",
        "        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n",
        "    return df.keys()[:-1][np.argmax(IG)]\n",
        "  \n",
        "  \n",
        "def get_subtable(df, node,value):\n",
        "    return df[df[node] == value].reset_index(drop=True)\n",
        "\n",
        "\n",
        "def buildTree(df,tree=None): \n",
        "    Class = df.keys()[-1]   #To make the code generic, changing target variable class name\n",
        "    \n",
        "    #Here we build our decision tree\n",
        "\n",
        "    #Get attribute with maximum information gain\n",
        "    node = find_winner(df)\n",
        "    \n",
        "    #Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n",
        "    attValue = np.unique(df[node])\n",
        "    \n",
        "    #Create an empty dictionary to create tree    \n",
        "    if tree is None:                    \n",
        "        tree={}\n",
        "        tree[node] = {}\n",
        "    \n",
        "    #We make loop to construct a tree by calling this function recursively. \n",
        "    #In this we check if the subset is pure and stops if it is pure. \n",
        "\n",
        "    for value in attValue:\n",
        "        \n",
        "        subtable = get_subtable(df,node,value)\n",
        "        clValue,counts = np.unique(subtable['Eat'],return_counts=True)                        \n",
        "        \n",
        "        if len(counts)==1:#Checking purity of subset\n",
        "            tree[node][value] = clValue[0]                                                    \n",
        "        else:        \n",
        "            tree[node][value] = buildTree(subtable) #Calling the function recursively \n",
        "                   \n",
        "    return tree\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}